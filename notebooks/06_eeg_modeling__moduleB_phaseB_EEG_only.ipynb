{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57967546-f20f-4989-b63c-3b6de7168eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3940cdd7-f747-4921-8f11-954e2c0a7365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:44:15] INFO: === Module B (BF: EEG本体 / B+F統合) start ===\n",
      "[11:44:15] INFO: ROOT_DIR     : /Users/shunsuke/EEG_48sounds\n",
      "[11:44:15] INFO: OUT_DIR      : /Users/shunsuke/EEG_48sounds/moduleB_outputs\n",
      "[11:44:15] INFO: trial_table  : /Users/shunsuke/EEG_48sounds/derivatives/trial_table/trial_table_all_runs.csv\n",
      "[11:44:15] INFO: master_sound : /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_sound_level_with_PC.csv\n",
      "[11:44:15] INFO: epochs_all   : /Users/shunsuke/EEG_48sounds/derivatives/epochs_all/epochs_all-epo.fif\n",
      "[11:44:15] INFO: qc_by_trial  : /Users/shunsuke/EEG_48sounds/derivatives/qc_all/qc_by_trial_all.csv\n",
      "[11:44:15] INFO: seed=42 n_perm=1000 n_perm_map=200 n_jobs=1 perm_within=True\n",
      "[11:44:15] INFO: crop         : -2.0..12.0 sec | ERP/TFR baseline=(-0.2,0.0)\n",
      "[11:44:15] INFO: erp_windows_ms=[(-2000, -1500), (-1500, -1000), (-1000, -500), (-500, 0), (0, 200), (200, 400), (400, 800), (800, 1200), (1200, 2000), (2000, 3000), (3000, 4000), (4000, 5000), (5000, 12000)]\n",
      "[11:44:15] INFO: tfr_windows_ms=[(-2000, -1500), (-1500, -1000), (-1000, -500), (-500, 0), (0, 200), (200, 400), (400, 800), (800, 1200), (1200, 2000), (2000, 3000), (3000, 4000), (4000, 5000), (5000, 12000)]\n",
      "[11:44:15] INFO: bands=[('theta', 4, 7), ('alpha', 8, 12), ('beta', 13, 30), ('gamma', 31, 40)]\n",
      "[11:44:15] INFO: tasks=['emo_arousal', 'emo_approach', 'emo_valence', 'emo_arousal_high', 'emo_approach_high', 'emo_valence_high', 'is_ambiguous', 'category_3']\n",
      "[11:44:15] INFO: with_maps=False map_tasks=ALL\n",
      "[11:44:15] INFO: model_family=linear\n",
      "[11:44:15] INFO: log          : /Users/shunsuke/EEG_48sounds/moduleB_outputs/logs/moduleB_20260128_114415.log\n",
      "[11:44:15] INFO: [BUILD] epochs_all: /Users/shunsuke/EEG_48sounds/derivatives/epochs_all/epochs_all-epo.fif\n",
      "[11:44:36] INFO: [SAVE] built trial_features: /Users/shunsuke/EEG_48sounds/moduleB_outputs/tables/moduleB_trial_eeg_features.csv\n",
      "[11:44:37] INFO: [QC] keys=['subject_id', 'run_id', 'trial_in_run'] | matched=1728/1728 | kept(pass)=1588/1728\n",
      "[11:44:37] INFO: [DATA] shape=(1588, 500) subjects=12 sounds=48 eeg_features=455\n",
      "[11:44:37] INFO: [SAVE] run metadata: /Users/shunsuke/EEG_48sounds/moduleB_outputs/tables/moduleB_run_metadata.json\n",
      "[11:44:37] INFO: [CAT] category-like cols=['category', 'カテゴリー'] | has_カテゴリー=True\n",
      "[11:44:37] INFO: --- Task: emo_arousal (regression) ---\n",
      "[11:53:53] INFO: [task:emo_arousal] LINEAR mean=0.1079 p_perm=0.0010\n",
      "[11:53:54] INFO: --- Task: emo_approach (regression) ---\n",
      "[12:02:49] INFO: [task:emo_approach] LINEAR mean=0.0607 p_perm=0.0160\n",
      "[12:02:49] INFO: --- Task: emo_valence (regression) ---\n",
      "[12:11:47] INFO: [task:emo_valence] LINEAR mean=0.0395 p_perm=0.0919\n",
      "[12:11:47] INFO: --- Task: emo_arousal_high (binary) ---\n",
      "[13:05:39] INFO: [task:emo_arousal_high] LINEAR mean=0.5616 p_perm=0.0010\n",
      "[13:05:39] INFO: --- Task: emo_approach_high (binary) ---\n",
      "[14:00:30] INFO: [task:emo_approach_high] LINEAR mean=0.5802 p_perm=0.0010\n",
      "[14:00:30] INFO: --- Task: emo_valence_high (binary) ---\n",
      "[15:32:32] INFO: [task:emo_valence_high] LINEAR mean=0.5181 p_perm=0.1538\n",
      "[15:32:32] INFO: --- Task: is_ambiguous (binary) ---\n",
      "[16:25:18] INFO: [task:is_ambiguous] LINEAR mean=0.5405 p_perm=0.0400\n",
      "[16:25:18] INFO: --- Task: category_3 (multiclass) ---\n",
      "[16:36:40] INFO: [task:category_3] LINEAR mean=0.3846 p_perm=0.0010\n",
      "[16:36:40] INFO: === Module B complete ===\n",
      "[16:36:40] INFO: - tables : /Users/shunsuke/EEG_48sounds/moduleB_outputs/tables\n",
      "[16:36:40] INFO: - figures: /Users/shunsuke/EEG_48sounds/moduleB_outputs/figures\n",
      "[16:36:40] INFO: - log    : /Users/shunsuke/EEG_48sounds/moduleB_outputs/logs/moduleB_20260128_114415.log\n",
      "[16:36:40] INFO: - elapsed: 17545.8 sec\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "============================================================\n",
    "Module B (BF: EEG本体 / B+F統合)  再現性プロトコル完全実装（最終版）\n",
    "============================================================\n",
    "\n",
    "【主目的】\n",
    "- EEG特徴量（ERP + TFR）からターゲット（主観/PC/カテゴリ）を予測\n",
    "- 教授の「EEGが薄い」を封殺するため、LOSO + 置換検定（必須）を主軸に\n",
    "- 必要な場合のみ「いつ/どの帯域が効くか」(maps) と「AIっぽい補助モデル」を追加\n",
    "\n",
    "【入力】\n",
    "- epochs_all-epo.fif（trialごとのEpochs; 例: derivatives/epochs_all/epochs_all-epo.fif）\n",
    "- trial_table_all_runs.csv（trial ↔ sound ↔ subject ↔ run ↔ trial_in_run）\n",
    "- master_sound_level_with_PC.csv（sound-level targets）\n",
    "- qc_by_trial_all.csv（trial-level QC; qc_pass True のみ残す）\n",
    "\n",
    "【出力】\n",
    "- moduleB_outputs/\n",
    "  - tables/: 主要結果（fold別・summary・置換・重要特徴） + 任意で maps\n",
    "  - figures/: 任意（mapsや重要特徴）\n",
    "  - logs/: 実行ログ\n",
    "  - moduleB_run_metadata.json\n",
    "\n",
    "============================================================\n",
    "5) 次に“最強”にするなら（手戻り最小で効く順）\n",
    "============================================================\n",
    "(1) QCをONにして主結果を確定（デフォルトで apply_qc=True 推奨）\n",
    "(2) 特徴量CSVを再利用して再実行を高速化（--reuse-built）\n",
    "(3) 置換を並列化して時間短縮（--n-jobs -1 など）\n",
    "(4) “いつ効くか”を図で出す（--with-maps, --map-tasks で最小限のタスクだけ）\n",
    "(5) 「AI要素」を補助で追加（--model-family linear+mlp など）\n",
    "    ※主結果は線形（解釈可能）で勝ち、AIは“補助結果”として見栄えを足すのが最強\n",
    "\n",
    "============================================================\n",
    "実行例（推奨：QCあり・主結果のみ）\n",
    "============================================================\n",
    "python moduleB_BF_final.py --root-dir /Users/shunsuke/EEG_48sounds\n",
    "\n",
    "（再実行を速く）\n",
    "python moduleB_BF_final.py --root-dir /Users/shunsuke/EEG_48sounds --reuse-built\n",
    "\n",
    "（mapsも出す：重要タスクだけ）\n",
    "python moduleB_BF_final.py --root-dir /Users/shunsuke/EEG_48sounds --with-maps \\\n",
    "  --map-tasks emo_arousal_high,category_3 --n-perm-map 200\n",
    "\n",
    "（置換を並列）\n",
    "python moduleB_BF_final.py --root-dir /Users/shunsuke/EEG_48sounds --n-jobs -1\n",
    "\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import zlib\n",
    "import argparse\n",
    "import logging\n",
    "import platform\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mne\n",
    "mne.set_log_level(\"ERROR\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, accuracy_score, r2_score\n",
    "\n",
    "# Optional (AIっぽい補助モデル)\n",
    "try:\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    _HAS_MLP = True\n",
    "except Exception:\n",
    "    _HAS_MLP = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utils\n",
    "# ============================================================\n",
    "\n",
    "U32_MOD = 2**32\n",
    "\n",
    "def seed_u32(x: int) -> int:\n",
    "    return int(x % U32_MOD)\n",
    "\n",
    "def stable_hash_u32(text: str) -> int:\n",
    "    # pythonのhash()はセッションで変わるのでCRC32を使う\n",
    "    return int(zlib.crc32(text.encode(\"utf-8\")) % U32_MOD)\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def now_tag() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def setup_logging(log_path: Path) -> None:\n",
    "    ensure_dir(log_path.parent)\n",
    "    for h in list(logging.root.handlers):\n",
    "        logging.root.removeHandler(h)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "def setup_matplotlib(auto_font: bool = True) -> None:\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "    if not auto_font:\n",
    "        return\n",
    "    # macOS想定（無ければ勝手にフォールバック）\n",
    "    for f in [\"Hiragino Sans\", \"Hiragino Kaku Gothic ProN\", \"Arial Unicode MS\"]:\n",
    "        try:\n",
    "            mpl.rcParams[\"font.family\"] = f\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def json_dump(path: Path, obj: Any) -> None:\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def safe_read_csv(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"CSV not found: {p}\")\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "def to_int_series(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def extract_int_prefix(s: pd.Series) -> pd.Series:\n",
    "    txt = s.astype(str)\n",
    "    m = txt.str.extract(r\"^\\s*0*(\\d+)\", expand=False)\n",
    "    return pd.to_numeric(m, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def extract_any_int(s: pd.Series) -> pd.Series:\n",
    "    txt = s.astype(str)\n",
    "    m = txt.str.extract(r\"(\\d+)\", expand=False)\n",
    "    return pd.to_numeric(m, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def pearsonr_safe(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    if len(a) < 2:\n",
    "        return np.nan\n",
    "    if np.allclose(np.std(a), 0) or np.allclose(np.std(b), 0):\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "def fdr_bh(pvals: np.ndarray, alpha: float = 0.05) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Benjamini-Hochberg FDR. returns (reject_bool, qvals)\"\"\"\n",
    "    p = np.asarray(pvals, dtype=float)\n",
    "    n = len(p)\n",
    "    order = np.argsort(p)\n",
    "    ranked = p[order]\n",
    "    q = ranked * n / (np.arange(n) + 1)\n",
    "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
    "    q = np.clip(q, 0, 1)\n",
    "    qvals = np.empty_like(q)\n",
    "    qvals[order] = q\n",
    "    reject = qvals <= alpha\n",
    "    return reject, qvals\n",
    "\n",
    "def _first_col(df: pd.DataFrame, cands: List[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "    \n",
    "def normalize_category_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    mergeでカテゴリーが カテゴリー_x / カテゴリー_y に割れたときに\n",
    "    最終的に df['カテゴリー'] を確実に作る。\n",
    "    ついでに不要な列は落とす。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 候補（あなたの現状に合わせて）\n",
    "    cand = [c for c in [\"カテゴリー\", \"カテゴリー_y\", \"カテゴリー_x\", \"category\", \"Category\", \"カテゴリ\"] if c in df.columns]\n",
    "    if not cand:\n",
    "        return df\n",
    "\n",
    "    # 「不快/中間/快」を含む列を優先して採用（最も堅い）\n",
    "    def looks_like_3cat(s: pd.Series) -> bool:\n",
    "        u = set(s.dropna().astype(str).str.strip().unique().tolist())\n",
    "        return len(u & {\"不快\", \"中間\", \"快\"}) > 0\n",
    "\n",
    "    pick = None\n",
    "    for c in [\"カテゴリー_y\", \"カテゴリー_x\", \"カテゴリー\", \"category\", \"Category\", \"カテゴリ\"]:\n",
    "        if c in df.columns and looks_like_3cat(df[c]):\n",
    "            pick = c\n",
    "            break\n",
    "\n",
    "    # それでも決まらない場合は、存在する最優先列を使う\n",
    "    if pick is None:\n",
    "        pick = cand[0]\n",
    "\n",
    "    df[\"カテゴリー\"] = df[pick].astype(str).str.strip()\n",
    "\n",
    "    # いらない派生列を削除（事故防止）\n",
    "    drop_cols = [c for c in [\"カテゴリー_x\", \"カテゴリー_y\"] if c in df.columns]\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Args\n",
    "# ============================================================\n",
    "\n",
    "def parse_windows_ms(s: str) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    例: \"-500-0,0-200,200-400,5000-12000\"\n",
    "    \"-2000--500\" のような負数もOK\n",
    "    \"\"\"\n",
    "    out: List[Tuple[int, int]] = []\n",
    "    for chunk in [c.strip() for c in s.split(\",\") if c.strip()]:\n",
    "        m = re.match(r\"^\\s*(-?\\d+)\\s*-\\s*(-?\\d+)\\s*$\", chunk)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Bad window format: {chunk}\")\n",
    "        out.append((int(m.group(1)), int(m.group(2))))\n",
    "    return out\n",
    "\n",
    "def parse_bands(s: str) -> List[Tuple[str, int, int]]:\n",
    "    # 例: \"4-7,8-12,13-30,31-40\" → theta/alpha/beta/gamma\n",
    "    names = [\"theta\", \"alpha\", \"beta\", \"gamma\", \"band5\", \"band6\"]\n",
    "    chunks = [c.strip() for c in s.split(\",\") if c.strip()]\n",
    "    bands: List[Tuple[str, int, int]] = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        m = re.match(r\"^\\s*(\\d+)\\s*-\\s*(\\d+)\\s*$\", ch)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Bad band format: {ch}\")\n",
    "        f0, f1 = int(m.group(1)), int(m.group(2))\n",
    "        bands.append((names[i] if i < len(names) else f\"band{i+1}\", f0, f1))\n",
    "    return bands\n",
    "\n",
    "def parse_list(s: str) -> List[str]:\n",
    "    return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "\n",
    "def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Module B (BF: EEG本体 / B+F統合) - QC + LOSO + permutation (+ optional maps/AI)\",\n",
    "        add_help=True,\n",
    "    )\n",
    "    p.add_argument(\"--root-dir\", type=str, default=\"/Users/shunsuke/EEG_48sounds\")\n",
    "\n",
    "    # Inputs\n",
    "    p.add_argument(\"--trial-features-csv\", type=str, default=\"\")  # 任意（無ければ build）\n",
    "    p.add_argument(\"--reuse-built\", action=\"store_true\")          # moduleB_trial_eeg_features.csv があれば再利用\n",
    "    p.add_argument(\"--epochs-all\", type=str, default=\"\")          # 任意（無ければ ROOT/derivatives/epochs_all/epochs_all-epo.fif）\n",
    "    p.add_argument(\"--trial-table\", type=str, default=\"\")         # 任意（無ければ ROOT/derivatives/trial_table/trial_table_all_runs.csv）\n",
    "    p.add_argument(\"--master-sound-pc\", type=str, default=\"\")     # 任意（無ければ ROOT/derivatives/master_tables/master_sound_level_with_PC.csv）\n",
    "\n",
    "    # QC（デフォルトON）\n",
    "    p.add_argument(\"--no-qc\", action=\"store_true\") \n",
    "    p.add_argument(\"--qc-by-trial\", type=str, default=\"\")  # 任意（無ければ ROOT/derivatives/qc_all/qc_by_trial_all.csv）\n",
    "    p.add_argument(\"--qc-pass-col\", type=str, default=\"\")  # 任意（空なら自動で qc_pass / qc_autoreject_pass を探す）\n",
    "\n",
    "    # Repro / compute\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--n-jobs\", type=int, default=1)\n",
    "\n",
    "    # Permutation（主結果）\n",
    "    p.add_argument(\"--n-perm\", type=int, default=1000)\n",
    "    p.add_argument(\"--perm-global\", action=\"store_true\")  # Trueなら全体シャッフル（デフォルトは被験者内）\n",
    "\n",
    "    # Optional maps（重い：必要時だけ）\n",
    "    p.add_argument(\"--with-maps\", action=\"store_true\")\n",
    "    p.add_argument(\"--no-maps\", action=\"store_true\") \n",
    "    p.add_argument(\"--map-tasks\", type=str, default=\"\")      # 空なら全taskにmaps（推奨は重要taskだけ）\n",
    "    p.add_argument(\"--n-perm-map\", type=int, default=200)    # mapsセル置換（軽め推奨）\n",
    "\n",
    "    # Feature build\n",
    "    p.add_argument(\"--resample\", type=float, default=250.0)\n",
    "    p.add_argument(\"--crop-tmin\", type=float, default=-2.0)\n",
    "    p.add_argument(\"--crop-tmax\", type=float, default=12.0)\n",
    "    p.add_argument(\"--baseline-tmin\", type=float, default=-0.2)\n",
    "    p.add_argument(\"--baseline-tmax\", type=float, default=0.0)\n",
    "\n",
    "    # Windows (ms)\n",
    "    p.add_argument(\n",
    "        \"--erp-windows\",\n",
    "        type=str,\n",
    "        default=\"-2000--1500,-1500--1000,-1000--500,-500-0,0-200,200-400,400-800,800-1200,1200-2000,2000-3000,3000-4000,4000-5000,5000-12000\"\n",
    "    )\n",
    "    p.add_argument(\n",
    "        \"--tfr-windows\",\n",
    "        type=str,\n",
    "        default=\"-2000--1500,-1500--1000,-1000--500,-500-0,0-200,200-400,400-800,800-1200,1200-2000,2000-3000,3000-4000,4000-5000,5000-12000\"\n",
    "    )\n",
    "    p.add_argument(\"--tfr-freqs\", type=str, default=\"4-7,8-12,13-30,31-40\")\n",
    "\n",
    "    # Tasks\n",
    "    p.add_argument(\n",
    "        \"--tasks\",\n",
    "        type=str,\n",
    "        default=\"emo_arousal,emo_approach,emo_valence,emo_arousal_high,emo_approach_high,emo_valence_high,is_ambiguous,category_3\"\n",
    "    )\n",
    "\n",
    "    # Model family\n",
    "    p.add_argument(\n",
    "        \"--model-family\",\n",
    "        type=str,\n",
    "        default=\"linear\",\n",
    "        choices=[\"linear\", \"linear+mlp\"],\n",
    "        help=\"主結果は線形。mlpは補助（AIっぽさ用）\"\n",
    "    )\n",
    "\n",
    "    p.add_argument(\"--no-auto-font\", action=\"store_true\")\n",
    "\n",
    "    if argv is None:\n",
    "        argv = sys.argv[1:]\n",
    "    args, _unknown = p.parse_known_args(argv)  # notebook の -f kernel.json を無視\n",
    "    return args\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    root_dir: Path\n",
    "    seed: int\n",
    "    n_jobs: int\n",
    "    n_perm: int\n",
    "    n_perm_map: int\n",
    "    perm_within_subject: bool\n",
    "\n",
    "    apply_qc: bool\n",
    "    qc_by_trial_csv: Path\n",
    "    qc_pass_col: str\n",
    "\n",
    "    with_maps: bool\n",
    "    map_tasks: List[str]\n",
    "\n",
    "    resample_sfreq: float\n",
    "    crop_tmin: float\n",
    "    crop_tmax: float\n",
    "    baseline_tmin: float\n",
    "    baseline_tmax: float\n",
    "\n",
    "    erp_windows_ms: List[Tuple[int, int]]\n",
    "    tfr_windows_ms: List[Tuple[int, int]]\n",
    "    bands: List[Tuple[str, int, int]]\n",
    "\n",
    "    tasks: List[str]\n",
    "    model_family: str\n",
    "\n",
    "    # Paths\n",
    "    derivatives_dir: Path\n",
    "    trial_table_csv: Path\n",
    "    master_sound_pc_csv: Path\n",
    "    epochs_all_path: Path\n",
    "\n",
    "    out_dir: Path\n",
    "    tables_dir: Path\n",
    "    figures_dir: Path\n",
    "    logs_dir: Path\n",
    "\n",
    "    trial_features_csv_out: Path\n",
    "    run_metadata_json: Path\n",
    "\n",
    "def make_config(args: argparse.Namespace) -> Config:\n",
    "    root = Path(args.root_dir).expanduser()\n",
    "    derivatives = root / \"derivatives\"\n",
    "\n",
    "    trial_table = Path(args.trial_table).expanduser() if args.trial_table else derivatives / \"trial_table\" / \"trial_table_all_runs.csv\"\n",
    "    master_pc  = Path(args.master_sound_pc).expanduser() if args.master_sound_pc else derivatives / \"master_tables\" / \"master_sound_level_with_PC.csv\"\n",
    "    epochs_all = Path(args.epochs_all).expanduser() if args.epochs_all else derivatives / \"epochs_all\" / \"epochs_all-epo.fif\"\n",
    "    qc_by_trial = Path(args.qc_by_trial).expanduser() if args.qc_by_trial else derivatives / \"qc_all\" / \"qc_by_trial_all.csv\"\n",
    "\n",
    "    out_dir = root / \"moduleB_outputs\"\n",
    "    tables  = out_dir / \"tables\"\n",
    "    figs    = out_dir / \"figures\"\n",
    "    logs    = out_dir / \"logs\"\n",
    "    for p in [out_dir, tables, figs, logs]:\n",
    "        ensure_dir(p)\n",
    "\n",
    "    erp_windows = parse_windows_ms(args.erp_windows)\n",
    "    tfr_windows = parse_windows_ms(args.tfr_windows)\n",
    "    bands       = parse_bands(args.tfr_freqs)\n",
    "    tasks       = parse_list(args.tasks)\n",
    "\n",
    "    # ★ここで「代入」を済ませる（Config呼び出しの中に書かない）\n",
    "    apply_qc  = (not bool(args.no_qc))   # デフォルトON、--no-qcでOFF\n",
    "    with_maps = (bool(args.with_maps) and (not bool(args.no_maps)))\n",
    "    map_tasks = parse_list(args.map_tasks) if args.map_tasks else []\n",
    "\n",
    "    cfg = Config(\n",
    "        root_dir=root,\n",
    "        seed=int(args.seed),\n",
    "        n_jobs=int(args.n_jobs),\n",
    "        n_perm=int(args.n_perm),\n",
    "        n_perm_map=int(args.n_perm_map),\n",
    "        perm_within_subject=(not bool(args.perm_global)),\n",
    "\n",
    "        apply_qc=apply_qc,\n",
    "        qc_by_trial_csv=qc_by_trial,\n",
    "        qc_pass_col=str(args.qc_pass_col).strip(),\n",
    "\n",
    "        with_maps=with_maps,\n",
    "        map_tasks=map_tasks,\n",
    "\n",
    "        resample_sfreq=float(args.resample),\n",
    "        crop_tmin=float(args.crop_tmin),\n",
    "        crop_tmax=float(args.crop_tmax),\n",
    "        baseline_tmin=float(args.baseline_tmin),\n",
    "        baseline_tmax=float(args.baseline_tmax),\n",
    "\n",
    "        erp_windows_ms=erp_windows,\n",
    "        tfr_windows_ms=tfr_windows,\n",
    "        bands=bands,\n",
    "\n",
    "        tasks=tasks,\n",
    "        model_family=str(args.model_family),\n",
    "\n",
    "        derivatives_dir=derivatives,\n",
    "        trial_table_csv=trial_table,\n",
    "        master_sound_pc_csv=master_pc,\n",
    "        epochs_all_path=epochs_all,\n",
    "\n",
    "        out_dir=out_dir,\n",
    "        tables_dir=tables,\n",
    "        figures_dir=figs,\n",
    "        logs_dir=logs,\n",
    "\n",
    "        trial_features_csv_out=tables / \"moduleB_trial_eeg_features.csv\",\n",
    "        run_metadata_json=tables / \"moduleB_run_metadata.json\",\n",
    "    )\n",
    "\n",
    "    # QCファイルが無ければOFFに落とす（ログはmainのsetup_logging後が望ましいが、挙動はOK）\n",
    "    if cfg.apply_qc and (not cfg.qc_by_trial_csv.exists()):\n",
    "        cfg.apply_qc = False\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load tables: trial_table & master_sound\n",
    "# ============================================================\n",
    "\n",
    "def load_trial_table(cfg: Config) -> pd.DataFrame:\n",
    "    tt = safe_read_csv(cfg.trial_table_csv)\n",
    "\n",
    "    if \"subject_id\" not in tt.columns:\n",
    "        c = _first_col(tt, [\"subject\", \"Subject\", \"participant\", \"participant_id\", \"sub\", \"sub_id\"])\n",
    "        if c:\n",
    "            tt[\"subject_id\"] = tt[c]\n",
    "    if \"subject_id\" in tt.columns and not pd.api.types.is_numeric_dtype(tt[\"subject_id\"]):\n",
    "        sid = to_int_series(tt[\"subject_id\"])\n",
    "        if sid.isna().mean() > 0.5:\n",
    "            sid = extract_int_prefix(tt[\"subject_id\"])\n",
    "        tt[\"subject_id\"] = sid\n",
    "    tt[\"subject_id\"] = to_int_series(tt[\"subject_id\"])\n",
    "\n",
    "    if \"run_id\" not in tt.columns:\n",
    "        c = _first_col(tt, [\"run\", \"Run\", \"block\", \"session\"])\n",
    "        if c:\n",
    "            tt[\"run_id\"] = tt[c]\n",
    "    if \"run_id\" in tt.columns and not pd.api.types.is_numeric_dtype(tt[\"run_id\"]):\n",
    "        tt[\"run_id\"] = extract_any_int(tt[\"run_id\"])\n",
    "    tt[\"run_id\"] = to_int_series(tt[\"run_id\"])\n",
    "\n",
    "    if \"trial_in_run\" not in tt.columns:\n",
    "        c = _first_col(tt, [\"trial_in_run\", \"trial\", \"Trial\", \"trial_num\", \"trial_index\"])\n",
    "        if c:\n",
    "            tt[\"trial_in_run\"] = tt[c]\n",
    "    tt[\"trial_in_run\"] = to_int_series(tt[\"trial_in_run\"])\n",
    "\n",
    "    if \"number\" not in tt.columns:\n",
    "        c = _first_col(tt, [\"sound_id\", \"SoundID\", \"sound\", \"stim_id\", \"stimulus_id\"])\n",
    "        if c:\n",
    "            tt[\"number\"] = tt[c]\n",
    "    if \"number\" in tt.columns and not pd.api.types.is_numeric_dtype(tt[\"number\"]):\n",
    "        nn = to_int_series(tt[\"number\"])\n",
    "        if nn.isna().mean() > 0.5:\n",
    "            nn = extract_any_int(tt[\"number\"])\n",
    "        tt[\"number\"] = nn\n",
    "    tt[\"number\"] = to_int_series(tt[\"number\"])\n",
    "\n",
    "    if \"カテゴリー\" not in tt.columns:\n",
    "        c = _first_col(tt, [\"category\", \"Category\", \"カテゴリ\"])\n",
    "        if c:\n",
    "            tt[\"カテゴリー\"] = tt[c]\n",
    "\n",
    "    keep = [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\"]\n",
    "    if \"カテゴリー\" in tt.columns:\n",
    "        keep.append(\"カテゴリー\")\n",
    "    tt = tt[keep].copy()\n",
    "    # uniqueness for merge keys\n",
    "    return tt\n",
    "\n",
    "def load_master_sound_pc(cfg: Config) -> pd.DataFrame:\n",
    "    ms = safe_read_csv(cfg.master_sound_pc_csv)\n",
    "    if \"number\" not in ms.columns:\n",
    "        c = _first_col(ms, [\"sound_id\", \"SoundID\", \"sound\", \"stim_id\"])\n",
    "        if c:\n",
    "            ms[\"number\"] = ms[c]\n",
    "    ms[\"number\"] = to_int_series(ms[\"number\"])\n",
    "    return ms\n",
    "\n",
    "def add_highlow_and_ambiguous(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # continuous emo -> high/low median split\n",
    "    for col in [\"emo_arousal\", \"emo_approach\", \"emo_valence\"]:\n",
    "        if col in df.columns and f\"{col}_high\" not in df.columns:\n",
    "            med = float(pd.to_numeric(df[col], errors=\"coerce\").median())\n",
    "            df[f\"{col}_high\"] = (pd.to_numeric(df[col], errors=\"coerce\") >= med).astype(int)\n",
    "\n",
    "    # ambiguous\n",
    "    if \"is_ambiguous\" not in df.columns:\n",
    "        if \"is_ambiguous_approach_sd_top10\" in df.columns:\n",
    "            x = df[\"is_ambiguous_approach_sd_top10\"]\n",
    "            if x.dtype != bool:\n",
    "                x = x.fillna(False).astype(bool)\n",
    "            df[\"is_ambiguous\"] = x.astype(int)\n",
    "        else:\n",
    "            df[\"is_ambiguous\"] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Metadata recovery: subject_id, run_id, trial_in_run, number\n",
    "# ============================================================\n",
    "\n",
    "def ensure_keys_in_metadata(meta: pd.DataFrame, tt: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    epochs.metadata 行数を変えずに keys を復元する（dropしない）\n",
    "    keys: subject_id, run_id, trial_in_run, number\n",
    "    \"\"\"\n",
    "    meta = meta.copy()\n",
    "    n = len(meta)\n",
    "    meta[\"_row_id\"] = np.arange(n)\n",
    "\n",
    "    # subject_id\n",
    "    if \"subject_id\" not in meta.columns:\n",
    "        c = _first_col(meta, [\"subject_id\", \"subject\", \"Subject\", \"participant\", \"participant_id\", \"sub\", \"sub_id\"])\n",
    "        if c:\n",
    "            meta[\"subject_id\"] = meta[c]\n",
    "    if \"subject_id\" in meta.columns and not pd.api.types.is_numeric_dtype(meta[\"subject_id\"]):\n",
    "        sid = to_int_series(meta[\"subject_id\"])\n",
    "        if sid.isna().mean() > 0.5:\n",
    "            sid = extract_int_prefix(meta[\"subject_id\"])\n",
    "        meta[\"subject_id\"] = sid\n",
    "    if \"subject_id\" in meta.columns:\n",
    "        meta[\"subject_id\"] = to_int_series(meta[\"subject_id\"])\n",
    "\n",
    "    # run_id\n",
    "    if \"run_id\" not in meta.columns:\n",
    "        c = _first_col(meta, [\"run_id\", \"run\", \"Run\", \"block\", \"session\"])\n",
    "        if c:\n",
    "            meta[\"run_id\"] = meta[c]\n",
    "    if \"run_id\" in meta.columns and not pd.api.types.is_numeric_dtype(meta[\"run_id\"]):\n",
    "        meta[\"run_id\"] = extract_any_int(meta[\"run_id\"])\n",
    "    if \"run_id\" in meta.columns:\n",
    "        meta[\"run_id\"] = to_int_series(meta[\"run_id\"])\n",
    "\n",
    "    # trial_in_run\n",
    "    if \"trial_in_run\" not in meta.columns:\n",
    "        c = _first_col(meta, [\"trial_in_run\", \"trial\", \"Trial\", \"trial_num\", \"trial_index\"])\n",
    "        if c:\n",
    "            meta[\"trial_in_run\"] = meta[c]\n",
    "    if \"trial_in_run\" in meta.columns and not pd.api.types.is_numeric_dtype(meta[\"trial_in_run\"]):\n",
    "        meta[\"trial_in_run\"] = extract_any_int(meta[\"trial_in_run\"])\n",
    "    if \"trial_in_run\" in meta.columns:\n",
    "        meta[\"trial_in_run\"] = to_int_series(meta[\"trial_in_run\"])\n",
    "\n",
    "    # number\n",
    "    if \"number\" not in meta.columns:\n",
    "        c = _first_col(meta, [\"number\", \"sound_id\", \"SoundID\", \"sound\", \"stim_id\", \"stimulus_id\"])\n",
    "        if c:\n",
    "            meta[\"number\"] = meta[c]\n",
    "    if \"number\" in meta.columns and not pd.api.types.is_numeric_dtype(meta[\"number\"]):\n",
    "        nn = to_int_series(meta[\"number\"])\n",
    "        if nn.isna().mean() > 0.5:\n",
    "            nn = extract_any_int(meta[\"number\"])\n",
    "        meta[\"number\"] = nn\n",
    "    if \"number\" in meta.columns:\n",
    "        meta[\"number\"] = to_int_series(meta[\"number\"])\n",
    "\n",
    "    # merge with trial_table to fill missing keys (strong: subject_id+run_id+trial_in_run)\n",
    "    keys = [\"subject_id\", \"run_id\", \"trial_in_run\"]\n",
    "    if all(k in meta.columns for k in keys):\n",
    "        tmp = meta.merge(\n",
    "            tt[keys + [\"number\", \"カテゴリー\"]].drop_duplicates(keys),\n",
    "            on=keys,\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_tt\"),\n",
    "        )\n",
    "        if \"number_tt\" in tmp.columns:\n",
    "            if \"number\" in tmp.columns:\n",
    "                tmp[\"number\"] = tmp[\"number\"].fillna(tmp[\"number_tt\"])\n",
    "            else:\n",
    "                tmp[\"number\"] = tmp[\"number_tt\"]\n",
    "        if \"カテゴリー_tt\" in tmp.columns and \"カテゴリー\" not in tmp.columns:\n",
    "            tmp[\"カテゴリー\"] = tmp[\"カテゴリー_tt\"]\n",
    "        meta = tmp.drop(columns=[c for c in [\"number_tt\", \"カテゴリー_tt\"] if c in tmp.columns])\n",
    "\n",
    "    # final fallback: row align only if N一致\n",
    "    needed = [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\"]\n",
    "    if any((k not in meta.columns) for k in needed) or any(meta[k].isna().all() for k in needed if k in meta.columns):\n",
    "        if len(meta) == len(tt):\n",
    "            logging.warning(\"[meta-fix] fallback row-align: epochs order assumed identical to trial_table (N一致).\")\n",
    "            tt2 = tt.reset_index(drop=True)\n",
    "            meta = meta.reset_index(drop=True)\n",
    "            for k in [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\"]:\n",
    "                meta[k] = tt2[k].values\n",
    "            if \"カテゴリー\" in tt2.columns and \"カテゴリー\" not in meta.columns:\n",
    "                meta[\"カテゴリー\"] = tt2[\"カテゴリー\"].values\n",
    "        else:\n",
    "            raise RuntimeError(\"[meta-fix] cannot recover keys (merge failed and N mismatch).\")\n",
    "\n",
    "    # validate\n",
    "    for k in [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\"]:\n",
    "        if k not in meta.columns:\n",
    "            raise RuntimeError(f\"[meta-fix] missing {k} in metadata. cols={list(meta.columns)}\")\n",
    "        if meta[k].isna().any():\n",
    "            raise RuntimeError(f\"[meta-fix] still NA in {k}: {int(meta[k].isna().sum())}/{len(meta)}\")\n",
    "\n",
    "    meta = meta.drop(columns=[\"_row_id\"], errors=\"ignore\")\n",
    "    return meta\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EEG channel canonicalization (19ch)\n",
    "# ============================================================\n",
    "\n",
    "CANON_19 = [\"Fp1\",\"Fp2\",\"F7\",\"F3\",\"Fz\",\"F4\",\"F8\",\"T7\",\"C3\",\"Cz\",\"C4\",\"T8\",\"P7\",\"P3\",\"Pz\",\"P4\",\"P8\",\"O1\",\"O2\"]\n",
    "\n",
    "def _norm_ch(name: str) -> str:\n",
    "    s = name.upper()\n",
    "    s = s.replace(\"EEG\", \"\")\n",
    "    s = s.replace(\" \", \"\")\n",
    "    s = s.replace(\"-REF\", \"\").replace(\"REF\", \"\")\n",
    "    s = s.replace(\"-\", \"\")\n",
    "    # 旧命名互換\n",
    "    s = s.replace(\"T3\", \"T7\").replace(\"T4\", \"T8\").replace(\"T5\", \"P7\").replace(\"T6\", \"P8\")\n",
    "    return s\n",
    "\n",
    "def pick_and_rename_19ch(epochs: mne.Epochs) -> mne.Epochs:\n",
    "    # pick EEG only\n",
    "    try:\n",
    "        epochs = epochs.copy().pick_types(eeg=True, eog=False, stim=False, misc=False)\n",
    "    except Exception:\n",
    "        epochs = epochs.copy().pick(\"eeg\")\n",
    "\n",
    "    cur = list(epochs.ch_names)\n",
    "    norm_map = {_norm_ch(c): c for c in cur}\n",
    "    rename = {}\n",
    "    keep = []\n",
    "    for canon in CANON_19:\n",
    "        key = canon.upper()\n",
    "        if key in norm_map:\n",
    "            orig = norm_map[key]\n",
    "            rename[orig] = canon\n",
    "            keep.append(orig)\n",
    "\n",
    "    if not keep:\n",
    "        raise RuntimeError(f\"[channels] none of 19ch matched. available={cur}\")\n",
    "\n",
    "    epochs = epochs.copy().pick_channels(keep, ordered=True)\n",
    "    epochs.rename_channels(rename)\n",
    "    ordered = [c for c in CANON_19 if c in epochs.ch_names]\n",
    "    epochs = epochs.copy().reorder_channels(ordered)\n",
    "    return epochs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature extraction: ERP + TFR\n",
    "# ============================================================\n",
    "\n",
    "def compute_erp_features(epochs: mne.Epochs, windows_ms: List[Tuple[int,int]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ERP: channel × time-window mean amplitude\n",
    "    features: ERP_<ch>_<t0>_<t1>ms\n",
    "    ※ DataFrame断片化を避けるため、最後に一括concat\n",
    "    \"\"\"\n",
    "    data = epochs.get_data()  # (n, ch, t)\n",
    "    times_ms = epochs.times * 1000.0\n",
    "    chs = epochs.ch_names\n",
    "\n",
    "    md = epochs.metadata.reset_index(drop=True).copy()\n",
    "    col_names: List[str] = []\n",
    "    col_arrays: List[np.ndarray] = []\n",
    "\n",
    "    for (t0, t1) in windows_ms:\n",
    "        m = (times_ms >= t0) & (times_ms < t1)\n",
    "        if not m.any():\n",
    "            continue\n",
    "        seg = data[:, :, m].mean(axis=2)  # (n, ch)\n",
    "        for ci, ch in enumerate(chs):\n",
    "            col_names.append(f\"ERP_{ch}_{t0}_{t1}ms\")\n",
    "            col_arrays.append(seg[:, ci])\n",
    "\n",
    "    if not col_arrays:\n",
    "        return md\n",
    "\n",
    "    feat = np.column_stack(col_arrays)\n",
    "    feat_df = pd.DataFrame(feat, columns=col_names)\n",
    "    return pd.concat([md, feat_df], axis=1)\n",
    "\n",
    "def make_rois_from_19ch(ch_names: List[str]) -> Dict[str, List[str]]:\n",
    "    rois = {\n",
    "        \"frontal\":  [c for c in [\"F3\",\"F4\",\"F7\",\"F8\",\"Fp1\",\"Fp2\",\"Fz\"] if c in ch_names],\n",
    "        \"central\":  [c for c in [\"C3\",\"C4\",\"Cz\"] if c in ch_names],\n",
    "        \"parietal\": [c for c in [\"P3\",\"P4\",\"Pz\",\"P7\",\"P8\"] if c in ch_names],\n",
    "        \"occipital\":[c for c in [\"O1\",\"O2\"] if c in ch_names],\n",
    "    }\n",
    "    return {k:v for k,v in rois.items() if len(v)>0}\n",
    "\n",
    "def compute_tfr_features(\n",
    "    epochs: mne.Epochs,\n",
    "    rois: Dict[str,List[str]],\n",
    "    bands: List[Tuple[str,int,int]],\n",
    "    windows_ms: List[Tuple[int,int]],\n",
    "    baseline: Tuple[float, float],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TFR: ROI平均（仮想1ch）→ Morlet → baseline ratio → band×time-window平均\n",
    "    features: TFR_<band>_<roi>_<t0>_<t1>ms\n",
    "    \"\"\"\n",
    "    from mne.time_frequency import tfr_morlet\n",
    "\n",
    "    md_base = epochs.metadata.reset_index(drop=True).copy()\n",
    "    n = len(epochs)\n",
    "\n",
    "    # freqs: union of bands (step=2Hz)\n",
    "    fmin = min(b[1] for b in bands)\n",
    "    fmax = max(b[2] for b in bands)\n",
    "    freqs = np.arange(float(fmin), float(fmax) + 0.1, 2.0)\n",
    "    n_cycles = freqs / 2.0\n",
    "\n",
    "    feat_names: List[str] = []\n",
    "    feat_arrays: List[np.ndarray] = []\n",
    "\n",
    "    for roi, chs in rois.items():\n",
    "        ep = epochs.copy().pick_channels(chs)\n",
    "        data = ep.get_data().mean(axis=1, keepdims=True).astype(np.float32)  # (n,1,t)\n",
    "\n",
    "        info = mne.create_info([f\"ROI_{roi}\"], sfreq=ep.info[\"sfreq\"], ch_types=[\"eeg\"])\n",
    "        events = np.c_[np.arange(n), np.zeros(n, dtype=int), np.ones(n, dtype=int)]\n",
    "        ep_roi = mne.EpochsArray(data, info, events=events, tmin=ep.tmin, verbose=\"ERROR\")\n",
    "\n",
    "        power = tfr_morlet(\n",
    "            ep_roi,\n",
    "            freqs=freqs,\n",
    "            n_cycles=n_cycles,\n",
    "            use_fft=True,\n",
    "            return_itc=False,\n",
    "            average=False,\n",
    "            decim=5,\n",
    "            n_jobs=1,\n",
    "            verbose=\"ERROR\",\n",
    "        )\n",
    "        try:\n",
    "            power.apply_baseline(baseline=baseline, mode=\"ratio\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        pow_data = power.data[:, 0, :, :]  # (n, f, t)\n",
    "        times_ms = power.times * 1000.0\n",
    "\n",
    "        for (bname, f0, f1) in bands:\n",
    "            fm = (freqs >= f0) & (freqs <= f1)\n",
    "            if not fm.any():\n",
    "                continue\n",
    "            for (t0, t1) in windows_ms:\n",
    "                tm = (times_ms >= t0) & (times_ms < t1)\n",
    "                if not tm.any():\n",
    "                    continue\n",
    "                vals = pow_data[:, fm][:, :, tm].mean(axis=(1, 2))\n",
    "                feat_names.append(f\"TFR_{bname}_{roi}_{t0}_{t1}ms\")\n",
    "                feat_arrays.append(vals)\n",
    "\n",
    "    if not feat_arrays:\n",
    "        return md_base\n",
    "\n",
    "    feat = np.column_stack(feat_arrays)\n",
    "    feat_df = pd.DataFrame(feat, columns=feat_names)\n",
    "    return pd.concat([md_base, feat_df], axis=1)\n",
    "\n",
    "def eeg_feature_cols(df: pd.DataFrame) -> List[str]:\n",
    "    return [c for c in df.columns if c.startswith(\"ERP_\") or c.startswith(\"TFR_\")]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# QC\n",
    "# ============================================================\n",
    "\n",
    "def load_qc_by_trial(cfg: Config) -> pd.DataFrame:\n",
    "    qc = safe_read_csv(cfg.qc_by_trial_csv)\n",
    "\n",
    "    # keys\n",
    "    if \"subject_id\" not in qc.columns:\n",
    "        c = _first_col(qc, [\"subject\", \"Subject\", \"participant\", \"participant_id\"])\n",
    "        if c:\n",
    "            qc[\"subject_id\"] = qc[c]\n",
    "    if \"subject_id\" in qc.columns and not pd.api.types.is_numeric_dtype(qc[\"subject_id\"]):\n",
    "        sid = to_int_series(qc[\"subject_id\"])\n",
    "        if sid.isna().mean() > 0.5:\n",
    "            sid = extract_int_prefix(qc[\"subject_id\"])\n",
    "        qc[\"subject_id\"] = sid\n",
    "    qc[\"subject_id\"] = to_int_series(qc[\"subject_id\"])\n",
    "\n",
    "    if \"run_id\" not in qc.columns:\n",
    "        c = _first_col(qc, [\"run\", \"Run\", \"block\", \"session\"])\n",
    "        if c:\n",
    "            qc[\"run_id\"] = qc[c]\n",
    "    if \"run_id\" in qc.columns and not pd.api.types.is_numeric_dtype(qc[\"run_id\"]):\n",
    "        qc[\"run_id\"] = extract_any_int(qc[\"run_id\"])\n",
    "    qc[\"run_id\"] = to_int_series(qc[\"run_id\"])\n",
    "\n",
    "    if \"trial_in_run\" not in qc.columns:\n",
    "        c = _first_col(qc, [\"trial_in_run\", \"trial\", \"Trial\", \"trial_num\", \"trial_index\"])\n",
    "        if c:\n",
    "            qc[\"trial_in_run\"] = qc[c]\n",
    "    qc[\"trial_in_run\"] = to_int_series(qc[\"trial_in_run\"])\n",
    "\n",
    "    # pass col auto-detect\n",
    "    pass_col = cfg.qc_pass_col\n",
    "    if not pass_col:\n",
    "        for cand in [\"qc_pass\", \"qc_autoreject_pass\", \"pass\", \"is_pass\"]:\n",
    "            if cand in qc.columns:\n",
    "                pass_col = cand\n",
    "                break\n",
    "    if not pass_col or pass_col not in qc.columns:\n",
    "        raise RuntimeError(f\"[QC] pass column not found. candidates: qc_pass/qc_autoreject_pass. cols={list(qc.columns)}\")\n",
    "\n",
    "    qc[\"qc_pass\"] = qc[pass_col].astype(bool)\n",
    "\n",
    "    keep = [\"subject_id\", \"run_id\", \"trial_in_run\", \"qc_pass\"]\n",
    "    qc = qc[keep].copy()\n",
    "    qc = qc.drop_duplicates([\"subject_id\", \"run_id\", \"trial_in_run\"])\n",
    "    return qc\n",
    "\n",
    "def apply_qc_filter(df: pd.DataFrame, qc: pd.DataFrame) -> pd.DataFrame:\n",
    "    keys = [\"subject_id\", \"run_id\", \"trial_in_run\"]\n",
    "    for k in keys:\n",
    "        if k not in df.columns:\n",
    "            raise RuntimeError(f\"[QC] feature df missing key {k}. cols={list(df.columns)}\")\n",
    "\n",
    "    # ★衝突回避：df側にqc_passが既にあるなら捨てる（QCファイルを正とする）\n",
    "    df2 = df.drop(columns=[\"qc_pass\"], errors=\"ignore\").copy()\n",
    "\n",
    "    # ★サフィックスを明示して、qc側の判定列を必ず拾えるようにする\n",
    "    merged = df2.merge(qc, on=keys, how=\"left\", suffixes=(\"\", \"_qc\"))\n",
    "\n",
    "    # ここで qc_pass が無いケースも潰す（保険）\n",
    "    if \"qc_pass\" not in merged.columns:\n",
    "        if \"qc_pass_qc\" in merged.columns:\n",
    "            merged[\"qc_pass\"] = merged[\"qc_pass_qc\"]\n",
    "        else:\n",
    "            raise RuntimeError(f\"[QC] qc_pass missing after merge. cols={list(merged.columns)}\")\n",
    "\n",
    "    matched = int(merged[\"qc_pass\"].notna().sum())\n",
    "    kept = int((merged[\"qc_pass\"] == True).sum())\n",
    "    logging.info(f\"[QC] keys={keys} | matched={matched}/{len(merged)} | kept(pass)={kept}/{len(merged)}\")\n",
    "\n",
    "    merged = merged.loc[merged[\"qc_pass\"] == True].drop(columns=[\"qc_pass\"], errors=\"ignore\")\n",
    "    merged = merged.drop(columns=[\"qc_pass_qc\"], errors=\"ignore\")\n",
    "    return merged.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build / Load trial features\n",
    "# ============================================================\n",
    "\n",
    "def load_or_build_features(cfg: Config, trial_features_in: Optional[Path], reuse_built: bool) -> pd.DataFrame:\n",
    "    tt = load_trial_table(cfg)\n",
    "    ms = load_master_sound_pc(cfg)\n",
    "\n",
    "    # 0) reuse built\n",
    "    if reuse_built and cfg.trial_features_csv_out.exists() and (trial_features_in is None or not trial_features_in.exists()):\n",
    "        logging.info(f\"[LOAD] reuse built trial_features: {cfg.trial_features_csv_out}\")\n",
    "        df = pd.read_csv(cfg.trial_features_csv_out)\n",
    "        df = add_highlow_and_ambiguous(df)\n",
    "        df = normalize_category_column(df)\n",
    "        return df\n",
    "\n",
    "    # 1) if provided CSV exists\n",
    "    if trial_features_in and trial_features_in.exists():\n",
    "        logging.info(f\"[LOAD] trial_features_csv_in: {trial_features_in}\")\n",
    "        df = pd.read_csv(trial_features_in)\n",
    "        # minimal key normalization\n",
    "        if \"subject_id\" not in df.columns:\n",
    "            c = _first_col(df, [\"subject\", \"Subject\", \"participant\", \"participant_id\", \"sub\", \"sub_id\"])\n",
    "            if c:\n",
    "                df[\"subject_id\"] = df[c]\n",
    "        if \"subject_id\" in df.columns and not pd.api.types.is_numeric_dtype(df[\"subject_id\"]):\n",
    "            sid = to_int_series(df[\"subject_id\"])\n",
    "            if sid.isna().mean() > 0.5:\n",
    "                sid = extract_int_prefix(df[\"subject_id\"])\n",
    "            df[\"subject_id\"] = sid\n",
    "        df[\"subject_id\"] = to_int_series(df[\"subject_id\"])\n",
    "\n",
    "        if \"run_id\" in df.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(df[\"run_id\"]):\n",
    "                df[\"run_id\"] = extract_any_int(df[\"run_id\"])\n",
    "            df[\"run_id\"] = to_int_series(df[\"run_id\"])\n",
    "        if \"trial_in_run\" in df.columns:\n",
    "            df[\"trial_in_run\"] = to_int_series(extract_any_int(df[\"trial_in_run\"]) if not pd.api.types.is_numeric_dtype(df[\"trial_in_run\"]) else df[\"trial_in_run\"])\n",
    "\n",
    "        if \"number\" not in df.columns:\n",
    "            c = _first_col(df, [\"sound_id\", \"SoundID\", \"sound\", \"stim_id\", \"stimulus_id\"])\n",
    "            if c:\n",
    "                df[\"number\"] = df[c]\n",
    "        if \"number\" in df.columns and not pd.api.types.is_numeric_dtype(df[\"number\"]):\n",
    "            nn = to_int_series(df[\"number\"])\n",
    "            if nn.isna().mean() > 0.5:\n",
    "                nn = extract_any_int(df[\"number\"])\n",
    "            df[\"number\"] = nn\n",
    "        df[\"number\"] = to_int_series(df[\"number\"])\n",
    "\n",
    "        # attach category/keys from trial_table if possible\n",
    "        if all(k in df.columns for k in [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\"]):\n",
    "            df = df.merge(tt, on=[\"subject_id\",\"run_id\",\"trial_in_run\",\"number\"], how=\"left\")\n",
    "        else:\n",
    "            tmp = tt[[\"subject_id\",\"number\",\"カテゴリー\"]].drop_duplicates([\"subject_id\",\"number\"])\n",
    "            if \"カテゴリー\" not in df.columns:\n",
    "                df = df.merge(tmp, on=[\"subject_id\",\"number\"], how=\"left\")\n",
    "\n",
    "        # attach master targets\n",
    "        keep = [\"number\"]\n",
    "        for c in [\"emo_arousal\",\"emo_approach\",\"emo_valence\",\"PC1_emotion\",\"PC2_emotion\",\"PC3_emotion\",\"is_ambiguous_approach_sd_top10\"]:\n",
    "            if c in ms.columns:\n",
    "                keep.append(c)\n",
    "        df = df.merge(ms[keep].drop_duplicates(\"number\"), on=\"number\", how=\"left\")\n",
    "        df = add_highlow_and_ambiguous(df)\n",
    "        df = normalize_category_column(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # 2) build from epochs_all\n",
    "    if not cfg.epochs_all_path.exists():\n",
    "        raise FileNotFoundError(f\"epochs_all not found: {cfg.epochs_all_path}\")\n",
    "\n",
    "    logging.info(f\"[BUILD] epochs_all: {cfg.epochs_all_path}\")\n",
    "    epochs = mne.read_epochs(cfg.epochs_all_path, preload=True, verbose=\"ERROR\")\n",
    "    epochs = pick_and_rename_19ch(epochs)\n",
    "\n",
    "    # resample\n",
    "    if abs(epochs.info[\"sfreq\"] - cfg.resample_sfreq) > 1e-6:\n",
    "        epochs.resample(cfg.resample_sfreq)\n",
    "\n",
    "    # crop\n",
    "    try:\n",
    "        epochs.crop(tmin=cfg.crop_tmin, tmax=cfg.crop_tmax, include_tmax=False)\n",
    "    except TypeError:\n",
    "        epochs.crop(tmin=cfg.crop_tmin, tmax=cfg.crop_tmax - (1.0 / epochs.info[\"sfreq\"]))\n",
    "\n",
    "    # baseline\n",
    "    try:\n",
    "        epochs.apply_baseline(baseline=(cfg.baseline_tmin, cfg.baseline_tmax))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # metadata keys\n",
    "    md = epochs.metadata.reset_index(drop=True).copy() if epochs.metadata is not None else pd.DataFrame(index=np.arange(len(epochs)))\n",
    "    md = ensure_keys_in_metadata(md, tt)\n",
    "    epochs.metadata = md  # length一致\n",
    "\n",
    "    # features\n",
    "    md_erp = compute_erp_features(epochs, cfg.erp_windows_ms)\n",
    "    rois = make_rois_from_19ch(epochs.ch_names)\n",
    "    md_tfr = compute_tfr_features(\n",
    "        epochs,\n",
    "        rois=rois,\n",
    "        bands=cfg.bands,\n",
    "        windows_ms=cfg.tfr_windows_ms,\n",
    "        baseline=(cfg.baseline_tmin, cfg.baseline_tmax),\n",
    "    )\n",
    "\n",
    "    df = md_erp.copy()\n",
    "    # add TFR cols (一括代入)\n",
    "    tfr_cols = [c for c in md_tfr.columns if c.startswith(\"TFR_\")]\n",
    "    if tfr_cols:\n",
    "        df = pd.concat([df, md_tfr[tfr_cols].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # attach trial_table category (強結合)\n",
    "    df = df.merge(tt, on=[\"subject_id\",\"run_id\",\"trial_in_run\",\"number\"], how=\"left\")\n",
    "\n",
    "    # attach master targets\n",
    "    keep = [\"number\"]\n",
    "    for c in [\"emo_arousal\",\"emo_approach\",\"emo_valence\",\"PC1_emotion\",\"PC2_emotion\",\"PC3_emotion\",\"is_ambiguous_approach_sd_top10\"]:\n",
    "        if c in ms.columns:\n",
    "            keep.append(c)\n",
    "    df = df.merge(ms[keep].drop_duplicates(\"number\"), on=\"number\", how=\"left\")\n",
    "    df = add_highlow_and_ambiguous(df)\n",
    "    df = normalize_category_column(df)\n",
    "    df.to_csv(cfg.trial_features_csv_out, index=False)\n",
    "    logging.info(f\"[SAVE] built trial_features: {cfg.trial_features_csv_out}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Task specs\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TaskSpec:\n",
    "    name: str\n",
    "    kind: str  # \"regression\" | \"binary\" | \"multiclass\"\n",
    "    target_col: str\n",
    "    n_classes: Optional[int] = None\n",
    "\n",
    "def build_task_specs(df: pd.DataFrame, tasks: List[str]) -> List[TaskSpec]:\n",
    "    specs: List[TaskSpec] = []\n",
    "    for t in tasks:\n",
    "        if t in [\"emo_arousal\",\"emo_approach\",\"emo_valence\",\"PC1_emotion\",\"PC2_emotion\",\"PC3_emotion\"]:\n",
    "            if t in df.columns:\n",
    "                specs.append(TaskSpec(name=t, kind=\"regression\", target_col=t))\n",
    "        elif t.endswith(\"_high\") or t == \"is_ambiguous\":\n",
    "            if t in df.columns:\n",
    "                specs.append(TaskSpec(name=t, kind=\"binary\", target_col=t))\n",
    "        elif t == \"category_3\":\n",
    "            if \"カテゴリー\" in df.columns:\n",
    "                specs.append(TaskSpec(name=\"category_3\", kind=\"multiclass\", target_col=\"カテゴリー\", n_classes=3))\n",
    "    return specs\n",
    "\n",
    "def encode_category_3(y: pd.Series) -> np.ndarray:\n",
    "    # 不快=0, 中間=1, 快=2\n",
    "    m = {\"不快\":0, \"中間\":1, \"快\":2}\n",
    "    yy = y.astype(str).map(m)\n",
    "    return pd.to_numeric(yy, errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Models (robust across sklearn versions)\n",
    "# ============================================================\n",
    "\n",
    "def make_logreg(**kwargs) -> LogisticRegression:\n",
    "    sig = inspect.signature(LogisticRegression)\n",
    "    clean = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if k in sig.parameters:\n",
    "            clean[k] = v\n",
    "    return LogisticRegression(**clean)\n",
    "\n",
    "def make_mlp_reg(seed: int) -> Any:\n",
    "    if not _HAS_MLP:\n",
    "        return None\n",
    "    return MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-4,\n",
    "        random_state=seed_u32(seed),\n",
    "        max_iter=2000,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "    )\n",
    "\n",
    "def make_mlp_clf(seed: int) -> Any:\n",
    "    if not _HAS_MLP:\n",
    "        return None\n",
    "    return MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-4,\n",
    "        random_state=seed_u32(seed),\n",
    "        max_iter=2000,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOSO core\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    subject_id: int\n",
    "    score_primary: float\n",
    "    score_aux: float\n",
    "    n_test: int\n",
    "\n",
    "def _fit_predict_score_fold_linear(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_te: np.ndarray, y_te: np.ndarray,\n",
    "    kind: str,\n",
    "    rng_seed: int,\n",
    ") -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    primary:\n",
    "      - regression: Pearson r\n",
    "      - binary: AUC\n",
    "      - multiclass: balanced accuracy\n",
    "    aux:\n",
    "      - regression: R2\n",
    "      - multiclass: accuracy\n",
    "      - binary: nan\n",
    "    coef importance:\n",
    "      - regression: coef\n",
    "      - binary: coef\n",
    "      - multiclass: L2 norm across classes\n",
    "    \"\"\"\n",
    "    if kind == \"regression\":\n",
    "        model = RidgeCV(alphas=np.logspace(-3, 3, 13))\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        pred = pipe.predict(X_te)\n",
    "        r = pearsonr_safe(pred, y_te)\n",
    "        r2 = float(r2_score(y_te, pred)) if np.isfinite(pred).all() else np.nan\n",
    "        coef = pipe.named_steps[\"model\"].coef_.astype(float).ravel()\n",
    "        return float(r), float(r2), coef\n",
    "\n",
    "    if kind == \"binary\":\n",
    "        model = make_logreg(\n",
    "            max_iter=4000,\n",
    "            solver=\"liblinear\",\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=seed_u32(rng_seed),\n",
    "        )\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        prob = pipe.predict_proba(X_te)[:, 1]\n",
    "        try:\n",
    "            auc = float(roc_auc_score(y_te, prob))\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        coef = pipe.named_steps[\"model\"].coef_.astype(float).ravel()\n",
    "        return float(auc), float(\"nan\"), coef\n",
    "\n",
    "    # multiclass\n",
    "    model = make_logreg(\n",
    "        max_iter=5000,\n",
    "        solver=\"lbfgs\",\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=seed_u32(rng_seed),\n",
    "    )\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_te)\n",
    "    bacc = float(balanced_accuracy_score(y_te, pred))\n",
    "    acc = float(accuracy_score(y_te, pred))\n",
    "    coef = pipe.named_steps[\"model\"].coef_.astype(float)  # (K,F)\n",
    "    coef_vec = np.linalg.norm(coef, axis=0)\n",
    "   \n",
    "    return bacc, acc, coef_vec\n",
    "def fit_linear_model(X_tr, y_tr, kind: str, seed: int = 42):\n",
    "    \"\"\"\n",
    "    temporal_generalization 用の公開API\n",
    "    戻り値: 学習済み sklearn Pipeline (StandardScaler + model)\n",
    "    \"\"\"\n",
    "    if kind == \"regression\":\n",
    "        model = RidgeCV(alphas=np.logspace(-3, 3, 13))\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        return pipe\n",
    "\n",
    "    if kind == \"binary\":\n",
    "        model = make_logreg(\n",
    "            max_iter=4000, solver=\"liblinear\",\n",
    "            class_weight=\"balanced\", random_state=seed_u32(seed),\n",
    "        )\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        return pipe\n",
    "\n",
    "    # multiclass\n",
    "    model = make_logreg(\n",
    "        max_iter=5000, solver=\"lbfgs\",\n",
    "        class_weight=\"balanced\", random_state=seed_u32(seed),\n",
    "    )\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    return pipe\n",
    "\n",
    "def _fit_predict_score_fold_mlp(\n",
    "    X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "    X_te: np.ndarray, y_te: np.ndarray,\n",
    "    kind: str,\n",
    "    rng_seed: int,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    MLPは“AIっぽさ用の補助結果”。\n",
    "    - regression: primary=r, aux=R2\n",
    "    - binary: primary=AUC, aux=nan\n",
    "    - multiclass: primary=bAcc, aux=acc\n",
    "    \"\"\"\n",
    "    if not _HAS_MLP:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    if kind == \"regression\":\n",
    "        model = make_mlp_reg(rng_seed)\n",
    "        if model is None:\n",
    "            return np.nan, np.nan\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        pred = pipe.predict(X_te)\n",
    "        r = pearsonr_safe(pred, y_te)\n",
    "        r2 = float(r2_score(y_te, pred)) if np.isfinite(pred).all() else np.nan\n",
    "        return float(r), float(r2)\n",
    "\n",
    "    if kind == \"binary\":\n",
    "        model = make_mlp_clf(rng_seed)\n",
    "        if model is None:\n",
    "            return np.nan, np.nan\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        prob = pipe.predict_proba(X_te)[:, 1]\n",
    "        try:\n",
    "            auc = float(roc_auc_score(y_te, prob))\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        return float(auc), float(\"nan\")\n",
    "\n",
    "    model = make_mlp_clf(rng_seed)\n",
    "    if model is None:\n",
    "        return np.nan, np.nan\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", model)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_te)\n",
    "    bacc = float(balanced_accuracy_score(y_te, pred))\n",
    "    acc = float(accuracy_score(y_te, pred))\n",
    "    return float(bacc), float(acc)\n",
    "\n",
    "def loso_evaluate_linear(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    kind: str,\n",
    "    feature_names: List[str],\n",
    "    seed: int,\n",
    ") -> Tuple[List[FoldResult], pd.DataFrame]:\n",
    "    logo = LeaveOneGroupOut()\n",
    "    fold_rows: List[FoldResult] = []\n",
    "\n",
    "    imp_accum = np.zeros(len(feature_names), dtype=float)\n",
    "    imp_n = 0\n",
    "\n",
    "    for tr_idx, te_idx in logo.split(X, y, groups=groups):\n",
    "        sid = int(groups[te_idx][0])\n",
    "        rs = seed_u32(seed + sid)\n",
    "\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        # validity check\n",
    "        if kind in [\"binary\", \"multiclass\"]:\n",
    "            if np.unique(y_tr).size < 2:\n",
    "                fold_rows.append(FoldResult(subject_id=sid, score_primary=np.nan, score_aux=np.nan, n_test=int(len(te_idx))))\n",
    "                continue\n",
    "            if kind == \"binary\" and np.unique(y_te).size < 2:\n",
    "                primary, aux, coef = _fit_predict_score_fold_linear(X_tr, y_tr, X_te, y_te, kind, rs)\n",
    "                primary = np.nan\n",
    "            else:\n",
    "                primary, aux, coef = _fit_predict_score_fold_linear(X_tr, y_tr, X_te, y_te, kind, rs)\n",
    "        else:\n",
    "            primary, aux, coef = _fit_predict_score_fold_linear(X_tr, y_tr, X_te, y_te, kind, rs)\n",
    "\n",
    "        fold_rows.append(FoldResult(subject_id=sid, score_primary=float(primary), score_aux=float(aux), n_test=int(len(te_idx))))\n",
    "        if np.all(np.isfinite(coef)):\n",
    "            imp_accum += np.abs(coef)\n",
    "            imp_n += 1\n",
    "\n",
    "    imp = (imp_accum / max(1, imp_n)).astype(float)\n",
    "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance_abscoef\": imp})\n",
    "    imp_df = imp_df.sort_values(\"importance_abscoef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return fold_rows, imp_df\n",
    "\n",
    "def loso_evaluate_mlp(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    kind: str,\n",
    "    seed: int,\n",
    ") -> List[FoldResult]:\n",
    "    logo = LeaveOneGroupOut()\n",
    "    fold_rows: List[FoldResult] = []\n",
    "    for tr_idx, te_idx in logo.split(X, y, groups=groups):\n",
    "        sid = int(groups[te_idx][0])\n",
    "        rs = seed_u32(seed + 100000 + sid)\n",
    "\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        if kind in [\"binary\", \"multiclass\"] and np.unique(y_tr).size < 2:\n",
    "            fold_rows.append(FoldResult(subject_id=sid, score_primary=np.nan, score_aux=np.nan, n_test=int(len(te_idx))))\n",
    "            continue\n",
    "        if kind == \"binary\" and np.unique(y_te).size < 2:\n",
    "            primary, aux = _fit_predict_score_fold_mlp(X_tr, y_tr, X_te, y_te, kind, rs)\n",
    "            primary = np.nan\n",
    "        else:\n",
    "            primary, aux = _fit_predict_score_fold_mlp(X_tr, y_tr, X_te, y_te, kind, rs)\n",
    "\n",
    "        fold_rows.append(FoldResult(subject_id=sid, score_primary=float(primary), score_aux=float(aux), n_test=int(len(te_idx))))\n",
    "    return fold_rows\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Permutation\n",
    "# ============================================================\n",
    "\n",
    "def permute_y(y: np.ndarray, groups: np.ndarray, rng: np.random.Generator, within_subject: bool) -> np.ndarray:\n",
    "    y2 = y.copy()\n",
    "    if within_subject:\n",
    "        for g in np.unique(groups):\n",
    "            idx = np.where(groups == g)[0]\n",
    "            y2[idx] = rng.permutation(y2[idx])\n",
    "    else:\n",
    "        y2 = rng.permutation(y2)\n",
    "    return y2\n",
    "\n",
    "def permutation_test_mean_score_linear(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    kind: str,\n",
    "    feature_names: List[str],\n",
    "    seed: int,\n",
    "    n_perm: int,\n",
    "    within_subject: bool,\n",
    "    n_jobs: int,\n",
    "    obs_mean: Optional[float] = None,   # ★追加\n",
    ") -> Tuple[float, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    returns: (p_perm, null, obs)\n",
    "    obs_mean を渡せば、観測値のためのLOSO再計算をスキップ（高速化）\n",
    "    \"\"\"\n",
    "    if obs_mean is None:\n",
    "        folds, _ = loso_evaluate_linear(X, y, groups, kind, feature_names, seed)\n",
    "        obs = float(np.nanmean([f.score_primary for f in folds]))\n",
    "    else:\n",
    "        obs = float(obs_mean)\n",
    "\n",
    "    def one_perm(i: int) -> float:\n",
    "        rng = np.random.default_rng(seed_u32(seed + 20000 + i))\n",
    "        yp = permute_y(y, groups, rng, within_subject)\n",
    "        fr, _ = loso_evaluate_linear(X, yp, groups, kind, feature_names, seed_u32(seed + 50000 + i))\n",
    "        return float(np.nanmean([f.score_primary for f in fr]))\n",
    "\n",
    "    null = Parallel(n_jobs=n_jobs)(delayed(one_perm)(i) for i in range(int(n_perm)))\n",
    "    null = np.asarray(null, dtype=float)\n",
    "    p = float((1 + np.sum(null >= obs)) / (1 + len(null)))\n",
    "    return p, null, obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Maps: feature subsets\n",
    "# ============================================================\n",
    "\n",
    "def select_erp_window_features(df: pd.DataFrame, t0: int, t1: int) -> List[str]:\n",
    "    suffix = f\"_{t0}_{t1}ms\"\n",
    "    return [c for c in df.columns if c.startswith(\"ERP_\") and c.endswith(suffix)]\n",
    "\n",
    "def select_tfr_band_window_features(df: pd.DataFrame, band_name: str, t0: int, t1: int) -> List[str]:\n",
    "    prefix = f\"TFR_{band_name}_\"\n",
    "    suffix = f\"_{t0}_{t1}ms\"\n",
    "    return [c for c in df.columns if c.startswith(prefix) and c.endswith(suffix)]\n",
    "\n",
    "# ============================================================\n",
    "# Plotting (optional)\n",
    "# ============================================================\n",
    "\n",
    "def plot_loso_subject_points(task_name: str, kind: str, fold_df: pd.DataFrame, out_png: Path) -> None:\n",
    "    y = fold_df[\"score_primary\"].to_numpy(dtype=float)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(len(y)) + 1, y)\n",
    "    plt.axhline(float(np.nanmean(y)), linestyle=\"--\")\n",
    "    plt.title(f\"LOSO performance: {task_name} ({kind})\")\n",
    "    plt.xlabel(\"Subject (fold index)\")\n",
    "    plt.ylabel(\"AUC\" if kind==\"binary\" else (\"bAcc\" if kind==\"multiclass\" else \"Pearson r\"))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    ensure_dir(out_png.parent)\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def plot_top_importances(task_name: str, imp_df: pd.DataFrame, out_png: Path, top_k: int = 20) -> None:\n",
    "    d = imp_df.head(top_k).copy().iloc[::-1]\n",
    "    plt.figure()\n",
    "    plt.barh(d[\"feature\"].astype(str), d[\"importance_abscoef\"].to_numpy(dtype=float))\n",
    "    plt.title(f\"Feature importance (|coef| mean): {task_name} top{top_k}\")\n",
    "    plt.xlabel(\"mean |coef|\")\n",
    "    plt.tight_layout()\n",
    "    ensure_dir(out_png.parent)\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Prepare X/y\n",
    "# ============================================================\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame, spec: TaskSpec, features: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    base_cols = [\"subject_id\", \"run_id\", \"trial_in_run\", \"number\", spec.target_col]\n",
    "    sub = df[base_cols + features].copy()\n",
    "    sub = sub.dropna()\n",
    "\n",
    "    sub[\"subject_id\"] = to_int_series(sub[\"subject_id\"])\n",
    "    groups = sub[\"subject_id\"].astype(int).to_numpy()\n",
    "    X = sub[features].to_numpy(dtype=float)\n",
    "\n",
    "    if spec.kind == \"regression\":\n",
    "        y = pd.to_numeric(sub[spec.target_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    elif spec.kind == \"binary\":\n",
    "        y = pd.to_numeric(sub[spec.target_col], errors=\"coerce\").astype(int).to_numpy()\n",
    "    else:\n",
    "        y0 = encode_category_3(sub[spec.target_col])\n",
    "        ok = np.isfinite(y0)\n",
    "        sub = sub.loc[ok].copy()\n",
    "        X = X[ok]\n",
    "        groups = groups[ok]\n",
    "        y = y0[ok].astype(int)\n",
    "\n",
    "    return X, y, groups, sub\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run one task (main results + optional maps + optional AI model)\n",
    "# ============================================================\n",
    "\n",
    "def run_task(cfg: Config, df: pd.DataFrame, spec: TaskSpec) -> None:\n",
    "    all_feats = eeg_feature_cols(df)\n",
    "    if len(all_feats) < 2:\n",
    "        logging.warning(f\"[task:{spec.name}] not enough EEG features -> skip\")\n",
    "        return\n",
    "\n",
    "    X, y, groups, sub = prepare_xy(df, spec, all_feats)\n",
    "    if len(np.unique(groups)) < 2:\n",
    "        logging.warning(f\"[task:{spec.name}] too few subjects -> skip\")\n",
    "        return\n",
    "    if spec.kind in [\"binary\",\"multiclass\"] and np.unique(y).size < 2:\n",
    "        logging.warning(f\"[task:{spec.name}] single class -> skip\")\n",
    "        return\n",
    "\n",
    "    # ---------- Linear (PRIMARY) ----------\n",
    "    folds, imp_df = loso_evaluate_linear(X, y, groups, spec.kind, all_feats, cfg.seed)\n",
    "    fold_df = pd.DataFrame([{\n",
    "        \"task\": spec.name,\n",
    "        \"kind\": spec.kind,\n",
    "        \"model\": \"linear\",\n",
    "        \"subject_id\": f.subject_id,\n",
    "        \"score_primary\": f.score_primary,\n",
    "        \"score_aux\": f.score_aux,\n",
    "        \"n_test\": f.n_test,\n",
    "    } for f in folds])\n",
    "    obs_mean = float(np.nanmean(fold_df[\"score_primary\"].to_numpy(dtype=float)))\n",
    "\n",
    "    p_perm, null, _obs = permutation_test_mean_score_linear(\n",
    "        X, y, groups, spec.kind, all_feats,\n",
    "        seed=cfg.seed,\n",
    "        n_perm=cfg.n_perm,\n",
    "        within_subject=cfg.perm_within_subject,\n",
    "        n_jobs=cfg.n_jobs,\n",
    "        obs_mean=obs_mean,  # ★ここ\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # save main tables\n",
    "    fold_df.to_csv(cfg.tables_dir / f\"moduleB_LOSO_folds_{spec.name}_linear.csv\", index=False)\n",
    "    imp_df.to_csv(cfg.tables_dir / f\"moduleB_importance_{spec.name}_linear.csv\", index=False)\n",
    "    np.save(cfg.tables_dir / f\"moduleB_null_{spec.name}_linear.npy\", null)\n",
    "\n",
    "    summary = pd.DataFrame([{\n",
    "        \"task\": spec.name,\n",
    "        \"kind\": spec.kind,\n",
    "        \"model\": \"linear\",\n",
    "        \"n_trials\": int(len(sub)),\n",
    "        \"n_subjects\": int(np.unique(groups).size),\n",
    "        \"n_features\": int(len(all_feats)),\n",
    "        \"score_mean\": obs_mean,\n",
    "        \"p_perm\": float(p_perm),\n",
    "        \"n_perm\": int(cfg.n_perm),\n",
    "        \"perm_within_subject\": bool(cfg.perm_within_subject),\n",
    "        \"apply_qc\": bool(cfg.apply_qc),\n",
    "        \"with_maps\": bool(cfg.with_maps),\n",
    "    }])\n",
    "    summary.to_csv(cfg.tables_dir / f\"moduleB_summary_{spec.name}_linear.csv\", index=False)\n",
    "\n",
    "    logging.info(f\"[task:{spec.name}] LINEAR mean={obs_mean:.4f} p_perm={p_perm:.4f}\")\n",
    "\n",
    "    # optional figures (軽いので主結果でも出してOK)\n",
    "    plot_loso_subject_points(spec.name, spec.kind, fold_df, cfg.figures_dir / f\"FIG1_LOSO_{spec.name}_linear.png\")\n",
    "    plot_top_importances(spec.name, imp_df, cfg.figures_dir / f\"FIG3_importance_{spec.name}_linear.png\", top_k=20)\n",
    "\n",
    "    # ---------- Optional: MLP (AIっぽい補助) ----------\n",
    "    if cfg.model_family == \"linear+mlp\":\n",
    "        if not _HAS_MLP:\n",
    "            logging.warning(\"[MLP] sklearn MLP not available -> skip\")\n",
    "        else:\n",
    "            folds_mlp = loso_evaluate_mlp(X, y, groups, spec.kind, cfg.seed)\n",
    "            fold_mlp_df = pd.DataFrame([{\n",
    "                \"task\": spec.name,\n",
    "                \"kind\": spec.kind,\n",
    "                \"model\": \"mlp\",\n",
    "                \"subject_id\": f.subject_id,\n",
    "                \"score_primary\": f.score_primary,\n",
    "                \"score_aux\": f.score_aux,\n",
    "                \"n_test\": f.n_test,\n",
    "            } for f in folds_mlp])\n",
    "            mlp_mean = float(np.nanmean(fold_mlp_df[\"score_primary\"].to_numpy(dtype=float)))\n",
    "            fold_mlp_df.to_csv(cfg.tables_dir / f\"moduleB_LOSO_folds_{spec.name}_mlp.csv\", index=False)\n",
    "            pd.DataFrame([{\n",
    "                \"task\": spec.name,\n",
    "                \"kind\": spec.kind,\n",
    "                \"model\": \"mlp\",\n",
    "                \"n_trials\": int(len(sub)),\n",
    "                \"n_subjects\": int(np.unique(groups).size),\n",
    "                \"score_mean\": mlp_mean,\n",
    "                \"note\": \"AI-like supplementary (not primary). No permutation by default.\",\n",
    "            }]).to_csv(cfg.tables_dir / f\"moduleB_summary_{spec.name}_mlp.csv\", index=False)\n",
    "            logging.info(f\"[task:{spec.name}] MLP (supp) mean={mlp_mean:.4f}\")\n",
    "\n",
    "        # ---------- Optional maps ----------\n",
    "    if not cfg.with_maps:\n",
    "        return\n",
    "    if cfg.map_tasks and (spec.name not in cfg.map_tasks):\n",
    "        return\n",
    "\n",
    "    logging.info(f\"[maps] task={spec.name} enabled | map_tasks={cfg.map_tasks or 'ALL'} | n_perm_map={cfg.n_perm_map}\")\n",
    "    logging.info(f\"[maps] eeg_feature example: {all_feats[:5]}\")\n",
    "\n",
    "    # ERP window map\n",
    "    erp_rows = []\n",
    "    pvals = []\n",
    "\n",
    "    for (t0, t1) in cfg.erp_windows_ms:\n",
    "        feats = select_erp_window_features(df, t0, t1)\n",
    "        if len(feats) < 2:\n",
    "            logging.info(f\"[maps-ERP] window {t0}-{t1}: feats={len(feats)} (skip)\")\n",
    "            continue\n",
    "\n",
    "        Xw, yw, gw, _subw = prepare_xy(df, spec, feats)\n",
    "        if len(np.unique(gw)) < 2:\n",
    "            continue\n",
    "        if spec.kind in [\"binary\",\"multiclass\"] and np.unique(yw).size < 2:\n",
    "            continue\n",
    "\n",
    "        fr, _ = loso_evaluate_linear(Xw, yw, gw, spec.kind, feats, cfg.seed)\n",
    "        mean_score = float(np.nanmean([f.score_primary for f in fr]))\n",
    "\n",
    "        local_seed = seed_u32(cfg.seed + 1000 + t0 + 17)\n",
    "        p_perm_w, _null_w, _obs_w = permutation_test_mean_score_linear(\n",
    "            Xw, yw, gw, spec.kind, feats,\n",
    "            seed=local_seed,\n",
    "            n_perm=cfg.n_perm_map,\n",
    "            within_subject=cfg.perm_within_subject,\n",
    "            n_jobs=cfg.n_jobs,\n",
    "            obs_mean=mean_score,  # ★入れておくと高速化にもなる\n",
    "        )\n",
    "\n",
    "\n",
    "        erp_rows.append({\n",
    "            \"task\": spec.name,\n",
    "            \"window\": f\"{t0}-{t1}\",\n",
    "            \"t0_ms\": t0,\n",
    "            \"t1_ms\": t1,\n",
    "            \"score\": mean_score,\n",
    "            \"p_perm\": float(p_perm_w),\n",
    "            \"n_perm\": int(cfg.n_perm_map),\n",
    "            \"n_features\": int(len(feats)),\n",
    "        })\n",
    "        pvals.append(float(p_perm_w))\n",
    "\n",
    "    if erp_rows:\n",
    "        erp_map = pd.DataFrame(erp_rows).sort_values([\"t0_ms\",\"t1_ms\"]).reset_index(drop=True)\n",
    "        rej, q = fdr_bh(np.asarray(pvals, dtype=float), alpha=0.05)\n",
    "        erp_map[\"q_fdr\"] = q\n",
    "        erp_map[\"significant_fdr\"] = rej\n",
    "        erp_map.to_csv(cfg.tables_dir / f\"moduleB_map_ERP_{spec.name}.csv\", index=False)\n",
    "\n",
    "    # TFR band × time map\n",
    "    tfr_rows = []\n",
    "    cell_p = []\n",
    "\n",
    "    for (bname, _f0, _f1) in cfg.bands:\n",
    "        bseed = stable_hash_u32(bname)\n",
    "        for (t0, t1) in cfg.tfr_windows_ms:\n",
    "            feats = select_tfr_band_window_features(df, bname, t0, t1)\n",
    "            if len(feats) < 2:\n",
    "                logging.info(f\"[maps-TFR] {bname} {t0}-{t1}: feats={len(feats)} (skip)\")\n",
    "                continue\n",
    "\n",
    "            Xw, yw, gw, _subw = prepare_xy(df, spec, feats)\n",
    "            if len(np.unique(gw)) < 2:\n",
    "                continue\n",
    "            if spec.kind in [\"binary\",\"multiclass\"] and np.unique(yw).size < 2:\n",
    "                continue\n",
    "\n",
    "            fr, _ = loso_evaluate_linear(Xw, yw, gw, spec.kind, feats, cfg.seed)\n",
    "            mean_score = float(np.nanmean([f.score_primary for f in fr]))\n",
    "\n",
    "            local_seed = seed_u32(cfg.seed + 2000 + t0 + int(bseed % 10000))\n",
    "            p_perm_w, _null_w, _obs_w = permutation_test_mean_score_linear(\n",
    "                Xw, yw, gw, spec.kind, feats,\n",
    "                seed=local_seed,\n",
    "                n_perm=cfg.n_perm_map,\n",
    "                within_subject=cfg.perm_within_subject,\n",
    "                n_jobs=cfg.n_jobs,\n",
    "                obs_mean=mean_score,\n",
    "            )\n",
    "\n",
    "\n",
    "            tfr_rows.append({\n",
    "                \"task\": spec.name,\n",
    "                \"band\": bname,\n",
    "                \"window\": f\"{t0}-{t1}\",\n",
    "                \"t0_ms\": t0,\n",
    "                \"t1_ms\": t1,\n",
    "                \"score\": mean_score,\n",
    "                \"p_perm\": float(p_perm_w),\n",
    "                \"n_perm\": int(cfg.n_perm_map),\n",
    "                \"n_features\": int(len(feats)),\n",
    "            })\n",
    "            cell_p.append(float(p_perm_w))\n",
    "\n",
    "    if tfr_rows:\n",
    "        tfr_map = pd.DataFrame(tfr_rows).sort_values([\"band\",\"t0_ms\",\"t1_ms\"]).reset_index(drop=True)\n",
    "        rej, q = fdr_bh(np.asarray(cell_p, dtype=float), alpha=0.05)\n",
    "        tfr_map[\"q_fdr\"] = q\n",
    "        tfr_map[\"significant_fdr\"] = rej\n",
    "        tfr_map.to_csv(cfg.tables_dir / f\"moduleB_map_TFR_{spec.name}.csv\", index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Metadata / snapshot\n",
    "# ============================================================\n",
    "\n",
    "def save_run_metadata(cfg: Config, df: pd.DataFrame, trial_features_in: Optional[Path]) -> None:\n",
    "    meta = {\n",
    "        \"module\": \"ModuleB_BF_EEGcore_final\",\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"root_dir\": str(cfg.root_dir),\n",
    "        \"inputs\": {\n",
    "            \"trial_features_csv_in\": str(trial_features_in) if trial_features_in else \"\",\n",
    "            \"epochs_all\": str(cfg.epochs_all_path),\n",
    "            \"trial_table\": str(cfg.trial_table_csv),\n",
    "            \"master_sound_pc\": str(cfg.master_sound_pc_csv),\n",
    "            \"qc_by_trial\": str(cfg.qc_by_trial_csv) if cfg.apply_qc else \"\",\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"out_dir\": str(cfg.out_dir),\n",
    "            \"tables\": str(cfg.tables_dir),\n",
    "            \"figures\": str(cfg.figures_dir),\n",
    "            \"logs\": str(cfg.logs_dir),\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"seed\": cfg.seed,\n",
    "            \"n_jobs\": cfg.n_jobs,\n",
    "            \"n_perm\": cfg.n_perm,\n",
    "            \"n_perm_map\": cfg.n_perm_map,\n",
    "            \"perm_within_subject\": cfg.perm_within_subject,\n",
    "            \"apply_qc\": cfg.apply_qc,\n",
    "            \"with_maps\": cfg.with_maps,\n",
    "            \"map_tasks\": cfg.map_tasks,\n",
    "            \"resample_sfreq\": cfg.resample_sfreq,\n",
    "            \"crop_tmin\": cfg.crop_tmin,\n",
    "            \"crop_tmax\": cfg.crop_tmax,\n",
    "            \"baseline\": [cfg.baseline_tmin, cfg.baseline_tmax],\n",
    "            \"erp_windows_ms\": cfg.erp_windows_ms,\n",
    "            \"tfr_windows_ms\": cfg.tfr_windows_ms,\n",
    "            \"bands\": cfg.bands,\n",
    "            \"tasks\": cfg.tasks,\n",
    "            \"model_family\": cfg.model_family,\n",
    "        },\n",
    "        \"environment\": {\n",
    "            \"python\": sys.version,\n",
    "            \"platform\": platform.platform(),\n",
    "            \"numpy\": getattr(np, \"__version__\", \"unknown\"),\n",
    "            \"pandas\": getattr(pd, \"__version__\", \"unknown\"),\n",
    "            \"mne\": getattr(mne, \"__version__\", \"unknown\"),\n",
    "        },\n",
    "        \"data_snapshot\": {\n",
    "            \"n_rows\": int(len(df)),\n",
    "            \"n_subjects\": int(df[\"subject_id\"].nunique()) if \"subject_id\" in df.columns else None,\n",
    "            \"n_sounds\": int(df[\"number\"].nunique()) if \"number\" in df.columns else None,\n",
    "            \"n_eeg_features\": int(len(eeg_feature_cols(df))),\n",
    "            \"has_category\": bool(\"カテゴリー\" in df.columns),\n",
    "            \"target_cols_present\": [c for c in [\"emo_arousal\",\"emo_approach\",\"emo_valence\",\"is_ambiguous\",\"category_3\"] if (c in df.columns or (c==\"category_3\" and \"カテゴリー\" in df.columns))],\n",
    "        },\n",
    "    }\n",
    "    json_dump(cfg.run_metadata_json, meta)\n",
    "    logging.info(f\"[SAVE] run metadata: {cfg.run_metadata_json}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "def main(argv: Optional[List[str]] = None) -> None:\n",
    "    args = parse_args(argv)\n",
    "    setup_matplotlib(auto_font=not args.no_auto_font)\n",
    "\n",
    "    # logging needs cfg.logs_dir; first make cfg, then setup logging\n",
    "    cfg = make_config(args)\n",
    "\n",
    "    log_path = cfg.logs_dir / f\"moduleB_{now_tag()}.log\"\n",
    "    setup_logging(log_path)\n",
    "\n",
    "    t0 = time.time()\n",
    "    logging.info(\"=== Module B (BF: EEG本体 / B+F統合) start ===\")\n",
    "    logging.info(f\"ROOT_DIR     : {cfg.root_dir}\")\n",
    "    logging.info(f\"OUT_DIR      : {cfg.out_dir}\")\n",
    "    logging.info(f\"trial_table  : {cfg.trial_table_csv}\")\n",
    "    logging.info(f\"master_sound : {cfg.master_sound_pc_csv}\")\n",
    "    logging.info(f\"epochs_all   : {cfg.epochs_all_path}\")\n",
    "    logging.info(f\"qc_by_trial  : {cfg.qc_by_trial_csv if cfg.apply_qc else ''}\")\n",
    "    logging.info(f\"seed={cfg.seed} n_perm={cfg.n_perm} n_perm_map={cfg.n_perm_map} n_jobs={cfg.n_jobs} perm_within={cfg.perm_within_subject}\")\n",
    "    logging.info(f\"crop         : {cfg.crop_tmin}..{cfg.crop_tmax} sec | ERP/TFR baseline=({cfg.baseline_tmin},{cfg.baseline_tmax})\")\n",
    "    logging.info(f\"erp_windows_ms={cfg.erp_windows_ms}\")\n",
    "    logging.info(f\"tfr_windows_ms={cfg.tfr_windows_ms}\")\n",
    "    logging.info(f\"bands={cfg.bands}\")\n",
    "    logging.info(f\"tasks={cfg.tasks}\")\n",
    "    logging.info(f\"with_maps={cfg.with_maps} map_tasks={cfg.map_tasks if cfg.map_tasks else 'ALL'}\")\n",
    "    logging.info(f\"model_family={cfg.model_family}\")\n",
    "    logging.info(f\"log          : {log_path}\")\n",
    "\n",
    "    trial_features_in = Path(args.trial_features_csv).expanduser() if args.trial_features_csv else None\n",
    "    if trial_features_in and not trial_features_in.exists():\n",
    "        logging.warning(f\"[input] trial_features_csv not found -> build from epochs. path={trial_features_in}\")\n",
    "        trial_features_in = None\n",
    "\n",
    "    # build/load\n",
    "    df = load_or_build_features(cfg, trial_features_in, reuse_built=bool(args.reuse_built))\n",
    "\n",
    "    # QC\n",
    "    if cfg.apply_qc:\n",
    "        qc = load_qc_by_trial(cfg)\n",
    "        df = apply_qc_filter(df, qc)\n",
    "    \n",
    "    df = normalize_category_column(df)\n",
    "\n",
    "    logging.info(f\"[DATA] shape={df.shape} subjects={df['subject_id'].nunique()} sounds={df['number'].nunique()} eeg_features={len(eeg_feature_cols(df))}\")\n",
    "\n",
    "    # save metadata early\n",
    "    save_run_metadata(cfg, df, trial_features_in)\n",
    "\n",
    "    cat_cols = [c for c in df.columns if (\"カテゴリ\" in c) or (c.lower() == \"category\")]\n",
    "    logging.info(f\"[CAT] category-like cols={cat_cols} | has_カテゴリー={'カテゴリー' in df.columns}\")\n",
    "\n",
    "    # tasks\n",
    "    specs = build_task_specs(df, cfg.tasks)\n",
    "    if not specs:\n",
    "        raise RuntimeError(\"No valid tasks found (targets/labels missing).\")\n",
    "\n",
    "    for spec in specs:\n",
    "        logging.info(f\"--- Task: {spec.name} ({spec.kind}) ---\")\n",
    "        run_task(cfg, df, spec)\n",
    "\n",
    "    logging.info(\"=== Module B complete ===\")\n",
    "    logging.info(f\"- tables : {cfg.tables_dir}\")\n",
    "    logging.info(f\"- figures: {cfg.figures_dir}\")\n",
    "    logging.info(f\"- log    : {log_path}\")\n",
    "    logging.info(f\"- elapsed: {time.time() - t0:.1f} sec\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa325d6c-6a4b-4033-8f35-57439deac036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eeg48]",
   "language": "python",
   "name": "conda-env-eeg48-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
