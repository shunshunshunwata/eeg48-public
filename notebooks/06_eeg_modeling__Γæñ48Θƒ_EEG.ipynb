{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0261edd-508f-48af-ac62-59a8b346270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /Users/shunsuke/EEG_48sounds\n",
      "MASTER_INDEX_CSV: /Users/shunsuke/EEG_48sounds/derivatives/master_epoch_index.csv\n",
      "TRIAL_BASE_DIR: /Users/shunsuke/EEG_48sounds/derivatives/epochs_trial\n",
      "\n",
      "Building trial_table ...\n",
      "[INFO] FileName正規化あり -> /Users/shunsuke/EEG_48sounds/output/integration_audit/master_epoch_index_filename_normalized_head500.csv\n",
      "[OK] number→FileName は 1対1 です（正規化後）。\n",
      "Total trials in master_epoch_index: 1728\n",
      "Trials to process (qc filter=True): 1588\n",
      "Processing trials: 8/1588 (01_高見-run1-trial08)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 24/1588 (01_高見-run1-trial24)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 52/1588 (01_高見-run2-trial04)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 61/1588 (01_高見-run2-trial13)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 64/1588 (01_高見-run2-trial16)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 69/1588 (01_高見-run2-trial21)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 74/1588 (01_高見-run2-trial26)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 85/1588 (01_高見-run2-trial37)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 87/1588 (01_高見-run2-trial39)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 92/1588 (01_高見-run2-trial44)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 96/1588 (01_高見-run2-trial48)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 97/1588 (01_高見-run3-trial01)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 99/1588 (01_高見-run3-trial03)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 109/1588 (01_高見-run3-trial13)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 117/1588 (01_高見-run3-trial21)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 127/1588 (01_高見-run3-trial31)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 300/1588 (03_江口-run1-trial29)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 301/1588 (03_江口-run1-trial30)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 322/1588 (03_江口-run2-trial05)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 340/1588 (03_江口-run2-trial24)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 346/1588 (03_江口-run2-trial30)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 358/1588 (03_江口-run2-trial42)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 364/1588 (03_江口-run2-trial48)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 380/1588 (03_江口-run3-trial19)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 382/1588 (03_江口-run3-trial21)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 390/1588 (03_江口-run3-trial29)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 393/1588 (03_江口-run3-trial32)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 1294/1588 (10_細川-run3-trial19)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEG48/lib/python3.13/site-packages/scipy/_lib/_util.py:352: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "  return fun(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trials: 1588/1588 (12_濱津-run3-trial48)\n",
      "Finished trial_table. Shape: (1728, 1194)\n",
      "[WARN] feature_ok=False が 140 件あります。audit -> /Users/shunsuke/EEG_48sounds/output/integration_audit/eeg_feature_errors_head300.csv\n",
      "Saved trial-level features -> /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_trial.csv\n",
      "\n",
      "Aggregating sound_table ...\n",
      "Saved sound-level features -> /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_sound.csv\n",
      "  n_sounds: 48\n",
      "\n",
      "Aggregating subject_table ...\n",
      "Saved subject-level features -> /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_subject.csv\n",
      "  n_subjects: 12\n",
      "\n",
      "=== DONE ===\n",
      "trial : /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_trial.csv\n",
      "sound : /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_sound.csv\n",
      "subj  : /Users/shunsuke/EEG_48sounds/derivatives/eeg_features_subject.csv\n",
      "audit : /Users/shunsuke/EEG_48sounds/output/integration_audit\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "48音EEG用「統合前・最強特徴量」抽出スクリプト（trial → sound → subject）\n",
    "★統合解析(⑥)でそのままJOINできるように、trial出力に number/FileName を必ず載せる版\n",
    "\n",
    "入力:\n",
    "- derivatives/master_epoch_index.csv\n",
    "- derivatives/epochs_trial/<subject>/fif/<subject>_<run>_trialXX-epo.fif\n",
    "\n",
    "出力:\n",
    "- derivatives/eeg_features_trial.csv\n",
    "    -> 1行 = 1 trial（統合解析の基準テーブルにJOIN可能）\n",
    "- derivatives/eeg_features_sound.csv\n",
    "    -> 1行 = 1 sound(number×FileName)（※number/FileNameが揃っている時のみ）\n",
    "- derivatives/eeg_features_subject.csv\n",
    "    -> 1行 = 1 participant（subject→Pxx変換後、参加者平均）\n",
    "\n",
    "注意:\n",
    "- master_epoch_index に number / FileName が無い場合、sound集約は「trial位置」になってしまうので\n",
    "  この版では “sound集約を原則スキップ” し、監査ログを出します。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from scipy.signal import welch, hilbert\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. PATH & GLOBAL SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "# ★ここだけ自分の環境に合わせて変更\n",
    "ROOT_DIR = Path(\"/Users/shunsuke/EEG_48sounds\")\n",
    "\n",
    "DERIV_DIR = ROOT_DIR / \"derivatives\"\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 入力\n",
    "MASTER_INDEX_CSV = DERIV_DIR / \"master_epoch_index.csv\"\n",
    "TRIAL_BASE_DIR   = DERIV_DIR / \"epochs_trial\"\n",
    "\n",
    "# 出力\n",
    "OUT_TRIAL_CSV = DERIV_DIR / \"eeg_features_trial.csv\"\n",
    "OUT_SOUND_CSV = DERIV_DIR / \"eeg_features_sound.csv\"\n",
    "OUT_SUBJ_CSV  = DERIV_DIR / \"eeg_features_subject.csv\"\n",
    "\n",
    "# 監査ログ\n",
    "AUDIT_DIR = ROOT_DIR / \"output\" / \"integration_audit\"\n",
    "AUDIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 処理方針\n",
    "PROCESS_ONLY_QC_PASS = True   # Trueなら qc_pass==True のtrialだけ特徴抽出（推奨）\n",
    "KEEP_ERROR_ROWS      = True   # Trueならエラーtrialも feature_ok=False で行を残す（統合が安定）\n",
    "\n",
    "# フェーズ定義（音開始=0, 音長=5s を想定）\n",
    "PHASES = {\n",
    "    \"A\": (-2.0, -0.5),   # 広いベースライン\n",
    "    \"B\": (-0.5, 0.0),    # 直前期（SPN/CNV）\n",
    "    \"C\": (0.0, 0.5),     # オンセット\n",
    "    \"D\": (0.5, 3.0),     # 持続\n",
    "    \"E\": (3.0, 5.0),     # 末期\n",
    "    \"F\": (5.0, 12.0),    # オフ後（余韻）\n",
    "}\n",
    "\n",
    "# 周波数帯\n",
    "FREQ_BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 7.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "    \"gamma\": (30.0, 80.0),\n",
    "}\n",
    "\n",
    "# ROI（チャネル名の部分一致）\n",
    "ROI_PATTERNS = {\n",
    "    \"FC\": [\"Fz\", \"Cz\"],\n",
    "    \"P\":  [\"Pz\", \"P3\", \"P4\"],\n",
    "    \"Pz\": [\"Pz\"],\n",
    "    \"Cz\": [\"Cz\"],\n",
    "    \"Fz\": [\"Fz\"],\n",
    "    \"OCC\": [\"O1\", \"O2\"],\n",
    "    \"F_left\":  [\"Fp1\", \"F3\", \"F7\"],\n",
    "    \"F_right\": [\"Fp2\", \"F4\", \"F8\"],\n",
    "}\n",
    "\n",
    "FAA_LEFT_PATTERNS  = [\"Fp1\", \"F3\", \"F7\"]\n",
    "FAA_RIGHT_PATTERNS = [\"Fp2\", \"F4\", \"F8\"]\n",
    "\n",
    "# 結合性のペア（わかりやすい命名）\n",
    "CONNECTIVITY_PAIRS = {\n",
    "    \"FzCz\": ([\"Fz\"], [\"Cz\"]),\n",
    "    \"FzPz\": ([\"Fz\"], [\"Pz\"]),\n",
    "    \"F3P3\": ([\"F3\"], [\"P3\"]),\n",
    "    \"F4P4\": ([\"F4\"], [\"P4\"]),\n",
    "    \"T3T4\": ([\"T3\"], [\"T4\"]),\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. UTILS\n",
    "# ============================================================\n",
    "\n",
    "def subject_to_participant(subject: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    subject文字列の先頭数字を拾って Pxx に変換。\n",
    "    例: \"1_高見\" -> \"P01\"\n",
    "    \"\"\"\n",
    "    if subject is None:\n",
    "        return None\n",
    "    s = str(subject)\n",
    "    import re\n",
    "    m = re.match(r\"(\\d+)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    return f\"P{int(m.group(1)):02d}\"\n",
    "\n",
    "\n",
    "def normalize_run(run: Any) -> str:\n",
    "    \"\"\"\n",
    "    run表記ゆれ（1,2,3 / run1 / Run1 / etc）を run1 形式へ寄せる。\n",
    "    \"\"\"\n",
    "    r = str(run).strip()\n",
    "    # すでに run1 などならそのまま\n",
    "    if r.lower().startswith(\"run\"):\n",
    "        # run01 のような場合も run1 に寄せる\n",
    "        import re\n",
    "        m = re.match(r\"run\\s*0*(\\d+)\", r, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            return f\"run{int(m.group(1))}\"\n",
    "        return r.lower()\n",
    "    # 数字だけなら runN\n",
    "    try:\n",
    "        n = int(float(r))\n",
    "        return f\"run{n}\"\n",
    "    except Exception:\n",
    "        return r.lower()\n",
    "\n",
    "\n",
    "def pick_by_patterns(info: mne.Info, patterns: List[str]) -> np.ndarray:\n",
    "    \"\"\"チャネル名に指定パターンが含まれるインデックスを返す。\"\"\"\n",
    "    ch_names = np.array(info[\"ch_names\"])\n",
    "    idx = [i for i, name in enumerate(ch_names) if any(pat in name for pat in patterns)]\n",
    "    return np.asarray(idx, dtype=int)\n",
    "\n",
    "\n",
    "def get_time_mask(times: np.ndarray, tmin: float, tmax: float) -> np.ndarray:\n",
    "    \"\"\"timesベクトルから [tmin, tmax) のブールマスクを作る。\"\"\"\n",
    "    return (times >= tmin) & (times < tmax)\n",
    "\n",
    "\n",
    "def baseline_correct(data: np.ndarray, times: np.ndarray, tmin: float = -0.2, tmax: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"単一trial(n_channels, n_times)に対するベースライン補正。\"\"\"\n",
    "    mask = get_time_mask(times, tmin, tmax)\n",
    "    if not np.any(mask):\n",
    "        return data.copy()\n",
    "    baseline = data[:, mask].mean(axis=1, keepdims=True)\n",
    "    return data - baseline\n",
    "\n",
    "\n",
    "def safe_mean(x: np.ndarray) -> float:\n",
    "    \"\"\"空ならNaN、それ以外は平均。\"\"\"\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(x))\n",
    "\n",
    "\n",
    "def spd_logm(C: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    SPD行列用の logm（固有値分解ベース）。\n",
    "    \"\"\"\n",
    "    C_sym = 0.5 * (C + C.T)\n",
    "    w, V = np.linalg.eigh(C_sym)\n",
    "    w = np.clip(w, eps, None)\n",
    "    logw = np.log(w)\n",
    "    return (V * logw) @ V.T\n",
    "    \n",
    "def canonicalize_filename(name: Any) -> Any:\n",
    "    \"\"\"\n",
    "    FileName の表記ゆれを統合解析JOIN向けに正規化。\n",
    "    - 余計なメモ（例: '.wav 30秒後'）を切り落とす\n",
    "    - パスが混じっても basename にする\n",
    "    - NAはNAのまま維持\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return pd.NA\n",
    "\n",
    "    s = str(name).strip()\n",
    "    # パスが入っていたらファイル名だけに\n",
    "    s = Path(s).name\n",
    "\n",
    "    # 連続空白を1つに\n",
    "    import re\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # \".wav\" までで切る（大文字小文字無視）\n",
    "    m = re.search(r\"\\.wav\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        s = s[: m.end()]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. ERP / SLOW POTENTIALS\n",
    "# ============================================================\n",
    "\n",
    "def erp_peak(data_bc: np.ndarray, times: np.ndarray, info: mne.Info, roi_key: str,\n",
    "             tmin: float, tmax: float, mode: str = \"min\") -> float:\n",
    "    \"\"\"\n",
    "    ROI・時間窓でのERPピーク値。\n",
    "    mode: \"min\" / \"max\" / \"mean\"\n",
    "    \"\"\"\n",
    "    picks = pick_by_patterns(info, ROI_PATTERNS[roi_key])\n",
    "    if picks.size == 0:\n",
    "        return np.nan\n",
    "    mask = get_time_mask(times, tmin, tmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    roi_wave = data_bc[picks][:, mask].mean(axis=0)\n",
    "    if mode == \"min\":\n",
    "        return float(np.min(roi_wave))\n",
    "    if mode == \"max\":\n",
    "        return float(np.max(roi_wave))\n",
    "    if mode == \"mean\":\n",
    "        return float(np.mean(roi_wave))\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "\n",
    "def sustained_potential(data_bc: np.ndarray, times: np.ndarray, info: mne.Info,\n",
    "                        roi_key: str, tmin: float, tmax: float) -> float:\n",
    "    \"\"\"ROI・時間窓での平均電位（持続ポテンシャル）。\"\"\"\n",
    "    picks = pick_by_patterns(info, ROI_PATTERNS[roi_key])\n",
    "    if picks.size == 0:\n",
    "        return np.nan\n",
    "    mask = get_time_mask(times, tmin, tmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    roi_data = data_bc[picks][:, mask]\n",
    "    return safe_mean(roi_data)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. BAND POWER / ERD-ERS / FAA\n",
    "# ============================================================\n",
    "\n",
    "def bandpower_roi(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info,\n",
    "                  roi_key: str, band_name: str, tmin: float, tmax: float, nperseg: int = 256) -> float:\n",
    "    \"\"\"Welch法でROIのバンドパワーを算出。\"\"\"\n",
    "    picks = pick_by_patterns(info, ROI_PATTERNS[roi_key])\n",
    "    if picks.size == 0:\n",
    "        return np.nan\n",
    "    mask = get_time_mask(times, tmin, tmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "\n",
    "    seg = data[picks][:, mask]\n",
    "    fmin, fmax = FREQ_BANDS[band_name]\n",
    "\n",
    "    bp_list = []\n",
    "    for ch in range(seg.shape[0]):\n",
    "        f, Pxx = welch(seg[ch], fs=sfreq, nperseg=min(nperseg, seg.shape[1]))\n",
    "        band_mask = (f >= fmin) & (f <= fmax)\n",
    "        if np.any(band_mask):\n",
    "            bp_list.append(Pxx[band_mask].mean())\n",
    "\n",
    "    if len(bp_list) == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(bp_list))\n",
    "\n",
    "\n",
    "def erd_ers_db(bp_phase: float, bp_baseline: float, eps: float = 1e-12) -> float:\n",
    "    \"\"\"ERD/ERSをdB表現で返す。\"\"\"\n",
    "    if np.isnan(bp_phase) or np.isnan(bp_baseline):\n",
    "        return np.nan\n",
    "    return float(10.0 * np.log10((bp_phase + eps) / (bp_baseline + eps)))\n",
    "\n",
    "\n",
    "def faa_alpha(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info,\n",
    "              tmin: float, tmax: float, nperseg: int = 256) -> float:\n",
    "    \"\"\"前頭αバンド左右差（FAA = logα右 − logα左）。\"\"\"\n",
    "    fmin, fmax = FREQ_BANDS[\"alpha\"]\n",
    "\n",
    "    def bp_side(patterns: List[str]) -> float:\n",
    "        picks = pick_by_patterns(info, patterns)\n",
    "        if picks.size == 0:\n",
    "            return np.nan\n",
    "        mask = get_time_mask(times, tmin, tmax)\n",
    "        if not np.any(mask):\n",
    "            return np.nan\n",
    "        seg = data[picks][:, mask]\n",
    "        vals = []\n",
    "        for ch in range(seg.shape[0]):\n",
    "            f, Pxx = welch(seg[ch], fs=sfreq, nperseg=min(nperseg, seg.shape[1]))\n",
    "            band_mask = (f >= fmin) & (f <= fmax)\n",
    "            if np.any(band_mask):\n",
    "                vals.append(Pxx[band_mask].mean())\n",
    "        if len(vals) == 0:\n",
    "            return np.nan\n",
    "        return float(np.mean(vals))\n",
    "\n",
    "    bp_left = bp_side(FAA_LEFT_PATTERNS)\n",
    "    bp_right = bp_side(FAA_RIGHT_PATTERNS)\n",
    "    if np.isnan(bp_left) or np.isnan(bp_right):\n",
    "        return np.nan\n",
    "    return float(np.log(bp_right) - np.log(bp_left))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. RIEMANN / CONNECTIVITY / COMPLEXITY\n",
    "# ============================================================\n",
    "\n",
    "def compute_riemann_features(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info) -> Dict[str, float]:\n",
    "    \"\"\"フェーズごとのlog共分散（上三角）を特徴量として出す。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "    phases_for_cov = [\"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "    n_ch = data.shape[0]\n",
    "    eps = 1e-6\n",
    "\n",
    "    for phase_key in phases_for_cov:\n",
    "        tmin, tmax = PHASES[phase_key]\n",
    "        mask = get_time_mask(times, tmin, tmax)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        seg = data[:, mask]\n",
    "        if seg.shape[1] < n_ch + 1:\n",
    "            continue\n",
    "\n",
    "        C = np.cov(seg) + eps * np.eye(n_ch)\n",
    "        try:\n",
    "            C_log = spd_logm(C, eps=1e-12)\n",
    "        except Exception:\n",
    "            C_log = np.full_like(C, np.nan)\n",
    "\n",
    "        for i in range(n_ch):\n",
    "            for j in range(i, n_ch):\n",
    "                feats[f\"RG_phase{phase_key}_{i:02d}_{j:02d}\"] = float(np.real(C_log[i, j]))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def compute_connectivity_features(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info) -> Dict[str, float]:\n",
    "    \"\"\"PLV/Coherence を band×phase×pair で算出。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "\n",
    "    # bandごとにフィルタしたデータをキャッシュ\n",
    "    bandpass_data: Dict[str, np.ndarray] = {}\n",
    "    for band_name in [\"theta\", \"alpha\", \"beta\", \"gamma\"]:\n",
    "        fmin, fmax = FREQ_BANDS[band_name]\n",
    "        bp_data = mne.filter.filter_data(data.copy(), sfreq=sfreq, l_freq=fmin, h_freq=fmax, verbose=\"ERROR\")\n",
    "        bandpass_data[band_name] = bp_data\n",
    "\n",
    "    from scipy.signal import coherence as coh_fun\n",
    "\n",
    "    phases_for_conn = [\"C\", \"D\", \"E\", \"F\"]\n",
    "    for band_name, bp_data in bandpass_data.items():\n",
    "        fmin, fmax = FREQ_BANDS[band_name]\n",
    "        for phase_key in phases_for_conn:\n",
    "            tmin, tmax = PHASES[phase_key]\n",
    "            mask = get_time_mask(times, tmin, tmax)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            for pair_name, (pat1, pat2) in CONNECTIVITY_PAIRS.items():\n",
    "                picks1 = pick_by_patterns(info, pat1)\n",
    "                picks2 = pick_by_patterns(info, pat2)\n",
    "                key_plv = f\"PLV_{band_name}_{pair_name}_phase{phase_key}\"\n",
    "                key_coh = f\"COH_{band_name}_{pair_name}_phase{phase_key}\"\n",
    "\n",
    "                if picks1.size == 0 or picks2.size == 0:\n",
    "                    feats[key_plv] = np.nan\n",
    "                    feats[key_coh] = np.nan\n",
    "                    continue\n",
    "\n",
    "                seg1 = bp_data[picks1][:, mask].mean(axis=0)\n",
    "                seg2 = bp_data[picks2][:, mask].mean(axis=0)\n",
    "                if seg1.size < 10 or seg2.size < 10:\n",
    "                    feats[key_plv] = np.nan\n",
    "                    feats[key_coh] = np.nan\n",
    "                    continue\n",
    "\n",
    "                # PLV\n",
    "                analytic1 = hilbert(seg1)\n",
    "                analytic2 = hilbert(seg2)\n",
    "                phase_diff = np.angle(analytic1) - np.angle(analytic2)\n",
    "                feats[key_plv] = float(np.abs(np.exp(1j * phase_diff).mean()))\n",
    "\n",
    "                # Coherence\n",
    "                try:\n",
    "                    f, coh_vals = coh_fun(seg1, seg2, fs=sfreq, nperseg=min(256, seg1.size))\n",
    "                    band_mask = (f >= fmin) & (f <= fmax)\n",
    "                    feats[key_coh] = float(np.mean(coh_vals[band_mask])) if np.any(band_mask) else np.nan\n",
    "                except Exception:\n",
    "                    feats[key_coh] = np.nan\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def sample_entropy(signal: np.ndarray, m: int = 2, r: float | None = None) -> float:\n",
    "    \"\"\"Sample Entropy（簡易実装）。\"\"\"\n",
    "    x = np.array(signal, dtype=float)\n",
    "    N = x.size\n",
    "    if N <= m + 1:\n",
    "        return np.nan\n",
    "    if r is None:\n",
    "        r = 0.2 * np.std(x)\n",
    "        if r == 0:\n",
    "            return np.nan\n",
    "\n",
    "    def _phi(m_: int) -> float:\n",
    "        count = 0\n",
    "        Xm = np.array([x[i:i + m_] for i in range(N - m_ + 1)])\n",
    "        for i in range(Xm.shape[0] - 1):\n",
    "            dist = np.max(np.abs(Xm[i+1:] - Xm[i]), axis=1)\n",
    "            count += np.sum(dist <= r)\n",
    "        return float(count)\n",
    "\n",
    "    C_m = _phi(m)\n",
    "    C_m1 = _phi(m + 1)\n",
    "    if C_m == 0 or C_m1 == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return float(-np.log(C_m1 / C_m))\n",
    "\n",
    "\n",
    "def lz_complexity(signal: np.ndarray) -> float:\n",
    "    \"\"\"Lempel–Ziv Complexity（2値列での簡易正規化版）。\"\"\"\n",
    "    x = np.array(signal, dtype=float)\n",
    "    N = x.size\n",
    "    if N < 10:\n",
    "        return np.nan\n",
    "\n",
    "    thr = np.median(x)\n",
    "    s = ''.join('1' if v > thr else '0' for v in x)\n",
    "    n = len(s)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "\n",
    "    i = 0; k = 1; l = 1; c = 1\n",
    "    while True:\n",
    "        if l + k > n:\n",
    "            c += 1\n",
    "            break\n",
    "        if s[i:i + k] == s[l:l + k]:\n",
    "            k += 1\n",
    "            if l + k > n:\n",
    "                c += 1\n",
    "                break\n",
    "        else:\n",
    "            i += 1\n",
    "            if i == l:\n",
    "                c += 1\n",
    "                l += k\n",
    "                if l + 1 > n:\n",
    "                    break\n",
    "                i = 0; k = 1\n",
    "\n",
    "    return float(c * np.log2(n) / n)\n",
    "\n",
    "\n",
    "def compute_complexity_features(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info) -> Dict[str, float]:\n",
    "    \"\"\"複雑性：SampEn(Fz, D)＋LZC(all, F)。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "\n",
    "    # SampEn: PhaseD, Fz\n",
    "    tmin_D, tmax_D = PHASES[\"D\"]\n",
    "    mask_D = get_time_mask(times, tmin_D, tmax_D)\n",
    "    picks_Fz = pick_by_patterns(info, ROI_PATTERNS[\"Fz\"])\n",
    "    if picks_Fz.size > 0 and np.any(mask_D):\n",
    "        seg_Fz = data[picks_Fz][:, mask_D].mean(axis=0)\n",
    "        feats[\"SampEn_Fz_phaseD\"] = sample_entropy(seg_Fz, m=2, r=None) if seg_Fz.size > 50 else np.nan\n",
    "    else:\n",
    "        feats[\"SampEn_Fz_phaseD\"] = np.nan\n",
    "\n",
    "    # LZC: PhaseF, 全チャネル\n",
    "    tmin_F, tmax_F = PHASES[\"F\"]\n",
    "    mask_F = get_time_mask(times, tmin_F, tmax_F)\n",
    "    if np.any(mask_F):\n",
    "        seg_all = data[:, mask_F].flatten(order=\"C\")\n",
    "        feats[\"LZC_all_phaseF\"] = lz_complexity(seg_all) if seg_all.size > 50 else np.nan\n",
    "    else:\n",
    "        feats[\"LZC_all_phaseF\"] = np.nan\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. MICROSTATES / PAC / EMBEDDING\n",
    "# ============================================================\n",
    "\n",
    "def _run_lengths(labels: np.ndarray, state: int) -> List[int]:\n",
    "    \"\"\"labels中でstateが連続する区間長（サンプル数）のリスト。\"\"\"\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    mask = (labels == state).astype(int)\n",
    "    if mask.sum() == 0:\n",
    "        return []\n",
    "    diffs = np.diff(mask)\n",
    "    starts = np.where(diffs == 1)[0] + 1\n",
    "    ends = np.where(diffs == -1)[0] + 1\n",
    "    if mask[0] == 1:\n",
    "        starts = np.r_[0, starts]\n",
    "    if mask[-1] == 1:\n",
    "        ends = np.r_[ends, len(mask)]\n",
    "    return (ends - starts).tolist()\n",
    "\n",
    "\n",
    "def compute_microstate_features(data_bc: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info, n_states: int = 4) -> Dict[str, float]:\n",
    "    \"\"\"簡易マイクロステート（C〜E, 0〜5s）。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "    for s in range(n_states):\n",
    "        s_id = s + 1\n",
    "        feats[f\"MS_occupancy_state{s_id}\"] = np.nan\n",
    "        feats[f\"MS_mean_dur_state{s_id}\"]  = np.nan\n",
    "        feats[f\"MS_mean_gfp_state{s_id}\"]  = np.nan\n",
    "    feats[\"MS_transition_rate\"] = np.nan\n",
    "\n",
    "    mask = get_time_mask(times, 0.0, 5.0)\n",
    "    if not np.any(mask):\n",
    "        return feats\n",
    "\n",
    "    seg = data_bc[:, mask]\n",
    "    n_ch, n_t = seg.shape\n",
    "    if n_t < n_states * 5:\n",
    "        return feats\n",
    "\n",
    "    gfp = seg.std(axis=0)\n",
    "    if np.allclose(gfp, 0):\n",
    "        return feats\n",
    "\n",
    "    thr = np.percentile(gfp, 70)\n",
    "    high_idx = np.where(gfp >= thr)[0]\n",
    "    if high_idx.size < n_states * 5:\n",
    "        high_idx = np.arange(n_t)\n",
    "\n",
    "    maps = seg[:, high_idx].T\n",
    "    maps = maps - maps.mean(axis=1, keepdims=True)\n",
    "    maps_norm = maps / (np.linalg.norm(maps, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    try:\n",
    "        centroids, _ = kmeans2(maps_norm, n_states, minit=\"points\", iter=30)\n",
    "    except Exception:\n",
    "        return feats\n",
    "\n",
    "    all_maps = seg.T\n",
    "    all_maps = all_maps - all_maps.mean(axis=1, keepdims=True)\n",
    "    all_maps_norm = all_maps / (np.linalg.norm(all_maps, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    dists = np.sum((all_maps_norm[:, None, :] - centroids[None, :, :]) ** 2, axis=2)\n",
    "    labels_all = np.argmin(dists, axis=1)\n",
    "\n",
    "    duration_sec = n_t / sfreq\n",
    "    feats[\"MS_transition_rate\"] = float(np.sum(labels_all[1:] != labels_all[:-1]) / duration_sec)\n",
    "\n",
    "    for s in range(n_states):\n",
    "        s_id = s + 1\n",
    "        mask_s = (labels_all == s)\n",
    "        if not np.any(mask_s):\n",
    "            continue\n",
    "        feats[f\"MS_occupancy_state{s_id}\"] = float(mask_s.mean())\n",
    "        runs = _run_lengths(labels_all, s)\n",
    "        if len(runs) > 0:\n",
    "            feats[f\"MS_mean_dur_state{s_id}\"] = float(np.mean(runs) / sfreq)\n",
    "        feats[f\"MS_mean_gfp_state{s_id}\"] = float(gfp[mask_s].mean())\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def compute_pac_features(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info) -> Dict[str, float]:\n",
    "    \"\"\"Fzで θ位相×γ振幅PAC(Tort MI) を D/Eで算出。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "    picks = pick_by_patterns(info, ROI_PATTERNS[\"Fz\"])\n",
    "\n",
    "    for phase_key in [\"D\", \"E\"]:\n",
    "        name = f\"PAC_MI_theta_gamma_Fz_phase{phase_key}\"\n",
    "        feats[name] = np.nan\n",
    "\n",
    "        if picks.size == 0:\n",
    "            continue\n",
    "\n",
    "        tmin, tmax = PHASES[phase_key]\n",
    "        mask = get_time_mask(times, tmin, tmax)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        seg = data[picks][:, mask].mean(axis=0)\n",
    "        if seg.size < int(sfreq * 0.5):\n",
    "            continue\n",
    "\n",
    "        low  = mne.filter.filter_data(seg[np.newaxis, :], sfreq, 4.0, 7.0,  verbose=\"ERROR\")[0]\n",
    "        high = mne.filter.filter_data(seg[np.newaxis, :], sfreq, 30.0, 80.0, verbose=\"ERROR\")[0]\n",
    "\n",
    "        phase = np.angle(hilbert(low))\n",
    "        amp   = np.abs(hilbert(high))\n",
    "\n",
    "        n_bins = 18\n",
    "        bins = np.linspace(-np.pi, np.pi, n_bins + 1)\n",
    "        amp_means = np.zeros(n_bins)\n",
    "        for b in range(n_bins):\n",
    "            idx = (phase >= bins[b]) & (phase < bins[b + 1])\n",
    "            if np.any(idx):\n",
    "                amp_means[b] = amp[idx].mean()\n",
    "\n",
    "        if amp_means.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        p = amp_means / amp_means.sum()\n",
    "        H = -np.sum(p * np.log(p + 1e-12))\n",
    "        Hmax = np.log(n_bins)\n",
    "        feats[name] = float((Hmax - H) / Hmax)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def compute_embedding_features(data: np.ndarray, times: np.ndarray, sfreq: float, info: mne.Info) -> Dict[str, float]:\n",
    "    \"\"\"PhaseD共分散のPCA寄与率と有効次元。\"\"\"\n",
    "    feats: Dict[str, float] = {}\n",
    "    mask = get_time_mask(times, *PHASES[\"D\"])\n",
    "\n",
    "    feats[\"PCA_var_ratio_pc1_phaseD\"] = np.nan\n",
    "    feats[\"PCA_var_ratio_pc2_phaseD\"] = np.nan\n",
    "    feats[\"PCA_var_ratio_pc3_phaseD\"] = np.nan\n",
    "    feats[\"PCA_var_ratio_cum3_phaseD\"] = np.nan\n",
    "    feats[\"PCA_effdim_phaseD\"] = np.nan\n",
    "\n",
    "    if not np.any(mask):\n",
    "        return feats\n",
    "\n",
    "    seg = data[:, mask].T  # (time, ch)\n",
    "    if seg.shape[0] <= 3:\n",
    "        return feats\n",
    "\n",
    "    seg_centered = seg - seg.mean(axis=0, keepdims=True)\n",
    "    C = np.cov(seg_centered, rowvar=False)\n",
    "\n",
    "    evals, _ = np.linalg.eigh(C)\n",
    "    evals = np.maximum(evals, 0.0)\n",
    "    evals = evals[np.argsort(evals)[::-1]]\n",
    "\n",
    "    total = evals.sum()\n",
    "    if total <= 0:\n",
    "        return feats\n",
    "\n",
    "    var_ratio = evals / total\n",
    "    if len(var_ratio) > 0:\n",
    "        feats[\"PCA_var_ratio_pc1_phaseD\"] = float(var_ratio[0])\n",
    "    if len(var_ratio) > 1:\n",
    "        feats[\"PCA_var_ratio_pc2_phaseD\"] = float(var_ratio[1])\n",
    "    if len(var_ratio) > 2:\n",
    "        feats[\"PCA_var_ratio_pc3_phaseD\"] = float(var_ratio[2])\n",
    "        feats[\"PCA_var_ratio_cum3_phaseD\"] = float(var_ratio[:3].sum())\n",
    "\n",
    "    denom = float((evals**2).sum())\n",
    "    feats[\"PCA_effdim_phaseD\"] = float((total**2) / denom) if denom > 0 else np.nan\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. TRIAL-LEVEL EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TrialMeta:\n",
    "    subject: str\n",
    "    participant: Optional[str]\n",
    "    run: str\n",
    "    trial_in_run: int\n",
    "    number: Optional[int]        # 音番号（48音のID）\n",
    "    FileName: Optional[str]      # wav名（HCU/主観評価とJOINするキー）\n",
    "    category: str\n",
    "    qc_pass: bool\n",
    "\n",
    "\n",
    "def build_trial_fif_path(meta: TrialMeta) -> Path:\n",
    "    \"\"\"\n",
    "    derivatives/epochs_trial/<subject>/fif/<subject>_<run>_trialXX-epo.fif\n",
    "    \"\"\"\n",
    "    trial_str = f\"trial{meta.trial_in_run:02d}\"\n",
    "    fif_dir = TRIAL_BASE_DIR / meta.subject / \"fif\"\n",
    "    fname = f\"{meta.subject}_{meta.run}_{trial_str}-epo.fif\"\n",
    "    return fif_dir / fname\n",
    "\n",
    "\n",
    "def extract_features_for_trial(meta: TrialMeta) -> Dict[str, Any]:\n",
    "    \"\"\"1 trial (1 FIF) から全特徴量を抽出。\"\"\"\n",
    "    fif_path = build_trial_fif_path(meta)\n",
    "    epochs = mne.read_epochs(fif_path, preload=True, verbose=\"ERROR\")\n",
    "    if len(epochs) != 1:\n",
    "        raise ValueError(f\"Expected 1 epoch in {fif_path}, got {len(epochs)}\")\n",
    "\n",
    "    data  = epochs.get_data()[0]  # (ch, time)\n",
    "    times = epochs.times\n",
    "    sfreq = epochs.info[\"sfreq\"]\n",
    "    info  = epochs.info\n",
    "\n",
    "    # ERP向けベースライン補正\n",
    "    data_bc = baseline_correct(data, times, tmin=-0.2, tmax=0.0)\n",
    "\n",
    "    feats: Dict[str, Any] = {}\n",
    "\n",
    "    # --- 統合用メタ（ここが肝）---\n",
    "    feats[\"subject\"]      = meta.subject\n",
    "    feats[\"participant\"]  = meta.participant\n",
    "    feats[\"run\"]          = meta.run\n",
    "    feats[\"trial_in_run\"] = meta.trial_in_run\n",
    "    feats[\"number\"]       = meta.number\n",
    "    feats[\"FileName\"]     = meta.FileName\n",
    "    feats[\"category\"]     = meta.category\n",
    "    feats[\"qc_pass\"]      = bool(meta.qc_pass)\n",
    "\n",
    "    # === ERP ===\n",
    "    feats[\"ERP_N1_FC_80_130ms\"]   = erp_peak(data_bc, times, info, \"FC\", 0.080, 0.130, \"min\")\n",
    "    feats[\"ERP_P2_FC_150_250ms\"]  = erp_peak(data_bc, times, info, \"FC\", 0.150, 0.250, \"max\")\n",
    "    feats[\"ERP_N2_FC_250_350ms\"]  = erp_peak(data_bc, times, info, \"FC\", 0.250, 0.350, \"min\")\n",
    "    feats[\"ERP_P3_P_300_500ms\"]   = erp_peak(data_bc, times, info, \"P\",  0.300, 0.500, \"max\")\n",
    "    feats[\"ERP_LPP_P_400_800ms\"]  = erp_peak(data_bc, times, info, \"P\",  0.400, 0.800, \"mean\")\n",
    "\n",
    "    # オフセット（音終了=5s想定）\n",
    "    feats[\"ERP_P2off_FC_0_300ms_postOff\"]  = erp_peak(data_bc, times, info, \"FC\", 5.0, 5.3, \"max\")\n",
    "    feats[\"ERP_LPPoff_P_300_800ms_postOff\"] = erp_peak(data_bc, times, info, \"P\",  5.3, 5.8, \"mean\")\n",
    "\n",
    "    # === SPN / 持続 ===\n",
    "    feats[\"SPN_Cz_pre\"] = sustained_potential(data_bc, times, info, \"Cz\", *PHASES[\"B\"])\n",
    "    feats[\"SPN_Pz_pre\"] = sustained_potential(data_bc, times, info, \"Pz\", *PHASES[\"B\"])\n",
    "\n",
    "    feats[\"SP_D_Cz\"] = sustained_potential(data_bc, times, info, \"Cz\", *PHASES[\"D\"])\n",
    "    feats[\"SP_D_Pz\"] = sustained_potential(data_bc, times, info, \"Pz\", *PHASES[\"D\"])\n",
    "    feats[\"SP_E_Cz\"] = sustained_potential(data_bc, times, info, \"Cz\", *PHASES[\"E\"])\n",
    "    feats[\"SP_E_Pz\"] = sustained_potential(data_bc, times, info, \"Pz\", *PHASES[\"E\"])\n",
    "    feats[\"SP_F_Cz\"] = sustained_potential(data_bc, times, info, \"Cz\", *PHASES[\"F\"])\n",
    "    feats[\"SP_F_Pz\"] = sustained_potential(data_bc, times, info, \"Pz\", *PHASES[\"F\"])\n",
    "\n",
    "    # === バンドパワー/ERD ===\n",
    "    bp_baseline: Dict[tuple, float] = {}\n",
    "    baseline_roi_band = [(\"Fz\", \"theta\"), (\"Pz\", \"alpha\"), (\"OCC\", \"alpha\"), (\"Cz\", \"beta\")]\n",
    "    for roi_key, band_name in baseline_roi_band:\n",
    "        bp_baseline[(roi_key, band_name)] = bandpower_roi(data, times, sfreq, info, roi_key, band_name, *PHASES[\"A\"])\n",
    "\n",
    "    for phase_key in [\"C\", \"D\", \"E\", \"F\"]:\n",
    "        tmin, tmax = PHASES[phase_key]\n",
    "\n",
    "        bp = bandpower_roi(data, times, sfreq, info, \"Fz\", \"theta\", tmin, tmax)\n",
    "        feats[f\"BP_theta_Fz_phase{phase_key}\"]  = bp\n",
    "        feats[f\"ERD_theta_Fz_phase{phase_key}\"] = erd_ers_db(bp, bp_baseline[(\"Fz\", \"theta\")])\n",
    "\n",
    "        bp = bandpower_roi(data, times, sfreq, info, \"Pz\", \"alpha\", tmin, tmax)\n",
    "        feats[f\"BP_alpha_Pz_phase{phase_key}\"]  = bp\n",
    "        feats[f\"ERD_alpha_Pz_phase{phase_key}\"] = erd_ers_db(bp, bp_baseline[(\"Pz\", \"alpha\")])\n",
    "\n",
    "        bp = bandpower_roi(data, times, sfreq, info, \"OCC\", \"alpha\", tmin, tmax)\n",
    "        feats[f\"BP_alpha_Occ_phase{phase_key}\"]  = bp\n",
    "        feats[f\"ERD_alpha_Occ_phase{phase_key}\"] = erd_ers_db(bp, bp_baseline[(\"OCC\", \"alpha\")])\n",
    "\n",
    "        bp = bandpower_roi(data, times, sfreq, info, \"Cz\", \"beta\", tmin, tmax)\n",
    "        feats[f\"BP_beta_Cz_phase{phase_key}\"]  = bp\n",
    "        feats[f\"ERD_beta_Cz_phase{phase_key}\"] = erd_ers_db(bp, bp_baseline[(\"Cz\", \"beta\")])\n",
    "\n",
    "    # FAA(B〜F)\n",
    "    for phase_key in [\"B\", \"C\", \"D\", \"E\", \"F\"]:\n",
    "        feats[f\"FAA_alpha_phase{phase_key}\"] = faa_alpha(data, times, sfreq, info, *PHASES[phase_key])\n",
    "\n",
    "    # 高次特徴\n",
    "    feats.update(compute_riemann_features(data, times, sfreq, info))\n",
    "    feats.update(compute_connectivity_features(data, times, sfreq, info))\n",
    "    feats.update(compute_complexity_features(data, times, sfreq, info))\n",
    "    feats.update(compute_microstate_features(data_bc, times, sfreq, info))\n",
    "    feats.update(compute_pac_features(data, times, sfreq, info))\n",
    "    feats.update(compute_embedding_features(data, times, sfreq, info))\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. INDEX LOADING（統合用に “number/FileName” を必ず拾う）\n",
    "# ============================================================\n",
    "\n",
    "def load_epoch_index() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    master_epoch_index.csv を読み込み、\n",
    "    subject/run/trial_in_run/qc_pass/category/number/FileName を可能な限り揃える。\n",
    "    \"\"\"\n",
    "    if not MASTER_INDEX_CSV.exists():\n",
    "        raise FileNotFoundError(f\"NOT FOUND: {MASTER_INDEX_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(MASTER_INDEX_CSV)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    def find_col(candidates: List[str]) -> Optional[str]:\n",
    "        for c in candidates:\n",
    "            if c in cols:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    # --- 必須キー ---\n",
    "    subj_col = find_col([\"subject\", \"subj\", \"subj_id\", \"participant\", \"被験者\", \"Subject\"])\n",
    "    run_col  = find_col([\"run\", \"Run\", \"run_id\", \"session\", \"RunNo\", \"run_no\"])\n",
    "    tri_col  = find_col([\"trial_in_run\", \"trial\", \"Trial\", \"trial_index\", \"trial_idx\", \"trial_no\", \"trial_number\"])\n",
    "\n",
    "    if subj_col is None or run_col is None or tri_col is None:\n",
    "        raise ValueError(\n",
    "            \"master_epoch_index.csv に必要列が足りません。\\n\"\n",
    "            f\"  subject候補={subj_col}, run候補={run_col}, trial候補={tri_col}\\n\"\n",
    "            f\"  columns={cols}\"\n",
    "        )\n",
    "\n",
    "    # --- できれば欲しい（統合の要）---\n",
    "    num_col = find_col([\"number\", \"No\", \"sound_no\", \"sound_number\", \"sound_id\"])\n",
    "    fn_col  = find_col([\"FileName\", \"filename\", \"file_name\", \"wav\", \"sound_name\"])\n",
    "\n",
    "    cat_col = find_col([\"category\", \"カテゴリ\", \"カテゴリー\", \"valence_group\", \"label\"])\n",
    "    qc_col  = find_col([\"qc_pass\", \"qc_ok\", \"use\", \"valid\", \"qc_amp_pass\"])\n",
    "\n",
    "    # 正規化rename\n",
    "    ren = {subj_col: \"subject\", run_col: \"run\", tri_col: \"trial_in_run\"}\n",
    "    if num_col is not None: ren[num_col] = \"number\"\n",
    "    if fn_col  is not None: ren[fn_col]  = \"FileName\"\n",
    "    if cat_col is not None: ren[cat_col] = \"category\"\n",
    "    if qc_col  is not None: ren[qc_col]  = \"qc_pass\"\n",
    "    df = df.rename(columns=ren)\n",
    "\n",
    "    # 型・正規化\n",
    "    df[\"subject\"] = df[\"subject\"].astype(str)\n",
    "    df[\"run\"] = df[\"run\"].apply(normalize_run)\n",
    "    df[\"trial_in_run\"] = pd.to_numeric(df[\"trial_in_run\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    if \"qc_pass\" not in df.columns:\n",
    "        df[\"qc_pass\"] = True\n",
    "    df[\"qc_pass\"] = df[\"qc_pass\"].fillna(False).astype(bool)\n",
    "\n",
    "    if \"category\" not in df.columns:\n",
    "        df[\"category\"] = \"unknown\"\n",
    "    df[\"category\"] = df[\"category\"].astype(str)\n",
    "\n",
    "    if \"number\" in df.columns:\n",
    "        df[\"number\"] = pd.to_numeric(df[\"number\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    else:\n",
    "        df[\"number\"] = pd.Series([pd.NA] * len(df), dtype=\"Int64\")\n",
    "\n",
    "    # --- FileName 正規化（NAを壊さない）---\n",
    "    if \"FileName\" in df.columns:\n",
    "        df[\"FileName_raw\"] = df[\"FileName\"].astype(\"string\")\n",
    "        df[\"FileName\"] = df[\"FileName_raw\"].apply(canonicalize_filename).astype(\"string\")\n",
    "\n",
    "        changed = df[\"FileName_raw\"].notna() & (df[\"FileName_raw\"] != df[\"FileName\"])\n",
    "        if changed.any():\n",
    "            df.loc[changed, [\"subject\", \"run\", \"trial_in_run\", \"number\", \"FileName_raw\", \"FileName\"]].head(500).to_csv(\n",
    "                AUDIT_DIR / \"master_epoch_index_filename_normalized_head500.csv\",\n",
    "                index=False, encoding=\"utf-8-sig\"\n",
    "            )\n",
    "            print(f\"[INFO] FileName正規化あり -> {AUDIT_DIR/'master_epoch_index_filename_normalized_head500.csv'}\")\n",
    "    else:\n",
    "        df[\"FileName_raw\"] = pd.Series([pd.NA] * len(df), dtype=\"string\")\n",
    "        df[\"FileName\"] = pd.Series([pd.NA] * len(df), dtype=\"string\")\n",
    "\n",
    "\n",
    "\n",
    "    # participant付与（主観評価Pxxと統合しやすく）\n",
    "    df[\"participant\"] = df[\"subject\"].apply(subject_to_participant)\n",
    "\n",
    "    # キー重複チェック（統合の安定性）\n",
    "    key = [\"subject\", \"run\", \"trial_in_run\"]\n",
    "    dup = df.duplicated(subset=key, keep=False).sum()\n",
    "    if dup > 0:\n",
    "        ex = df[df.duplicated(subset=key, keep=False)].sort_values(key).head(40)\n",
    "        ex.to_csv(AUDIT_DIR / \"master_epoch_index_duplicate_keys.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        raise ValueError(\n",
    "            f\"master_epoch_index のキーが重複しています: dup_rows={dup}\\n\"\n",
    "            f\"監査ログ: {AUDIT_DIR/'master_epoch_index_duplicate_keys.csv'}\"\n",
    "        )\n",
    "\n",
    "    # number/FileName 欠損監査（ある程度は許容するが、sound集約は危険）\n",
    "    miss_n = int(df[\"number\"].isna().sum())\n",
    "    miss_f = int(df[\"FileName\"].isna().sum())\n",
    "    if miss_n > 0 or miss_f > 0:\n",
    "        df[df[\"number\"].isna() | df[\"FileName\"].isna()][key + [\"number\", \"FileName\"]].head(300).to_csv(\n",
    "            AUDIT_DIR / \"master_epoch_index_missing_sound_identity_head300.csv\",\n",
    "            index=False, encoding=\"utf-8-sig\"\n",
    "        )\n",
    "        print(f\"[WARN] master_epoch_index に number/FileName 欠損があります: number_missing={miss_n}, FileName_missing={miss_f}\")\n",
    "        print(f\"       audit -> {AUDIT_DIR/'master_epoch_index_missing_sound_identity_head300.csv'}\")\n",
    "\n",
    "    # --- number -> FileName が 1対1 になっているか（JOINの健全性チェック）---\n",
    "    sub = df.dropna(subset=[\"number\", \"FileName\"]).copy()\n",
    "\n",
    "    if sub.empty:\n",
    "        print(\"[WARN] number/FileName が全て欠損のため、1対1チェックをスキップします。\")\n",
    "    else:\n",
    "        nu = sub.groupby(\"number\")[\"FileName\"].nunique(dropna=True)\n",
    "        bad = nu[nu > 1]\n",
    "\n",
    "    if not bad.empty:\n",
    "        detail = sub[sub[\"number\"].isin(bad.index)][\n",
    "            [\"number\", \"FileName\", \"subject\", \"run\", \"trial_in_run\"]\n",
    "        ].sort_values([\"number\", \"FileName\", \"subject\", \"run\", \"trial_in_run\"])\n",
    "\n",
    "        out_path = AUDIT_DIR / \"master_epoch_index_number_to_multiple_filenames.csv\"\n",
    "        detail.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"number→FileName が 1対1 になっていません（{bad.size} numbers）\\n\"\n",
    "            f\"audit -> {out_path}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"[OK] number→FileName は 1対1 です（正規化後）。\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_trial_meta_list(df_index: pd.DataFrame) -> List[TrialMeta]:\n",
    "    \"\"\"master index DataFrame から TrialMeta のリストを生成。\"\"\"\n",
    "    metas: List[TrialMeta] = []\n",
    "    for _, row in df_index.iterrows():\n",
    "        metas.append(\n",
    "            TrialMeta(\n",
    "                subject      = str(row[\"subject\"]),\n",
    "                participant  = row.get(\"participant\", None),\n",
    "                run          = str(row[\"run\"]),\n",
    "                trial_in_run = int(row[\"trial_in_run\"]),\n",
    "                number       = (None if pd.isna(row[\"number\"]) else int(row[\"number\"])),\n",
    "                FileName     = (None if pd.isna(row[\"FileName\"]) else str(row[\"FileName\"])),\n",
    "                category     = str(row[\"category\"]),\n",
    "                qc_pass      = bool(row[\"qc_pass\"]),\n",
    "            )\n",
    "        )\n",
    "    return metas\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. TABLE BUILDING（trial → sound → subject）\n",
    "# ============================================================\n",
    "\n",
    "def build_trial_table() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    全trialについて特徴量を計算して trial_table を返す。\n",
    "    ★エラーtrialも残す（feature_ok=False）ことで、後段JOINが安定する。\n",
    "    \"\"\"\n",
    "    df_index = load_epoch_index()\n",
    "    metas = build_trial_meta_list(df_index)\n",
    "\n",
    "    total = len(metas)\n",
    "    n_target = sum(1 for m in metas if (m.qc_pass or not PROCESS_ONLY_QC_PASS))\n",
    "    print(f\"Total trials in master_epoch_index: {total}\")\n",
    "    print(f\"Trials to process (qc filter={PROCESS_ONLY_QC_PASS}): {n_target}\")\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    import sys\n",
    "    done = 0\n",
    "    for meta in metas:\n",
    "        # A運用：qc_fail は“行は残すが計算はしない”\n",
    "        if PROCESS_ONLY_QC_PASS and (not meta.qc_pass):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"subject\": meta.subject,\n",
    "                    \"participant\": meta.participant,\n",
    "                    \"run\": meta.run,\n",
    "                    \"trial_in_run\": meta.trial_in_run,\n",
    "                    \"number\": meta.number,\n",
    "                    \"FileName\": meta.FileName,\n",
    "                    \"category\": meta.category,\n",
    "                    \"qc_pass\": meta.qc_pass,\n",
    "                    \"feature_ok\": False,\n",
    "                    \"error_msg\": \"qc_fail_skipped\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        done += 1\n",
    "        try:\n",
    "            feats = extract_features_for_trial(meta)\n",
    "            feats[\"feature_ok\"] = True\n",
    "            feats[\"error_msg\"] = \"\"\n",
    "        except Exception as e:\n",
    "            if not KEEP_ERROR_ROWS:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                print(f\"[ERROR {done}/{n_target}] {meta.subject}-{meta.run}-trial{meta.trial_in_run:02d}: {e}\")\n",
    "                continue\n",
    "\n",
    "            feats = {\n",
    "                \"subject\": meta.subject,\n",
    "                \"participant\": meta.participant,\n",
    "                \"run\": meta.run,\n",
    "                \"trial_in_run\": meta.trial_in_run,\n",
    "                \"number\": meta.number,\n",
    "                \"FileName\": meta.FileName,\n",
    "                \"category\": meta.category,\n",
    "                \"qc_pass\": meta.qc_pass,\n",
    "                \"feature_ok\": False,\n",
    "                \"error_msg\": str(e),\n",
    "            }\n",
    "\n",
    "        rows.append(feats)\n",
    "\n",
    "        sys.stdout.write(\n",
    "            f\"\\rProcessing trials: {done}/{n_target} ({meta.subject}-{meta.run}-trial{meta.trial_in_run:02d})\"\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    print()\n",
    "    trial_table = pd.DataFrame(rows)\n",
    "    print(\"Finished trial_table. Shape:\", trial_table.shape)\n",
    "\n",
    "    # エラー監査\n",
    "    n_bad = int((trial_table[\"feature_ok\"] == False).sum()) if \"feature_ok\" in trial_table.columns else 0\n",
    "    if n_bad > 0:\n",
    "        trial_table[trial_table[\"feature_ok\"] == False].head(300).to_csv(\n",
    "            AUDIT_DIR / \"eeg_feature_errors_head300.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "        )\n",
    "        print(f\"[WARN] feature_ok=False が {n_bad} 件あります。audit -> {AUDIT_DIR/'eeg_feature_errors_head300.csv'}\")\n",
    "\n",
    "    return trial_table\n",
    "\n",
    "\n",
    "def aggregate_sound_table(trial_table: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    sound(number×FileName)単位で平均＋SEを集約。\n",
    "    ★number/FileName が揃っていないと “音” 集約にならないので、その場合はNoneを返す。\n",
    "    \"\"\" \n",
    "    trial_table = trial_table[trial_table[\"feature_ok\"] == True].copy()\n",
    "    if (\"number\" not in trial_table.columns) or (\"FileName\" not in trial_table.columns):\n",
    "        print(\"[WARN] trial_table に number/FileName が無いので sound-level 集約をスキップします。\")\n",
    "        return None\n",
    "\n",
    "    # 欠損が多い場合も危険（音の同定が曖昧）\n",
    "    miss = trial_table[\"number\"].isna().sum() + trial_table[\"FileName\"].isna().sum()\n",
    "    if miss > 0:\n",
    "        print(\"[WARN] number/FileName 欠損行があるため sound-level は不完全になる可能性があります。\")\n",
    "\n",
    "    # 数値列のうち、メタ（ID系）を除外して特徴量だけにする\n",
    "    meta_numeric = {\"trial_in_run\", \"number\"}  # ← numberも除外\n",
    "    numeric_cols = [c for c in trial_table.select_dtypes(include=[np.number]).columns if c not in meta_numeric]\n",
    "\n",
    "    group_keys = [\"number\", \"FileName\"]\n",
    "    g = trial_table.groupby(group_keys)[numeric_cols]\n",
    "\n",
    "    mean_df = g.mean().add_suffix(\"_mean\")\n",
    "    se_df   = (g.std() / np.sqrt(g.count())).add_suffix(\"_se\")\n",
    "\n",
    "    out = pd.concat([mean_df, se_df], axis=1).reset_index()\n",
    "    return out\n",
    "\n",
    "\n",
    "def aggregate_subject_table(trial_table: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    participant(Pxx)単位で平均。\n",
    "    participant が作れない場合は subject 単位にフォールバック。\n",
    "    \"\"\"\n",
    "    trial_table = trial_table[trial_table[\"feature_ok\"] == True].copy()\n",
    "    id_col = \"participant\" if \"participant\" in trial_table.columns and trial_table[\"participant\"].notna().any() else \"subject\"\n",
    "\n",
    "    numeric_cols = trial_table.select_dtypes(include=[np.number]).columns\n",
    "    # ID数値(trial_in_runなど)は外す\n",
    "    drop_numeric = {\"trial_in_run\"}\n",
    "    numeric_cols = [c for c in numeric_cols if c not in drop_numeric]\n",
    "\n",
    "    g = trial_table.groupby(id_col)[numeric_cols]\n",
    "    out = g.mean().add_suffix(\"_mean\").reset_index().rename(columns={id_col: \"participant\" if id_col==\"participant\" else \"subject\"})\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "    print(\"MASTER_INDEX_CSV:\", MASTER_INDEX_CSV)\n",
    "    print(\"TRIAL_BASE_DIR:\", TRIAL_BASE_DIR)\n",
    "\n",
    "    # 1) trial\n",
    "    print(\"\\nBuilding trial_table ...\")\n",
    "    trial_table = build_trial_table()\n",
    "    OUT_TRIAL_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    trial_table.to_csv(OUT_TRIAL_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved trial-level features -> {OUT_TRIAL_CSV}\")\n",
    "\n",
    "    # 2) sound（可能な場合のみ）\n",
    "    print(\"\\nAggregating sound_table ...\")\n",
    "    sound_table = aggregate_sound_table(trial_table)\n",
    "    if sound_table is not None:\n",
    "        sound_table.to_csv(OUT_SOUND_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved sound-level features -> {OUT_SOUND_CSV}\")\n",
    "        print(\"  n_sounds:\", len(sound_table))\n",
    "    else:\n",
    "        print(\"[INFO] sound-level features は未出力（number/FileName不足のため）\")\n",
    "\n",
    "    # 3) subject/participant\n",
    "    print(\"\\nAggregating subject_table ...\")\n",
    "    subj_table = aggregate_subject_table(trial_table)\n",
    "    subj_table.to_csv(OUT_SUBJ_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved subject-level features -> {OUT_SUBJ_CSV}\")\n",
    "    print(\"  n_subjects:\", len(subj_table))\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "    print(\"trial :\", OUT_TRIAL_CSV)\n",
    "    print(\"sound :\", OUT_SOUND_CSV if sound_table is not None else \"(skipped)\")\n",
    "    print(\"subj  :\", OUT_SUBJ_CSV)\n",
    "    print(\"audit :\", AUDIT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ecc4d-7d4d-40f3-937a-d69853bdccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EEG48]",
   "language": "python",
   "name": "conda-env-EEG48-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
