{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c7b41b-cac8-429b-b3da-8090f02783a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUT_DIR ] /Users/shunsuke/EEG_48sounds/moduleA_outputs\n",
      "[FIG_DIR ] /Users/shunsuke/EEG_48sounds/moduleA_outputs/figures\n",
      "[TAB_DIR ] /Users/shunsuke/EEG_48sounds/moduleA_outputs/tables\n",
      "[LOG_DIR ] /Users/shunsuke/EEG_48sounds/moduleA_outputs/logs\n",
      "[load] sound-level: (48, 12725) from /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_sound_level.csv\n",
      "[load] participant-level: (576, 11392) from /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_participant_sound_level.csv\n",
      "\n",
      "[subjective sound cols]\n",
      "  驚き: 驚き_mean\n",
      "  緊急感: 緊急感_mean\n",
      "  脅威感: 脅威感_mean\n",
      "  圧倒感: 圧倒感_mean\n",
      "  接近: 接近したい気持ち_mean\n",
      "  興味: 興味_mean\n",
      "  没入: 没入_mean\n",
      "  退屈: 退屈_mean\n",
      "\n",
      "[subjective participant cols]\n",
      "  驚き: 驚き\n",
      "  緊急感: 緊急感\n",
      "  脅威感: 脅威感\n",
      "  圧倒感: 圧倒感\n",
      "  接近: 接近したい気持ち\n",
      "  興味: 興味\n",
      "  没入: 没入\n",
      "  退屈: 退屈\n",
      "\n",
      "[join_key sound-level]: number\n",
      "[category_col]: カテゴリー\n",
      "[ambig_col]: is_ambiguous_approach_sd_top10\n",
      "[sign-fix] corr(PC1, proxy_arousal)=0.929  (>=0)\n",
      "[sign-fix] corr(PC2, proxy_approach)=0.772  (>=0)\n",
      "[backup] /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_sound_level__bak_20260112_205304.csv\n",
      "[save] sound-level with targets: /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_sound_level_with_PC.csv\n",
      "[save] participant-level with targets: /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_participant_sound_level_with_targets.csv\n",
      "[save] manifest: /Users/shunsuke/EEG_48sounds/moduleA_outputs/tables/moduleA_figure_manifest.csv /Users/shunsuke/EEG_48sounds/moduleA_outputs/tables/moduleA_table_manifest.csv\n",
      "[save] metadata: /Users/shunsuke/EEG_48sounds/moduleA_outputs/tables/metadata_moduleA.json\n",
      "\n",
      "[Module A DONE]\n",
      " - sound-level targets saved -> /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_sound_level_with_PC.csv\n",
      " - participant-level targets saved -> /Users/shunsuke/EEG_48sounds/derivatives/master_tables/master_participant_sound_level_with_targets.csv\n"
     ]
    }
   ],
   "source": [
    "# moduleA_define_subjective_targets.ipynb\n",
    "\"\"\"\n",
    "Module A：主観ターゲット（情動軸・カテゴリ）の確定【再現性最優先・運用手順書どおり】\n",
    "\n",
    "出力（主要）:\n",
    "- master_sound_level_with_PC.csv（48音：PC1-3 + emo_* + valence classes + ambiguous flag）\n",
    "- master_participant_sound_level_with_targets.csv（被験者×音：sound-levelターゲットを付与 + 任意で個人ターゲット）\n",
    "- figures/ : 図A1〜A4\n",
    "- tables/  : 表A1〜A12、manifest、metadata_moduleA.json\n",
    "\n",
    "注意:\n",
    "- PCAの符号は proxy 相関で固定（PC1>=0 with proxy_arousal, PC2>=0 with proxy_approach）\n",
    "- valenceはPCAに無理に割当てず proxy_valence をz化して emo_valence とする\n",
    "- 3分類は (1) 既存カテゴリ列があれば優先、なければ (2) emo_valence の三分位で生成（閾値固定）\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) ユーザー設定\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "ROOT_DIR = Path(\"/Users/shunsuke/EEG_48sounds\")  # ★環境に合わせる\n",
    "MASTER_TABLE_DIR = ROOT_DIR / \"derivatives\" / \"master_tables\"\n",
    "\n",
    "IN_SOUND = MASTER_TABLE_DIR / \"master_sound_level.csv\"\n",
    "IN_PSUB  = MASTER_TABLE_DIR / \"master_participant_sound_level.csv\"\n",
    "\n",
    "# ===== 出力は EEG_48sounds 直下の moduleA_outputs に統一 =====\n",
    "OUT_DIR  = ROOT_DIR / \"moduleA_outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figures\"\n",
    "TAB_DIR  = OUT_DIR / \"tables\"\n",
    "LOG_DIR  = OUT_DIR / \"logs\"\n",
    "\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[OUT_DIR ]\", OUT_DIR)\n",
    "print(\"[FIG_DIR ]\", FIG_DIR)\n",
    "print(\"[TAB_DIR ]\", TAB_DIR)\n",
    "print(\"[LOG_DIR ]\", LOG_DIR)\n",
    "\n",
    "\n",
    "FIG_DPI = 300\n",
    "N_COMPONENTS = 3\n",
    "N_PERM = 500  # proxy相関・類似度のPermutation p\n",
    "\n",
    "EXPECTED_N_PARTICIPANTS = 12  # ★必要に応じて変更\n",
    "\n",
    "# 日本語フォント（Mac想定）\n",
    "mpl.rcParams[\"font.family\"] = \"Hiragino Sans\"\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0.1) 便利関数\n",
    "# =========================================================\n",
    "FIG_MANIFEST: list[dict] = []\n",
    "TAB_MANIFEST: list[dict] = []\n",
    "\n",
    "def add_fig(fig_id: str, filename: str, title: str, what: str, how_to_read: str, key_result: str):\n",
    "    FIG_MANIFEST.append({\n",
    "        \"figure_id\": fig_id,\n",
    "        \"filename\": filename,\n",
    "        \"title\": title,\n",
    "        \"what\": what,\n",
    "        \"how_to_read\": how_to_read,\n",
    "        \"key_result\": key_result,\n",
    "    })\n",
    "\n",
    "def add_tab(tab_id: str, filename: str, title: str, what: str, columns: str):\n",
    "    TAB_MANIFEST.append({\n",
    "        \"table_id\": tab_id,\n",
    "        \"filename\": filename,\n",
    "        \"title\": title,\n",
    "        \"what\": what,\n",
    "        \"columns\": columns,\n",
    "    })\n",
    "\n",
    "def savefig(path: Path):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=FIG_DPI, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def find_first_column(df: pd.DataFrame, include_keywords: list[str], prefer_keywords: list[str] | None = None) -> str | None:\n",
    "    \"\"\"\n",
    "    include_keywords をすべて含む列を探す。prefer_keywords を含む列を優先。\n",
    "    \"\"\"\n",
    "    include_keywords = [k.lower() for k in include_keywords]\n",
    "    prefer_keywords = [k.lower() for k in (prefer_keywords or [])]\n",
    "\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        low = str(col).lower()\n",
    "        if all(k in low for k in include_keywords):\n",
    "            candidates.append(col)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    if prefer_keywords:\n",
    "        def key(c):\n",
    "            low = str(c).lower()\n",
    "            score = 0\n",
    "            for pk in prefer_keywords:\n",
    "                if pk in low:\n",
    "                    score -= 1\n",
    "            return score, len(str(c))\n",
    "        candidates = sorted(candidates, key=key)\n",
    "    else:\n",
    "        candidates = sorted(candidates, key=lambda x: len(str(x)))\n",
    "\n",
    "    return candidates[0]\n",
    "\n",
    "def perm_pvalue_corr(x: np.ndarray, y: np.ndarray, n_perm: int = 500, seed: int = 42) -> tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    相関rのPermutation p値（両側）。SciPy不要。\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x = x[m]; y = y[m]\n",
    "    n = len(x)\n",
    "    if n < 5:\n",
    "        return np.nan, np.nan, n\n",
    "    r_obs = np.corrcoef(x, y)[0, 1]\n",
    "    cnt = 0\n",
    "    for _ in range(n_perm):\n",
    "        yp = rng.permutation(y)\n",
    "        r = np.corrcoef(x, yp)[0, 1]\n",
    "        if abs(r) >= abs(r_obs):\n",
    "            cnt += 1\n",
    "    p = (cnt + 1) / (n_perm + 1)\n",
    "    return float(r_obs), float(p), int(n)\n",
    "\n",
    "def zscore(s: pd.Series) -> pd.Series:\n",
    "    return (s - s.mean()) / (s.std(ddof=0) + 1e-12)\n",
    "\n",
    "def safe_mean(df: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if len(cols) == 0:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    return df[cols].mean(axis=1)\n",
    "\n",
    "def sign_fix(pc_scores: np.ndarray, loading_vec: np.ndarray, proxy: np.ndarray) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    corr(PC, proxy) >= 0 となるよう符号を固定。\n",
    "    \"\"\"\n",
    "    r, _, _ = perm_pvalue_corr(pc_scores, proxy, n_perm=200, seed=SEED)\n",
    "    if np.isfinite(r) and r < 0:\n",
    "        return -pc_scores, -loading_vec, -r\n",
    "    return pc_scores, loading_vec, r\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) 入力読み込み\n",
    "# =========================================================\n",
    "if not IN_SOUND.exists():\n",
    "    raise FileNotFoundError(f\"master_sound_level.csv が見つかりません: {IN_SOUND}\")\n",
    "\n",
    "df_sound = pd.read_csv(IN_SOUND)\n",
    "print(\"[load] sound-level:\", df_sound.shape, \"from\", IN_SOUND)\n",
    "\n",
    "df_psub = None\n",
    "if IN_PSUB.exists():\n",
    "    df_psub = pd.read_csv(IN_PSUB)\n",
    "    print(\"[load] participant-level:\", df_psub.shape, \"from\", IN_PSUB)\n",
    "else:\n",
    "    print(\"[warn] participant-level が見つからないため、個人レベルの出力はスキップします。\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) 列マッピング（主観8項目）\n",
    "# =========================================================\n",
    "# 音レベル（*_mean）\n",
    "SUBJECTIVE_SOUND_PATTERNS = {\n",
    "    \"驚き\": [\"驚き\", \"mean\"],\n",
    "    \"緊急感\": [\"緊急感\", \"mean\"],\n",
    "    \"脅威感\": [\"脅威感\", \"mean\"],\n",
    "    \"圧倒感\": [\"圧倒\", \"mean\"],\n",
    "    \"接近\": [\"接近\", \"mean\"],\n",
    "    \"興味\": [\"興味\", \"mean\"],\n",
    "    \"没入\": [\"没入\", \"mean\"],\n",
    "    \"退屈\": [\"退屈\", \"mean\"],\n",
    "}\n",
    "\n",
    "sound_cols = []\n",
    "labels = []\n",
    "for lab, keys in SUBJECTIVE_SOUND_PATTERNS.items():\n",
    "    c = find_first_column(df_sound, keys, prefer_keywords=[\"_mean\", \"mean\"])\n",
    "    if c is None:\n",
    "        raise ValueError(f\"[fatal] 音レベル主観列が見つかりません: {lab} (keywords={keys})\")\n",
    "    sound_cols.append(c)\n",
    "    labels.append(lab)\n",
    "\n",
    "print(\"\\n[subjective sound cols]\")\n",
    "for lab, c in zip(labels, sound_cols):\n",
    "    print(f\"  {lab}: {c}\")\n",
    "\n",
    "# 個人レベル（*_meanなし想定）：存在すれば検出\n",
    "psub_cols = None\n",
    "pid_col = None\n",
    "sound_id_col = None\n",
    "\n",
    "if df_psub is not None:\n",
    "    pid_col = next((c for c in df_psub.columns if any(k in str(c).lower() for k in [\"participant\", \"subject\", \"sub_id\", \"sid\", \"pid\"])), None)\n",
    "    sound_id_col = next((c for c in df_psub.columns if any(k in str(c).lower() for k in [\"sound_id\", \"stim_id\", \"stimulus_id\", \"sound\", \"number\"])), None)\n",
    "    if pid_col is None or sound_id_col is None:\n",
    "        raise ValueError(\"[fatal] participant-level の participant列 or sound_id列 の自動検出に失敗しました。\")\n",
    "\n",
    "    tmp = []\n",
    "    for lab in labels:\n",
    "        c = find_first_column(df_psub, [lab], prefer_keywords=None)\n",
    "        if c is None:\n",
    "            # “接近したい気持ち”など別名救済\n",
    "            if lab == \"接近\":\n",
    "                c = find_first_column(df_psub, [\"接近\"], prefer_keywords=None)\n",
    "        if c is None:\n",
    "            raise ValueError(f\"[fatal] participant-level 主観列が見つかりません: {lab}\")\n",
    "        tmp.append(c)\n",
    "    psub_cols = tmp\n",
    "\n",
    "    print(\"\\n[subjective participant cols]\")\n",
    "    for lab, c in zip(labels, psub_cols):\n",
    "        print(f\"  {lab}: {c}\")\n",
    "\n",
    "# join key（音ID）\n",
    "join_key = None\n",
    "for k in [\"number\", \"sound_id\", \"SoundID\", \"stim_id\"]:\n",
    "    if k in df_sound.columns:\n",
    "        join_key = k\n",
    "        break\n",
    "if join_key is None:\n",
    "    raise ValueError(\"[fatal] sound-level の音ID列（number/sound_id等）が見つかりません。\")\n",
    "\n",
    "print(\"\\n[join_key sound-level]:\", join_key)\n",
    "\n",
    "# カテゴリ列（既存があれば）\n",
    "category_col = None\n",
    "for cand in [\"カテゴリー\", \"カテゴリ\", \"category\", \"Category\"]:\n",
    "    if cand in df_sound.columns:\n",
    "        category_col = cand\n",
    "        break\n",
    "if category_col is None:\n",
    "    # 部分一致\n",
    "    category_col = next((c for c in df_sound.columns if \"カテゴリ\" in str(c) or \"category\" in str(c).lower()), None)\n",
    "print(\"[category_col]:\", category_col)\n",
    "\n",
    "# 曖昧フラグ列（既存優先）\n",
    "ambig_col = \"is_ambiguous_approach_sd_top10\" if \"is_ambiguous_approach_sd_top10\" in df_sound.columns else None\n",
    "if ambig_col is None:\n",
    "    ambig_col = next((c for c in df_sound.columns if \"曖昧\" in str(c) or \"ambig\" in str(c).lower()), None)\n",
    "print(\"[ambig_col]:\", ambig_col)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) 入力QC（欠損・人数）\n",
    "# =========================================================\n",
    "qc_rows = []\n",
    "\n",
    "# 音レベル欠損\n",
    "na_sound = df_sound[sound_cols].isna().mean().to_dict()\n",
    "for c, r in na_sound.items():\n",
    "    qc_rows.append({\"level\": \"sound\", \"unit\": \"column\", \"name\": c, \"na_ratio\": r})\n",
    "\n",
    "# 音ごとの欠損率\n",
    "na_by_sound = df_sound[[join_key] + sound_cols].copy()\n",
    "na_by_sound[\"na_ratio\"] = na_by_sound[sound_cols].isna().mean(axis=1)\n",
    "for _, row in na_by_sound.iterrows():\n",
    "    qc_rows.append({\"level\": \"sound\", \"unit\": \"sound\", \"name\": int(row[join_key]), \"na_ratio\": float(row[\"na_ratio\"])})\n",
    "\n",
    "# participant-level: 人数一致\n",
    "if df_psub is not None:\n",
    "    n_subj = df_psub[pid_col].nunique()\n",
    "    n_by_sound = df_psub.groupby(sound_id_col)[pid_col].nunique()\n",
    "    qc_rows.append({\"level\": \"participant\", \"unit\": \"n_participants\", \"name\": \"unique_participants\", \"na_ratio\": np.nan, \"value\": int(n_subj)})\n",
    "    qc_rows.append({\"level\": \"participant\", \"unit\": \"n_by_sound_min\", \"name\": \"min\", \"na_ratio\": np.nan, \"value\": int(n_by_sound.min())})\n",
    "    qc_rows.append({\"level\": \"participant\", \"unit\": \"n_by_sound_max\", \"name\": \"max\", \"na_ratio\": np.nan, \"value\": int(n_by_sound.max())})\n",
    "\n",
    "    bad = n_by_sound[n_by_sound != EXPECTED_N_PARTICIPANTS]\n",
    "    if len(bad) > 0:\n",
    "        for sid, v in bad.items():\n",
    "            qc_rows.append({\"level\": \"participant\", \"unit\": \"bad_sound\", \"name\": int(sid), \"na_ratio\": np.nan, \"value\": int(v)})\n",
    "\n",
    "qc_df = pd.DataFrame(qc_rows)\n",
    "qc_path = TAB_DIR / \"A_table_input_qc.csv\"\n",
    "qc_df.to_csv(qc_path, index=False, encoding=\"utf-8-sig\")\n",
    "add_tab(\"A0\", qc_path.name, \"入力QC（欠損率・人数整合）\", \"解析前提の健全性チェックログ。\", \"level, unit, name, na_ratio, value\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) 主観相関（図A1 + 表A1）\n",
    "# =========================================================\n",
    "corr_subj = df_sound[sound_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    corr_subj, annot=True, fmt=\".2f\", square=True,\n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cbar_kws={\"label\": \"Pearson r\"},\n",
    "    xticklabels=labels, yticklabels=labels\n",
    ")\n",
    "plt.title(\"図A1 主観評価項目の相関行列（音レベル, n=48）\")\n",
    "fig_path = FIG_DIR / \"phaseA_subjective_corr_heatmap.png\"\n",
    "savefig(fig_path)\n",
    "\n",
    "corr_path = TAB_DIR / \"phaseA_subjective_corr_matrix.csv\"\n",
    "corr_subj.to_csv(corr_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "add_fig(\"A1\", fig_path.name, \"主観評価項目の相関行列（音レベル）\",\n",
    "        \"48音の音平均主観8項目の相関（r）。\",\n",
    "        \"強度系のクラスター・退屈の逆相関・接近/興味/没入のまとまりを見る。\",\n",
    "        \"強度系（驚き/緊急/脅威/圧倒）が同方向、退屈が反対側に位置しやすい。\")\n",
    "add_tab(\"A1\", corr_path.name, \"主観評価項目の相関行列（音レベル）\",\n",
    "        \"図A1の元データ（相関係数）。\", \"8×8（主観項目）\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) PCA（音レベル） + 寄与率/ローディング保存\n",
    "# =========================================================\n",
    "X_sound = df_sound[sound_cols].copy()\n",
    "# 原則：欠損がある音は除外（ここでは厳密にdrop）\n",
    "mask_complete = X_sound.notna().all(axis=1)\n",
    "excluded_sounds = df_sound.loc[~mask_complete, join_key].tolist()\n",
    "df_sound_ok = df_sound.loc[mask_complete].copy()\n",
    "X_sound_ok = X_sound.loc[mask_complete].copy()\n",
    "\n",
    "if len(df_sound_ok) < 10:\n",
    "    raise RuntimeError(\"[fatal] 欠損除外後の音数が少なすぎます。入力QCを修正してください。\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xz = scaler.fit_transform(X_sound_ok.values)\n",
    "\n",
    "pca = PCA(n_components=N_COMPONENTS, random_state=SEED)\n",
    "S = pca.fit_transform(Xz)\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "# loadings（features×PC）\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=sound_cols,\n",
    "    columns=[f\"PC{i}\" for i in range(1, N_COMPONENTS + 1)]\n",
    ")\n",
    "loadings[\"label\"] = labels\n",
    "\n",
    "# PCスコア付与（欠損除外のdf_sound_okに対して）\n",
    "for i in range(N_COMPONENTS):\n",
    "    df_sound_ok[f\"PC{i+1}_emotion\"] = S[:, i]\n",
    "\n",
    "# 寄与率\n",
    "var_table = pd.DataFrame({\n",
    "    \"PC\": [f\"PC{i}\" for i in range(1, N_COMPONENTS + 1)],\n",
    "    \"explained_variance_ratio\": explained,\n",
    "    \"cumulative\": np.cumsum(explained),\n",
    "})\n",
    "var_path = TAB_DIR / \"phaseA_PCA_explained_variance.csv\"\n",
    "var_table.to_csv(var_path, index=False, encoding=\"utf-8-sig\")\n",
    "add_tab(\"A2\", var_path.name, \"主観PCA寄与率（音レベル）\",\n",
    "        \"PC1〜PC3の寄与率と累積寄与率。\", \"PC, explained_variance_ratio, cumulative\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) proxy作成 + PC符号固定（最重要）\n",
    "# =========================================================\n",
    "# label -> col\n",
    "lab2col = dict(zip(labels, sound_cols))\n",
    "\n",
    "proxy_arousal  = safe_mean(df_sound_ok, [lab2col.get(\"緊急感\"), lab2col.get(\"脅威感\"), lab2col.get(\"驚き\"), lab2col.get(\"圧倒感\")])\n",
    "proxy_approach = df_sound_ok[lab2col.get(\"接近\")] - df_sound_ok[lab2col.get(\"退屈\")]\n",
    "proxy_valence  = (df_sound_ok[lab2col.get(\"興味\")] + df_sound_ok[lab2col.get(\"没入\")]) - df_sound_ok[lab2col.get(\"退屈\")]\n",
    "\n",
    "df_sound_ok[\"proxy_arousal\"] = proxy_arousal\n",
    "df_sound_ok[\"proxy_approach\"] = proxy_approach\n",
    "df_sound_ok[\"proxy_valence\"] = proxy_valence\n",
    "\n",
    "# PC1/PC2の符号固定\n",
    "pc1, l1, r1 = sign_fix(df_sound_ok[\"PC1_emotion\"].to_numpy(), loadings[\"PC1\"].to_numpy(), proxy_arousal.to_numpy())\n",
    "pc2, l2, r2 = sign_fix(df_sound_ok[\"PC2_emotion\"].to_numpy(), loadings[\"PC2\"].to_numpy(), proxy_approach.to_numpy())\n",
    "\n",
    "df_sound_ok[\"PC1_emotion\"] = pc1\n",
    "df_sound_ok[\"PC2_emotion\"] = pc2\n",
    "loadings[\"PC1\"] = l1\n",
    "loadings[\"PC2\"] = l2\n",
    "\n",
    "print(f\"[sign-fix] corr(PC1, proxy_arousal)={r1:.3f}  (>=0)\")\n",
    "print(f\"[sign-fix] corr(PC2, proxy_approach)={r2:.3f}  (>=0)\")\n",
    "\n",
    "# proxy↔PC 相関（Permutation pつき）\n",
    "rows = []\n",
    "for pr in [\"proxy_arousal\", \"proxy_valence\", \"proxy_approach\"]:\n",
    "    for pc in [\"PC1_emotion\", \"PC2_emotion\", \"PC3_emotion\"]:\n",
    "        r, p, n = perm_pvalue_corr(df_sound_ok[pc].to_numpy(), df_sound_ok[pr].to_numpy(), n_perm=N_PERM, seed=SEED)\n",
    "        rows.append({\"proxy\": pr, \"pc\": pc, \"r\": r, \"p_perm\": p, \"n\": n})\n",
    "corr_perm = pd.DataFrame(rows)\n",
    "corr_perm_path = TAB_DIR / \"phaseA_proxy_vs_PC_corr_with_perm_p.csv\"\n",
    "corr_perm.to_csv(corr_perm_path, index=False, encoding=\"utf-8-sig\")\n",
    "add_tab(\"A4\", corr_perm_path.name, \"proxy↔PC相関（Permutation p付き）\",\n",
    "        \"PC解釈の根拠（符号固定後）。\", \"proxy, pc, r, p_perm, n\")\n",
    "\n",
    "# ローディング保存（符号固定後）\n",
    "load_path = TAB_DIR / \"phaseA_PCA_loadings_subjective.csv\"\n",
    "loadings.to_csv(load_path, index=False, encoding=\"utf-8-sig\")\n",
    "add_tab(\"A3\", load_path.name, \"主観PCAローディング（音レベル）\",\n",
    "        \"主観8項目→PC寄与。PC1/PC2は符号固定済み。\", \"PC1, PC2, PC3, label\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) 図A2（Biplot）・図A3（散布図）\n",
    "# =========================================================\n",
    "pc1_lab = f\"PC1_emotion（覚醒/情動強度; {explained[0]*100:.1f}%）\"\n",
    "pc2_lab = f\"PC2_emotion（接近/関与; {explained[1]*100:.1f}%）\"\n",
    "\n",
    "# カテゴリ（快/中間/不快）を色に\n",
    "cats = None\n",
    "if category_col is not None and category_col in df_sound_ok.columns:\n",
    "    cats = df_sound_ok[category_col].dropna().unique().tolist()\n",
    "    # よくある順序を優先\n",
    "    pref = [\"快\", \"中間\", \"不快\"]\n",
    "    cats_sorted = [c for c in pref if c in cats] + [c for c in cats if c not in pref]\n",
    "    cats = cats_sorted\n",
    "else:\n",
    "    df_sound_ok[\"_category_tmp\"] = \"all\"\n",
    "    category_col = \"_category_tmp\"\n",
    "    cats = [\"all\"]\n",
    "\n",
    "palette = sns.color_palette(\"Set1\", n_colors=len(cats))\n",
    "cat2color = {c: palette[i] for i, c in enumerate(cats)}\n",
    "\n",
    "# 曖昧フラグが無ければ後で生成する可能性があるので一旦仮\n",
    "if ambig_col is None or ambig_col not in df_sound_ok.columns:\n",
    "    df_sound_ok[\"_ambig_tmp\"] = False\n",
    "    ambig_col = \"_ambig_tmp\"\n",
    "\n",
    "# 図A3：散布図\n",
    "plt.figure(figsize=(8, 6))\n",
    "for c in cats:\n",
    "    sub_cat = df_sound_ok[df_sound_ok[category_col] == c]\n",
    "    for is_ambig, marker in [(False, \"o\"), (True, \"s\")]:\n",
    "        sub = sub_cat[sub_cat[ambig_col] == is_ambig]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        plt.scatter(sub[\"PC1_emotion\"], sub[\"PC2_emotion\"],\n",
    "                    s=60, alpha=0.85, marker=marker,\n",
    "                    color=cat2color.get(c, \"C0\"),\n",
    "                    edgecolor=\"k\", linewidth=0.5,\n",
    "                    label=f\"{c}（{'曖昧' if is_ambig else '非曖昧'}）\")\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.xlabel(pc1_lab)\n",
    "plt.ylabel(pc2_lab)\n",
    "plt.title(\"図A3 48音の情動空間配置（PC1×PC2）\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "fig_path = FIG_DIR / \"phaseA_PC1_PC2_scatter.png\"\n",
    "savefig(fig_path)\n",
    "add_fig(\"A3\", fig_path.name, \"48音の情動空間配置（PC1×PC2）\",\n",
    "        \"各点=各音（音レベル）。色=カテゴリ、形=曖昧フラグ。\",\n",
    "        \"PC1方向の勾配（強度）と、同カテゴリ内のPC2分散を見る。\",\n",
    "        \"カテゴリ勾配＋カテゴリ内差（接近/関与）が残ることが多い。\")\n",
    "\n",
    "# 図A2：Biplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "for c in cats:\n",
    "    sub = df_sound_ok[df_sound_ok[category_col] == c]\n",
    "    plt.scatter(sub[\"PC1_emotion\"], sub[\"PC2_emotion\"], s=40, alpha=0.45,\n",
    "                color=cat2color.get(c, \"C0\"), label=str(c))\n",
    "\n",
    "xspan = np.nanpercentile(np.abs(df_sound_ok[\"PC1_emotion\"]), 95)\n",
    "yspan = np.nanpercentile(np.abs(df_sound_ok[\"PC2_emotion\"]), 95)\n",
    "arrow_scale = 0.8 * min(xspan, yspan)\n",
    "\n",
    "for col, lab in zip(sound_cols, labels):\n",
    "    x = float(loadings.loc[loadings[\"label\"] == lab, \"PC1\"].iloc[0]) * arrow_scale\n",
    "    y = float(loadings.loc[loadings[\"label\"] == lab, \"PC2\"].iloc[0]) * arrow_scale\n",
    "    plt.arrow(0, 0, x, y, head_width=0.06, head_length=0.06,\n",
    "              length_includes_head=True, color=\"black\", alpha=0.85)\n",
    "    plt.text(x * 1.12, y * 1.12, lab, fontsize=10, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.axvline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.xlabel(pc1_lab)\n",
    "plt.ylabel(pc2_lab)\n",
    "plt.title(\"図A2 情動空間Biplot（PC1×PC2＋主観ローディング）\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "fig_path = FIG_DIR / \"phaseA_biplot_PC1_PC2.png\"\n",
    "savefig(fig_path)\n",
    "add_fig(\"A2\", fig_path.name, \"情動空間Biplot（PC1×PC2＋主観ローディング）\",\n",
    "        \"点=各音、矢印=主観項目ローディング。\",\n",
    "        \"矢印方向（増加方向）と点の位置関係からPCの意味を読む。\",\n",
    "        \"PC1は強度系↔退屈、PC2は接近/関与の成分になりやすい。\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) 情動軸（emo_*）確定 + valence分類ターゲット確定\n",
    "# =========================================================\n",
    "df_sound_ok[\"emo_arousal\"]  = df_sound_ok[\"PC1_emotion\"]\n",
    "df_sound_ok[\"emo_approach\"] = df_sound_ok[\"PC2_emotion\"]\n",
    "df_sound_ok[\"emo_valence\"]  = zscore(df_sound_ok[\"proxy_valence\"])\n",
    "\n",
    "# 分類ターゲット\n",
    "valence_rule = {}\n",
    "if category_col is not None and category_col in df_sound_ok.columns and set(df_sound_ok[category_col].unique()) >= set([\"快\", \"不快\"]):\n",
    "    # 既存カテゴリを採用（存在する場合）\n",
    "    df_sound_ok[\"valence_3class\"] = df_sound_ok[category_col].astype(str)\n",
    "    valence_rule = {\"type\": \"existing_category_column\", \"column\": category_col}\n",
    "else:\n",
    "    # 三分位で生成（再現性最強）\n",
    "    q33 = float(df_sound_ok[\"emo_valence\"].quantile(1/3))\n",
    "    q67 = float(df_sound_ok[\"emo_valence\"].quantile(2/3))\n",
    "    df_sound_ok[\"valence_3class\"] = np.where(\n",
    "        df_sound_ok[\"emo_valence\"] <= q33, \"不快\",\n",
    "        np.where(df_sound_ok[\"emo_valence\"] >= q67, \"快\", \"中間\")\n",
    "    )\n",
    "    valence_rule = {\"type\": \"tertiles_of_emo_valence\", \"q33\": q33, \"q67\": q67}\n",
    "\n",
    "# 2値（任意）\n",
    "df_sound_ok[\"valence_binary\"] = (df_sound_ok[\"emo_valence\"] >= 0.0).astype(int)\n",
    "\n",
    "# 軸相関監査ログ\n",
    "axis_cols = [\"PC1_emotion\", \"PC2_emotion\", \"PC3_emotion\", \"emo_arousal\", \"emo_approach\", \"emo_valence\"]\n",
    "axis_corr = df_sound_ok[axis_cols].corr()\n",
    "axis_path = TAB_DIR / \"phaseA_axis_corr_check.csv\"\n",
    "axis_corr.to_csv(axis_path, encoding=\"utf-8-sig\")\n",
    "add_tab(\"A5\", axis_path.name, \"情動軸監査（PC↔emo_*相関）\",\n",
    "        \"軸の対応関係の監査ログ。\", \"相関行列\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) 曖昧音フラグ（無ければ計算して付与）\n",
    "# =========================================================\n",
    "ambig_rule = {\"source\": \"existing\"} if \"is_ambiguous_approach_sd_top10\" in df_sound_ok.columns else None\n",
    "\n",
    "if \"is_ambiguous_approach_sd_top10\" not in df_sound_ok.columns:\n",
    "    if df_psub is not None and psub_cols is not None:\n",
    "        # participant-levelから“接近”のSDが大きい音トップ10を曖昧扱い\n",
    "        # まず接近列を得る\n",
    "        approach_lab_idx = labels.index(\"接近\")\n",
    "        col_approach_psub = psub_cols[approach_lab_idx]\n",
    "        sd_by_sound = df_psub.groupby(sound_id_col)[col_approach_psub].std(ddof=0)\n",
    "        top10 = sd_by_sound.sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "        df_sound_ok[\"is_ambiguous_approach_sd_top10\"] = df_sound_ok[join_key].isin(top10)\n",
    "        ambig_rule = {\"source\": \"computed_from_participant_SD\", \"column\": col_approach_psub, \"topk\": 10}\n",
    "    else:\n",
    "        df_sound_ok[\"is_ambiguous_approach_sd_top10\"] = False\n",
    "        ambig_rule = {\"source\": \"not_available_set_false\"}\n",
    "\n",
    "# 曖昧列をメインとして統一\n",
    "df_sound_ok[\"is_ambiguous\"] = df_sound_ok[\"is_ambiguous_approach_sd_top10\"].astype(bool)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) sound-level テーブル保存（with_PC）\n",
    "# =========================================================\n",
    "# 元df_soundに戻す：欠損除外音は残すならNaNで埋める、ここでは “解析対象48音のうち欠損除外音は落とす” 仕様\n",
    "out_sound = df_sound_ok.copy()\n",
    "\n",
    "OUT_SOUND_WITH_PC = MASTER_TABLE_DIR / \"master_sound_level_with_PC.csv\"\n",
    "OUT_SOUND_MASTER  = MASTER_TABLE_DIR / \"master_sound_level.csv\"\n",
    "\n",
    "# backup master_sound_level.csv\n",
    "if OUT_SOUND_MASTER.exists():\n",
    "    bak = MASTER_TABLE_DIR / f\"master_sound_level__bak_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "    shutil.copy2(OUT_SOUND_MASTER, bak)\n",
    "    print(\"[backup]\", bak)\n",
    "\n",
    "out_sound.to_csv(OUT_SOUND_WITH_PC, index=False, encoding=\"utf-8-sig\")\n",
    "out_sound.to_csv(OUT_SOUND_MASTER,  index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save] sound-level with targets:\", OUT_SOUND_WITH_PC)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) participant-level へのターゲット付与（EEG実務用）\n",
    "# =========================================================\n",
    "OUT_PSUB_WITH = MASTER_TABLE_DIR / \"master_participant_sound_level_with_targets.csv\"\n",
    "\n",
    "psub_out_path = None\n",
    "participant_targets_info = {\"generated\": False}\n",
    "\n",
    "if df_psub is not None:\n",
    "    # sound-levelターゲットをjoinして付与（分類/回帰の主ターゲットはこれで固定可能）\n",
    "    merge_cols = [join_key, \"emo_arousal\", \"emo_approach\", \"emo_valence\",\n",
    "                  \"valence_3class\", \"valence_binary\", \"is_ambiguous\"]\n",
    "    sound_targets = out_sound[merge_cols].copy()\n",
    "\n",
    "    # participant側の音ID列名を join_key に揃えるためのrename\n",
    "    df_psub_work = df_psub.copy()\n",
    "    if sound_id_col != join_key:\n",
    "        # join_key が participant側に無ければrenameして合わせる\n",
    "        if join_key not in df_psub_work.columns:\n",
    "            df_psub_work = df_psub_work.rename(columns={sound_id_col: join_key})\n",
    "        else:\n",
    "            # 両方あるなら sound_id_col を使うのをやめる（join_key優先）\n",
    "            pass\n",
    "\n",
    "    df_psub_merged = df_psub_work.merge(sound_targets, on=join_key, how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "    # 任意：個人の主観から “個人ターゲット” も作る（個人差分析で役立つ）\n",
    "    # ここは “統合解析の主ターゲット” ではなく補助列として作る\n",
    "    if psub_cols is not None:\n",
    "        Xp = df_psub_merged[psub_cols].astype(float)\n",
    "\n",
    "        # 欠損行は落とさず、列平均で最小限処理（補助列なのでOK）\n",
    "        Xp = Xp.fillna(Xp.mean())\n",
    "\n",
    "        # participant内z（評定癖を除去）\n",
    "        Xp_wz = df_psub_merged.groupby(pid_col)[psub_cols].transform(\n",
    "            lambda x: (x - x.mean()) / (x.std(ddof=0) + 1e-12)\n",
    "        ).fillna(0.0)\n",
    "\n",
    "        # 個人レベルPCA（再現性チェック用）\n",
    "        pca_psub = PCA(n_components=N_COMPONENTS, random_state=SEED)\n",
    "        pca_psub.fit(Xp_wz.values)\n",
    "\n",
    "        load_psub = pd.DataFrame(\n",
    "            pca_psub.components_.T,\n",
    "            index=labels,\n",
    "            columns=[f\"PC{i}_psub\" for i in range(1, N_COMPONENTS+1)]\n",
    "        )\n",
    "        load_psub_path = TAB_DIR / \"phaseA_PCA_loadings_subjective_participant.csv\"\n",
    "        load_psub.to_csv(load_psub_path, encoding=\"utf-8-sig\")\n",
    "        add_tab(\"A10\", load_psub_path.name, \"主観PCAローディング（個人レベル）\",\n",
    "                \"参加者内z後の個人レベルPCAローディング。\", \"PC1_psub, PC2_psub, PC3_psub\")\n",
    "\n",
    "        # 音レベル loadings と類似度（相関）評価\n",
    "        load_sound_for_sim = loadings.set_index(\"label\")[[f\"PC{i}\" for i in range(1, N_COMPONENTS+1)]].loc[labels]\n",
    "        sim = pd.DataFrame(index=[f\"PC{i}_sound\" for i in range(1, N_COMPONENTS+1)],\n",
    "                           columns=[f\"PC{j}_psub\" for j in range(1, N_COMPONENTS+1)],\n",
    "                           dtype=float)\n",
    "        for i in range(N_COMPONENTS):\n",
    "            v1 = load_sound_for_sim.iloc[:, i].to_numpy()\n",
    "            for j in range(N_COMPONENTS):\n",
    "                v2 = load_psub.iloc[:, j].to_numpy()\n",
    "                sim.iloc[i, j] = np.corrcoef(v1, v2)[0, 1]\n",
    "\n",
    "        sim_path = TAB_DIR / \"phaseA_PCA_loading_similarity_sound_vs_participant.csv\"\n",
    "        sim.to_csv(sim_path, encoding=\"utf-8-sig\")\n",
    "        add_tab(\"A11\", sim_path.name, \"ローディング類似度（音レベル vs 個人レベル）\",\n",
    "                \"音レベルPCAと個人レベルPCAのローディング相関。\", \"PC*_sound × PC*_psub（r）\")\n",
    "\n",
    "        # 図A4\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(sim.astype(float), annot=True, fmt=\".2f\", vmin=-1, vmax=1, center=0, square=True,\n",
    "                    cbar_kws={\"label\": \"Pearson r\"})\n",
    "        plt.title(\"図A4 ローディング類似度（音レベル vs 個人レベル）\")\n",
    "        fig_path = FIG_DIR / \"phaseA_loading_similarity_sound_vs_participant.png\"\n",
    "        savefig(fig_path)\n",
    "        add_fig(\"A4\", fig_path.name, \"ローディング類似度（音レベル vs 個人レベル）\",\n",
    "                \"音平均の因子構造が個人レベルでも再現されるかのチェック。\",\n",
    "                \"対角が高い（PC1↔PC1など）か・非対角が低いかを見る。\",\n",
    "                \"構造が一致すれば、音レベル情動軸が個人レベルでも妥当と言える。\")\n",
    "\n",
    "        # 類似度Permutation p（表）\n",
    "        perm_rows = []\n",
    "        for i in range(N_COMPONENTS):\n",
    "            v1 = load_sound_for_sim.iloc[:, i].to_numpy()\n",
    "            for j in range(N_COMPONENTS):\n",
    "                v2 = load_psub.iloc[:, j].to_numpy()\n",
    "                r, p, n = perm_pvalue_corr(v1, v2, n_perm=N_PERM, seed=SEED)\n",
    "                perm_rows.append({\"sound_pc\": f\"PC{i+1}_sound\", \"psub_pc\": f\"PC{j+1}_psub\", \"r\": r, \"p_perm\": p, \"n\": n})\n",
    "        sim_p = pd.DataFrame(perm_rows)\n",
    "        sim_p_path = TAB_DIR / \"phaseA_loading_similarity_with_perm_p.csv\"\n",
    "        sim_p.to_csv(sim_p_path, index=False, encoding=\"utf-8-sig\")\n",
    "        add_tab(\"A12\", sim_p_path.name, \"ローディング類似度Permutation p\",\n",
    "                \"ローディング相関の偶然性をPermutationで評価。\", \"sound_pc, psub_pc, r, p_perm, n\")\n",
    "\n",
    "        participant_targets_info = {\n",
    "            \"generated\": True,\n",
    "            \"pid_col\": pid_col,\n",
    "            \"sound_id_col_used\": join_key,\n",
    "            \"psub_columns\": dict(zip(labels, psub_cols)),\n",
    "            \"participant_level_PCA\": \"within-subject z then PCA (for structure check)\",\n",
    "        }\n",
    "\n",
    "    # 保存\n",
    "    df_psub_merged.to_csv(OUT_PSUB_WITH, index=False, encoding=\"utf-8-sig\")\n",
    "    psub_out_path = OUT_PSUB_WITH\n",
    "    print(\"[save] participant-level with targets:\", psub_out_path)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) manifest / metadata 出力\n",
    "# =========================================================\n",
    "fig_manifest_path = TAB_DIR / \"moduleA_figure_manifest.csv\"\n",
    "tab_manifest_path = TAB_DIR / \"moduleA_table_manifest.csv\"\n",
    "pd.DataFrame(FIG_MANIFEST).to_csv(fig_manifest_path, index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(TAB_MANIFEST).to_csv(tab_manifest_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "metadata = {\n",
    "    \"module\": \"A\",\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"seed\": SEED,\n",
    "    \"paths\": {\n",
    "        \"root_dir\": str(ROOT_DIR),\n",
    "        \"in_sound\": str(IN_SOUND),\n",
    "        \"in_psub\": str(IN_PSUB),\n",
    "        \"out_dir\": str(OUT_DIR),\n",
    "        \"fig_dir\": str(FIG_DIR),\n",
    "        \"tab_dir\": str(TAB_DIR),\n",
    "        \"out_sound_with_pc\": str(OUT_SOUND_WITH_PC),\n",
    "        \"out_psub_with_targets\": str(psub_out_path) if psub_out_path else None,\n",
    "    },\n",
    "    \"columns\": {\n",
    "        \"join_key_sound\": join_key,\n",
    "        \"category_col\": category_col,\n",
    "        \"ambig_col_initial\": ambig_col,\n",
    "        \"subjective_sound\": dict(zip(labels, sound_cols)),\n",
    "        \"subjective_participant\": dict(zip(labels, psub_cols)) if psub_cols is not None else None,\n",
    "        \"pid_col\": pid_col,\n",
    "        \"sound_id_col_psub_original\": sound_id_col,\n",
    "    },\n",
    "    \"qc\": {\n",
    "        \"expected_n_participants\": EXPECTED_N_PARTICIPANTS,\n",
    "        \"excluded_sounds_due_to_missing_sound_level\": excluded_sounds,\n",
    "        \"n_sounds_used\": int(len(df_sound_ok)),\n",
    "    },\n",
    "    \"pca\": {\n",
    "        \"n_components\": N_COMPONENTS,\n",
    "        \"explained_variance_ratio\": [float(x) for x in explained],\n",
    "        \"sign_fix_rule\": {\n",
    "            \"PC1\": \"corr(PC1, proxy_arousal) >= 0\",\n",
    "            \"PC2\": \"corr(PC2, proxy_approach) >= 0\",\n",
    "            \"corr_after_fix\": {\"PC1_proxy_arousal_r\": float(r1), \"PC2_proxy_approach_r\": float(r2)}\n",
    "        }\n",
    "    },\n",
    "    \"targets\": {\n",
    "        \"emo_arousal\": \"PC1_emotion\",\n",
    "        \"emo_approach\": \"PC2_emotion\",\n",
    "        \"emo_valence\": \"zscore(proxy_valence)\",\n",
    "        \"valence_3class_rule\": valence_rule,\n",
    "        \"valence_binary_rule\": \"emo_valence >= 0 (z-scored)\",\n",
    "        \"ambiguous_rule\": ambig_rule,\n",
    "    },\n",
    "    \"participant_level\": participant_targets_info,\n",
    "}\n",
    "\n",
    "meta_path = TAB_DIR / \"metadata_moduleA.json\"\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[save] manifest:\", fig_manifest_path, tab_manifest_path)\n",
    "print(\"[save] metadata:\", meta_path)\n",
    "\n",
    "print(\"\\n[Module A DONE]\")\n",
    "print(\" - sound-level targets saved ->\", OUT_SOUND_WITH_PC)\n",
    "if psub_out_path:\n",
    "    print(\" - participant-level targets saved ->\", psub_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baaa7d1-3097-493e-9fe4-b82c8be01806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EEG48]",
   "language": "python",
   "name": "conda-env-EEG48-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
