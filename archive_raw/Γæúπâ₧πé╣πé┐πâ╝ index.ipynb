{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49fd584-a87a-48a0-83c1-d4c0d7875bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR    : /Users/shunsuke/EEG_48sounds\n",
      "DESIGN_DIR  : /Users/shunsuke/EEG_48sounds/design\n",
      "INDEX_CSV   : /Users/shunsuke/EEG_48sounds/derivatives/epoch_index_by_trial.csv\n",
      "QC_TRIAL_CSV: /Users/shunsuke/EEG_48sounds/derivatives/qc_all/qc_by_trial_all.csv\n",
      "OUT_CSV     : /Users/shunsuke/EEG_48sounds/derivatives/master_epoch_index.csv\n",
      "\n",
      "--- LOAD OK ---\n",
      "epoch_index_by_trial: (1728, 17)\n",
      "qc_by_trial_all     : (1728, 14)\n",
      "delete old design map cache: /Users/shunsuke/EEG_48sounds/derivatives/design_trial_map_all_participant_runs.csv\n",
      "Saved design map: /Users/shunsuke/EEG_48sounds/derivatives/design_trial_map_all_participant_runs.csv\n",
      "\n",
      "--- DESIGN MAP ---\n",
      "design rows: (1728, 8)\n",
      "missing participant×run blocks: 0\n",
      "saved: /Users/shunsuke/EEG_48sounds/output/qc_audit/missing_design_participant_run.csv\n",
      "\n",
      "[CHECK] design join missing rows: 0\n",
      "\n",
      "[CHECK] qc join missing rows: 0\n",
      "\n",
      "--- AUDIT SUMMARY ---\n",
      "trial count min/max: 48 48\n",
      "qc_pass rate by run:\n",
      "run\n",
      "run1    0.909722\n",
      "run2    0.977431\n",
      "run3    0.869792\n",
      "Name: qc_pass, dtype: float64\n",
      "\n",
      "=== DONE ===\n",
      "Saved: /Users/shunsuke/EEG_48sounds/derivatives/master_epoch_index.csv\n",
      "rows: 1728\n",
      "qc_pass True: 1588\n",
      "Design map saved: /Users/shunsuke/EEG_48sounds/derivatives/design_trial_map_all_participant_runs.csv\n",
      "Audit logs: /Users/shunsuke/EEG_48sounds/output/qc_audit\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# master_epoch_index 作成スクリプト（統合解析対応・最終版）\n",
    "#   - EEG trial index（②の出力）: derivatives/epoch_index_by_trial.csv\n",
    "#   - QC trial（③の出力）      : derivatives/qc_all/qc_by_trial_all.csv\n",
    "#   - design 提示順（CSV群）   : design/**/*.csv\n",
    "#\n",
    "# 出力:\n",
    "#   - derivatives/master_epoch_index.csv\n",
    "#   - derivatives/design_trial_map_all_participant_runs.csv\n",
    "#   - output/qc_audit/*.csv（監査ログ）\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ============================================================\n",
    "# 0) パス設定\n",
    "# ============================================================\n",
    "ROOT_DIR = Path(\"/Users/shunsuke/EEG_48sounds\")\n",
    "if not ROOT_DIR.exists():\n",
    "    ROOT_DIR = Path.cwd() / \"EEG_48sounds\"\n",
    "\n",
    "DESIGN_DIR = ROOT_DIR / \"design\"\n",
    "DERIV_DIR  = ROOT_DIR / \"derivatives\"\n",
    "OUT_AUDIT  = ROOT_DIR / \"output\" / \"qc_audit\"\n",
    "\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_AUDIT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_CSV     = DERIV_DIR / \"epoch_index_by_trial.csv\"\n",
    "QC_TRIAL_CSV  = DERIV_DIR / \"qc_all\" / \"qc_by_trial_all.csv\"\n",
    "\n",
    "DESIGN_ALL_OUT = DERIV_DIR / \"design_trial_map_all_participant_runs.csv\"\n",
    "FORCE_REBUILD_DESIGN_MAP = True  # ★必ずTrue推奨（古いキャッシュが事故の元）\n",
    "\n",
    "OUT_CSV = DERIV_DIR / \"master_epoch_index.csv\"\n",
    "\n",
    "print(\"ROOT_DIR    :\", ROOT_DIR)\n",
    "print(\"DESIGN_DIR  :\", DESIGN_DIR)\n",
    "print(\"INDEX_CSV   :\", INDEX_CSV)\n",
    "print(\"QC_TRIAL_CSV:\", QC_TRIAL_CSV)\n",
    "print(\"OUT_CSV     :\", OUT_CSV)\n",
    "\n",
    "# ============================================================\n",
    "# 1) ユーティリティ\n",
    "# ============================================================\n",
    "\n",
    "def normalize_run(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower().replace(\" \", \"\")\n",
    "    if s.startswith(\"run\"):\n",
    "        s = s[3:]\n",
    "    m = re.search(r\"(\\d+)\", s)\n",
    "    return f\"run{int(m.group(1))}\" if m else s\n",
    "\n",
    "def to_int_safe(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def extract_participant_id(text: str) -> str | None:\n",
    "    \"\"\"どこかに数字があれば拾って Pxx を作る\"\"\"\n",
    "    if text is None or (isinstance(text, float) and np.isnan(text)):\n",
    "        return None\n",
    "    s = str(text)\n",
    "    m = re.search(r\"[Pp](\\d{1,3})\", s)\n",
    "    if m:\n",
    "        return f\"P{int(m.group(1)):02d}\"\n",
    "    m2 = re.search(r\"(\\d{1,3})\", s)\n",
    "    if m2:\n",
    "        return f\"P{int(m2.group(1)):02d}\"\n",
    "    return None\n",
    "\n",
    "def normalize_filename_like_hcu(x: str) -> str:\n",
    "    \"\"\"HCU/主観評価の FileName（例 flute.wav）に寄せる\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = unicodedata.normalize(\"NFKC\", str(x)).strip()\n",
    "    s = re.sub(r\"^\\s*\\d+\\s*[:：]\\s*\", \"\", s)\n",
    "    s = re.sub(r\"(_-?\\d+dBA.*)\\.wav$\", \".wav\", s, flags=re.IGNORECASE)\n",
    "    return s\n",
    "\n",
    "def assert_unique(df: pd.DataFrame, keys: list[str], name: str):\n",
    "    dup = df.duplicated(subset=keys).sum()\n",
    "    if dup > 0:\n",
    "        ex = df[df.duplicated(subset=keys, keep=False)].sort_values(keys).head(30)\n",
    "        raise ValueError(f\"[{name}] キー重複: keys={keys}, dup_rows={dup}\\n例:\\n{ex}\")\n",
    "\n",
    "def standardize_trial_order_df(d: pd.DataFrame, fname: str) -> pd.DataFrame:\n",
    "    \"\"\"design/trial_order の列名ゆれ吸収 & trial補完\"\"\"\n",
    "    rename_map = {\n",
    "        \"No\": \"number\", \"番号\": \"number\", \"num\": \"number\",\n",
    "        \"FileName\": \"FileName\", \"filename\": \"FileName\", \"ファイル名\": \"FileName\",\n",
    "        \"カテゴリー\": \"category\", \"カテゴリ\": \"category\", \"Category\": \"category\", \"category\": \"category\",\n",
    "        \"Trial\": \"trial_in_run\", \"trial\": \"trial_in_run\", \"TRIAL\": \"trial_in_run\",\n",
    "        \"提示順\": \"trial_in_run\",\n",
    "        \"Trial(提示順)\": \"trial_in_run\",\n",
    "        \"Trial（提示順）\": \"trial_in_run\",\n",
    "        \"Order\": \"trial_in_run\", \"order\": \"trial_in_run\",\n",
    "    }\n",
    "    d = d.rename(columns={c: rename_map.get(c, c) for c in d.columns})\n",
    "\n",
    "    need = [\"number\", \"FileName\", \"category\"]\n",
    "    miss = [c for c in need if c not in d.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"{fname}: 必須列不足 {miss} / columns={list(d.columns)}\")\n",
    "\n",
    "    if \"trial_in_run\" not in d.columns:\n",
    "        print(f\"⚠️ [WARN] {fname}: Trial列が無いので行順(1..N)を trial_in_run として補完します。\")\n",
    "        d = d.copy()\n",
    "        d[\"trial_in_run\"] = np.arange(1, len(d) + 1)\n",
    "\n",
    "    d[\"number\"] = d[\"number\"].map(to_int_safe)\n",
    "    d[\"trial_in_run\"] = d[\"trial_in_run\"].map(to_int_safe)\n",
    "\n",
    "    if len(d) != 48:\n",
    "        raise ValueError(f\"{fname}: 48行ではありません: {len(d)}\")\n",
    "\n",
    "    if not (set(d[\"trial_in_run\"]) == set(range(1, 49)) and d[\"trial_in_run\"].is_unique):\n",
    "        raise ValueError(f\"{fname}: trial_in_run が 1..48でない/重複あり\")\n",
    "\n",
    "    if not (set(d[\"number\"]) == set(range(1, 49)) and d[\"number\"].is_unique):\n",
    "        raise ValueError(f\"{fname}: number が 1..48でない/重複あり\")\n",
    "\n",
    "    if d[\"FileName\"].isna().any():\n",
    "        raise ValueError(f\"{fname}: FileName に欠損があります\")\n",
    "    if d[\"FileName\"].duplicated().any():\n",
    "        raise ValueError(f\"{fname}: FileName が重複しています（同一runで同音2回扱いになる）\")\n",
    "\n",
    "    return d[[\"number\", \"FileName\", \"category\", \"trial_in_run\"]].copy()\n",
    "\n",
    "def parse_run_from_path(p: Path) -> str | None:\n",
    "    \"\"\"ファイル名＋親フォルダ名から run を推定\"\"\"\n",
    "    stem = p.stem.lower()\n",
    "    m = re.search(r\"(run[123])\", stem)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    for parent in p.parents:\n",
    "        name = parent.name.lower()\n",
    "        m2 = re.search(r\"(run[123])\", name)\n",
    "        if m2:\n",
    "            return m2.group(1)\n",
    "\n",
    "    m3 = re.search(r\"(?:_|-)([123])(?:_|-|$)\", stem)\n",
    "    if m3:\n",
    "        return f\"run{m3.group(1)}\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def build_design_map(design_dir: Path) -> pd.DataFrame:\n",
    "    if not design_dir.exists():\n",
    "        raise FileNotFoundError(f\"design フォルダが見つかりません: {design_dir}\")\n",
    "\n",
    "    rows = []\n",
    "    csvs = sorted(design_dir.rglob(\"*.csv\"))  # サブフォルダも拾う\n",
    "\n",
    "    if len(csvs) == 0:\n",
    "        raise FileNotFoundError(f\"design フォルダ配下にcsvがありません: {design_dir}\")\n",
    "\n",
    "    for p in csvs:\n",
    "        # -------------------------------\n",
    "        # 1) Jupyterのチェックポイント/隠し物を除外（最重要）\n",
    "        # -------------------------------\n",
    "        if \".ipynb_checkpoints\" in p.parts:\n",
    "            continue\n",
    "        if any(part.startswith(\".\") for part in p.parts):  # 念のため隠しフォルダ全般\n",
    "            continue\n",
    "        if p.name.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # 2) “テンプレっぽい”ものだけ除外（01_run1_trial_order は落とさない）\n",
    "        # -------------------------------\n",
    "        low = p.name.lower()\n",
    "        if any(k in low for k in [\"template\", \"sample\", \"example\"]):\n",
    "            continue\n",
    "\n",
    "        # 読み込み & 正規化\n",
    "        df_raw = pd.read_csv(p)\n",
    "        df_std = standardize_trial_order_df(df_raw, p.name)\n",
    "\n",
    "        run = parse_run_from_path(p)\n",
    "        if run is None:\n",
    "            raise ValueError(\n",
    "                f\"{p}: run を推定できません。ファイル名or親フォルダ名に run1/run2/run3 を入れてください。\"\n",
    "            )\n",
    "\n",
    "        participant = extract_participant_id(p.name)\n",
    "        if participant is None:\n",
    "            raise ValueError(\n",
    "                f\"{p}: participant(Pxx) を推定できません。ファイル名に P01 や 01 など数字を含めてください。\"\n",
    "            )\n",
    "\n",
    "        df_std[\"participant\"] = participant\n",
    "        df_std[\"run\"] = run\n",
    "        df_std[\"FileName_norm\"] = df_std[\"FileName\"].map(normalize_filename_like_hcu)\n",
    "        df_std[\"source_csv\"] = str(p.relative_to(design_dir))  # 監査しやすい\n",
    "\n",
    "        rows.append(df_std[[\"participant\",\"run\",\"trial_in_run\",\"number\",\"FileName\",\"FileName_norm\",\"category\",\"source_csv\"]])\n",
    "\n",
    "    design_all = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3) 重複があったときの安全処理\n",
    "    #    - 内容が同一ならデデュープ\n",
    "    #    - 内容が違うなら停止（危険）\n",
    "    # -------------------------------\n",
    "    keys = [\"participant\",\"run\",\"trial_in_run\"]\n",
    "    dup_mask = design_all.duplicated(subset=keys, keep=False)\n",
    "\n",
    "    if dup_mask.any():\n",
    "        dups = design_all[dup_mask].copy()\n",
    "\n",
    "        # どの列が食い違ってるか（同一キー内で複数値を持つなら “コンフリクト”）\n",
    "        conflict = (\n",
    "            dups.groupby(keys)\n",
    "            .agg(\n",
    "                n_files=(\"FileName_norm\", \"nunique\"),\n",
    "                n_num=(\"number\", \"nunique\"),\n",
    "                n_cat=(\"category\", \"nunique\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        conflict_bad = conflict[(conflict[\"n_files\"]>1) | (conflict[\"n_num\"]>1) | (conflict[\"n_cat\"]>1)]\n",
    "\n",
    "        # ログ出し（後で確認可能）\n",
    "        dups.sort_values(keys + [\"source_csv\"]).to_csv(\n",
    "            OUT_AUDIT / \"design_duplicates_all_rows.csv\",\n",
    "            index=False, encoding=\"utf-8-sig\"\n",
    "        )\n",
    "        conflict.to_csv(\n",
    "            OUT_AUDIT / \"design_duplicates_summary.csv\",\n",
    "            index=False, encoding=\"utf-8-sig\"\n",
    "        )\n",
    "\n",
    "        if len(conflict_bad) > 0:\n",
    "            # 中身が違う重複は危険なので停止\n",
    "            conflict_bad.head(50).to_csv(\n",
    "                OUT_AUDIT / \"design_duplicates_conflicts_head50.csv\",\n",
    "                index=False, encoding=\"utf-8-sig\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                \"design_map に同一キーで内容が食い違う重複があります（危険なので停止）。\\n\"\n",
    "                f\"ログ: {OUT_AUDIT/'design_duplicates_conflicts_head50.csv'}\"\n",
    "            )\n",
    "\n",
    "        # 内容が同一の重複だけ → 優先順位でデデュープ\n",
    "        # 優先: パスが短い（深い階層より本体）/ checkpointなどは既に除外済み\n",
    "        design_all[\"_path_len\"] = design_all[\"source_csv\"].astype(str).map(lambda s: len(Path(s).parts))\n",
    "        design_all = (\n",
    "            design_all.sort_values(keys + [\"_path_len\", \"source_csv\"])\n",
    "            .drop_duplicates(subset=keys, keep=\"first\")\n",
    "            .drop(columns=[\"_path_len\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    # 最終一意性保証\n",
    "    assert_unique(design_all, keys, \"design_all(participant,run,trial)\")\n",
    "\n",
    "    return design_all\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) 読み込み（EEG index / QC）\n",
    "# ============================================================\n",
    "df_idx = pd.read_csv(INDEX_CSV)\n",
    "df_qc  = pd.read_csv(QC_TRIAL_CSV)\n",
    "\n",
    "print(\"\\n--- LOAD OK ---\")\n",
    "print(\"epoch_index_by_trial:\", df_idx.shape)\n",
    "print(\"qc_by_trial_all     :\", df_qc.shape)\n",
    "\n",
    "for k in [\"subject\",\"run\",\"trial_in_run\"]:\n",
    "    if k not in df_idx.columns:\n",
    "        raise ValueError(f\"INDEX_CSV に列がありません: {k}\")\n",
    "    if k not in df_qc.columns:\n",
    "        raise ValueError(f\"QC_TRIAL_CSV に列がありません: {k}\")\n",
    "\n",
    "df_idx[\"subject\"] = df_idx[\"subject\"].astype(str).str.strip()\n",
    "df_idx[\"run\"] = df_idx[\"run\"].map(normalize_run)\n",
    "df_idx[\"trial_in_run\"] = df_idx[\"trial_in_run\"].map(to_int_safe)\n",
    "\n",
    "df_qc[\"subject\"] = df_qc[\"subject\"].astype(str).str.strip()\n",
    "df_qc[\"run\"] = df_qc[\"run\"].map(normalize_run)\n",
    "df_qc[\"trial_in_run\"] = df_qc[\"trial_in_run\"].map(to_int_safe)\n",
    "\n",
    "df_idx[\"participant\"] = df_idx[\"subject\"].apply(extract_participant_id)\n",
    "df_qc[\"participant\"]  = df_qc[\"subject\"].apply(extract_participant_id)\n",
    "\n",
    "if df_idx[\"participant\"].isna().any():\n",
    "    bad = df_idx[df_idx[\"participant\"].isna()][[\"subject\"]].drop_duplicates()\n",
    "    bad.to_csv(OUT_AUDIT / \"bad_subject_cannot_parse_participant.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    raise ValueError(\"EEG index の subject から participant を作れない行があります。ログを確認してください。\")\n",
    "\n",
    "assert_unique(df_idx, [\"participant\",\"run\",\"trial_in_run\"], \"df_idx(participant,run,trial)\")\n",
    "assert_unique(df_qc, [\"subject\",\"run\",\"trial_in_run\"], \"df_qc(subject,run,trial)\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) design_map を構築/読み込み（participant基準）\n",
    "# ============================================================\n",
    "if FORCE_REBUILD_DESIGN_MAP and DESIGN_ALL_OUT.exists():\n",
    "    print(\"delete old design map cache:\", DESIGN_ALL_OUT)\n",
    "    DESIGN_ALL_OUT.unlink()\n",
    "\n",
    "if DESIGN_ALL_OUT.exists() and (FORCE_REBUILD_DESIGN_MAP is False):\n",
    "    df_design = pd.read_csv(DESIGN_ALL_OUT)\n",
    "    df_design[\"run\"] = df_design[\"run\"].map(normalize_run)\n",
    "    df_design[\"trial_in_run\"] = df_design[\"trial_in_run\"].map(to_int_safe)\n",
    "else:\n",
    "    df_design = build_design_map(DESIGN_DIR)\n",
    "    df_design.to_csv(DESIGN_ALL_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved design map:\", DESIGN_ALL_OUT)\n",
    "\n",
    "print(\"\\n--- DESIGN MAP ---\")\n",
    "print(\"design rows:\", df_design.shape)\n",
    "\n",
    "need_blocks = df_idx[[\"participant\",\"run\"]].drop_duplicates()\n",
    "have_blocks = df_design[[\"participant\",\"run\"]].drop_duplicates()\n",
    "cov = need_blocks.merge(have_blocks, on=[\"participant\",\"run\"], how=\"left\", indicator=True)\n",
    "missing_blocks = cov[cov[\"_merge\"]==\"left_only\"][[\"participant\",\"run\"]].sort_values([\"run\",\"participant\"])\n",
    "missing_blocks.to_csv(OUT_AUDIT / \"missing_design_participant_run.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"missing participant×run blocks:\", len(missing_blocks))\n",
    "print(\"saved:\", OUT_AUDIT / \"missing_design_participant_run.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) EEG index ← design JOIN\n",
    "# ============================================================\n",
    "df = df_idx.merge(\n",
    "    df_design.drop(columns=[\"source_csv\"], errors=\"ignore\"),\n",
    "    on=[\"participant\",\"run\",\"trial_in_run\"],\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "missing_design = int(df[\"FileName\"].isna().sum())\n",
    "print(\"\\n[CHECK] design join missing rows:\", missing_design)\n",
    "\n",
    "if missing_design > 0:\n",
    "    df[df[\"FileName\"].isna()][[\"participant\",\"subject\",\"run\",\"trial_in_run\"]].head(500).to_csv(\n",
    "        OUT_AUDIT / \"missing_design_rows_head500.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    raise ValueError(\n",
    "        f\"design（提示順→音）が付与できていません: missing={missing_design}。\\n\"\n",
    "        f\"監査ログ: {OUT_AUDIT / 'missing_design_rows_head500.csv'} と \"\n",
    "        f\"{OUT_AUDIT / 'missing_design_participant_run.csv'} を確認してください。\"\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 5) QC JOIN（subject基準）\n",
    "# ============================================================\n",
    "qc_cols = [\n",
    "    \"n_eeg_channels\",\"max_ptp_uv\",\"mean_ptp_uv\",\"median_ptp_uv\",\n",
    "    \"n_flat\",\"n_extreme\",\"bad_ratio\",\"qc_pass\",\"qc_reason\",\n",
    "]\n",
    "missing = [c for c in qc_cols if c not in df_qc.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"QCファイルに列が不足しています: {missing}\")\n",
    "\n",
    "df_qc_small = df_qc[[\"subject\",\"run\",\"trial_in_run\"] + qc_cols].copy()\n",
    "df_qc_small = df_qc_small.rename(columns={\n",
    "    \"n_eeg_channels\": \"qc_n_eeg_channels\",\n",
    "    \"max_ptp_uv\":     \"qc_max_ptp_uv\",\n",
    "    \"mean_ptp_uv\":    \"qc_mean_ptp_uv\",\n",
    "    \"median_ptp_uv\":  \"qc_median_ptp_uv\",\n",
    "    \"n_flat\":         \"qc_n_flat\",\n",
    "    \"n_extreme\":      \"qc_n_extreme\",\n",
    "    \"bad_ratio\":      \"qc_bad_ratio\",\n",
    "    \"qc_pass\":        \"qc_amp_pass\",\n",
    "    \"qc_reason\":      \"qc_amp_reason\",\n",
    "})\n",
    "\n",
    "df = df.merge(\n",
    "    df_qc_small,\n",
    "    on=[\"subject\",\"run\",\"trial_in_run\"],\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "missing_qc = int(df[\"qc_amp_pass\"].isna().sum())\n",
    "print(\"\\n[CHECK] qc join missing rows:\", missing_qc)\n",
    "\n",
    "if missing_qc > 0:\n",
    "    df[df[\"qc_amp_pass\"].isna()][[\"participant\",\"subject\",\"run\",\"trial_in_run\"]].head(500).to_csv(\n",
    "        OUT_AUDIT / \"missing_qc_rows_head500.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    raise ValueError(\n",
    "        f\"QC が付与できていない行があります: missing={missing_qc}\\n\"\n",
    "        f\"監査ログ: {OUT_AUDIT / 'missing_qc_rows_head500.csv'}\"\n",
    "    )\n",
    "\n",
    "# 欠損が無いことを確認した上で確定\n",
    "df[\"qc_amp_pass\"] = df[\"qc_amp_pass\"].astype(bool)\n",
    "df[\"qc_pass\"] = df[\"qc_amp_pass\"]\n",
    "\n",
    "\n",
    "df[\"FileName_norm\"] = df[\"FileName\"].map(normalize_filename_like_hcu)\n",
    "\n",
    "# ============================================================\n",
    "# 6) 監査出力\n",
    "# ============================================================\n",
    "audit_trials = df.groupby([\"participant\",\"run\"])[\"trial_in_run\"].nunique().reset_index(name=\"n_trials\")\n",
    "audit_trials.to_csv(OUT_AUDIT / \"audit_trials_per_participant_run.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "audit_qc = df.groupby([\"participant\",\"run\"])[\"qc_pass\"].mean().reset_index(name=\"qc_pass_rate\")\n",
    "audit_qc.to_csv(OUT_AUDIT / \"audit_qc_pass_rate_per_participant_run.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n--- AUDIT SUMMARY ---\")\n",
    "print(\"trial count min/max:\", audit_trials[\"n_trials\"].min(), audit_trials[\"n_trials\"].max())\n",
    "print(\"qc_pass rate by run:\")\n",
    "print(df.groupby(\"run\")[\"qc_pass\"].mean())\n",
    "\n",
    "# ============================================================\n",
    "# 7) 並び替え & 保存\n",
    "# ============================================================\n",
    "front_cols = [c for c in [\n",
    "    \"participant\",\"subject\",\"run\",\"trial_in_run\",\n",
    "    \"number\",\"FileName\",\"FileName_norm\",\"category\",\n",
    "    \"qc_pass\",\"qc_amp_pass\",\"qc_amp_reason\",\n",
    "] if c in df.columns]\n",
    "df = df[front_cols + [c for c in df.columns if c not in front_cols]]\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"qc_pass True:\", int(df[\"qc_pass\"].sum()))\n",
    "print(\"Design map saved:\", DESIGN_ALL_OUT)\n",
    "print(\"Audit logs:\", OUT_AUDIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96391e70-f8ce-44bb-b6df-4395be7fa099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EEG48]",
   "language": "python",
   "name": "conda-env-EEG48-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
